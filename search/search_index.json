{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"], "fields": {"title": {"boost": 1000.0}, "text": {"boost": 1.0}, "tags": {"boost": 1000000.0}}}, "docs": [{"location": "getting-started/", "title": "Getting Started", "text": "<p>There are multiple ways to deploy Lakekeeper. Our self-contained examples are the easiest way to get started and deploy everything you need (including S3, Query Engines, Jupyter, ...). By default, compute outside of the docker network cannot access the example Warehouses due to docker networking.</p> <p>If you have your own Storage (e.g. S3) available, you can deploy Lakekeeper using docker compose, deploy on Kubernetes, deploy the pre-build Binary directly or compile Lakekeeper yourself.</p> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15.</p>"}, {"location": "getting-started/#deployment", "title": "Deployment", "text": ""}, {"location": "getting-started/#option-1-examples", "title": "Option 1: \ud83d\udc33 Examples", "text": "<p>Note</p> <p>Our docker compose examples are not designed to be used with compute outside of the docker network (e.g. external Spark).</p> <p>All docker compose examples come with batteries included (Identity Provider, Storage (S3), Query Engines, Jupyter) but are not accessible (by default) for compute outside of the docker network. To use Lakekeeper with external tools outside of the docker network, please check Option 2: Docker Compose</p> \ud83d\udc33 With Authentication &amp; Authorization - Advanced\ud83d\udc33 With Authentication &amp; Authorization - Simple\ud83d\udc33 Unsecured <p>The advanced examples contains multiple query engines, including query engines that are shared between users while still enforcing single-user permissions.</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd lakekeeper/examples/access-control-advanced\ndocker compose up -d\n</code></pre> <p>The simple access control example contains multiple query engines, each used by a single user.</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd lakekeeper/examples/access-control-simple\ndocker compose up -d\n</code></pre> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd lakekeeper/examples/minimal\ndocker compose up -d\n</code></pre> <p>Then open your browser and head to <code>localhost:8888</code> to load the example Jupyter notebooks or head to <code>localhost:8181</code> for the Lakekeeper UI.</p>"}, {"location": "getting-started/#option-2-docker-compose", "title": "Option 2: \ud83d\udc33 Docker Compose", "text": "<p>For a Docker-Compose deployment that is used with external object storage, and external Identity Providers, you can use the <code>docker-compose</code> Setup. Please also check the Examples and our User Guides for additional information on customization.</p> <p>While you can start the \"\ud83d\udc33 Unsecured\" variant without any external dependencies, you will need at least an external object store (S3, ADLS, GCS) to create a Warehouse.</p> \ud83d\udc33 Unsecured\ud83d\udc33 Authentication &amp; Authorization <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd lakekeeper/docker-compose\ndocker compose up -d\n</code></pre> <p>Please follow the Authentication Guide to prepare your Identity Provider. Additional environment variables might be required.</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper\ncd lakekeeper/docker-compose\nexport LAKEKEEPER__OPENID_PROVIDER_URI=... (required)\nexport LAKEKEEPER__OPENID_AUDIENCE=... (recommended)\nexport LAKEKEEPER__UI__OPENID_CLIENT_ID=... (required if UI is used)\nexport LAKEKEEPER__UI__OPENID_SCOPE=... (typically required if UI is used)\ndocker compose -f docker-compose.yaml -f openfga-overlay.yaml up\n</code></pre>"}, {"location": "getting-started/#option-3-kubernetes", "title": "Option 3: \u2638\ufe0f Kubernetes", "text": "<p>We recommend deploying the catalog on Kubernetes using our Helm Chart. Please check the Helm Chart's documentation for possible values. To enable Authentication and Authorization, an external identity provider is required.</p> <p>A community driven Kubernetes Operator is currently in development.</p>"}, {"location": "getting-started/#option-4-binary", "title": "Option 4: \u2699\ufe0f Binary", "text": "<p>For single node deployments, you can also download the Binary for your architecture from GitHub Releases. A basic configuration via environment variables would look like this:</p> <pre><code>export LAKEKEEPER__PG_DATABASE_URL_READ=\"postgres://postgres_user:postgres_urlencoded_password@hostname:5432/catalog_database\"\nexport LAKEKEEPER__PG_DATABASE_URL_WRITE=\"postgres://postgres_user:postgres_urlencoded_password@hostname:5432/catalog_database\"\nexport LAKEKEEPER__PG_ENCRYPTION_KEY=\"MySecretEncryptionKeyThatIBetterNotLoose\"\n\n./lakekeeper migrate\n./lakekeeper serve\n</code></pre> <p>To expose Lakekeeper behind a reverse proxy, most deployments also set: <pre><code>export LAKEKEEPER__BASE_URI=&lt;https://&lt;Url-where-Lakekeeper-is-externally-reachable&gt;\n</code></pre> The default <code>LAKEKEEPER__BASE_URI</code> is <code>https://localhost:8181</code>.</p>"}, {"location": "getting-started/#option-5-build-from-sources", "title": "Option 5: \ud83d\udc68\u200d\ud83d\udcbb Build from Sources", "text": "<p>To customize Lakekeeper, for example to connect to your own Authorization system, you might want to build the binary yourself. Please check the Developer Guide for more information. </p>"}, {"location": "getting-started/#first-steps", "title": "First Steps", "text": "<p>Now that the catalog is up-and-running, the following endpoints are available:</p> <ol> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/ui/</code> - the UI - by default: http://localhost:8181/ui/</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/catalog</code> is the Iceberg REST API</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/management</code> contains the management API</li> <li><code>&lt;LAKEKEEPER__BASE_URI&gt;/swagger-ui</code> hosts Swagger to inspect the API specifications</li> </ol>"}, {"location": "getting-started/#bootstrapping", "title": "Bootstrapping", "text": "<p>Our self-contained docker compose examples are already bootstrapped and require no further actions.</p> <p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the bootstrap endpoint. Among others, bootstrapping sets the initial administrator of Lakekeeper and creates the first project. Please find more information on bootstrapping in the Bootstrap Docs.</p>"}, {"location": "getting-started/#creating-a-warehouse", "title": "Creating a Warehouse", "text": "<p>Now that the server is running, we need to create a new warehouse. We recommend to do this via the UI.</p> <p></p> Create a Warehouse via UI <p></p> <p>Alternatively, we can use the REST-API directly. For an S3 backed warehouse, create a file called <code>create-warehouse-request.json</code>:</p> <pre><code>{\n  \"warehouse-name\": \"my-warehouse\",\n  \"storage-profile\": {\n    \"type\": \"s3\",\n    \"bucket\": \"my-example-bucket\",\n    \"key-prefix\": \"optional/path/in/bucket\",\n    \"region\": \"us-east-1\",\n    \"sts-role-arn\": \"arn:aws:iam::....:role/....\",\n    \"sts-enabled\": true,\n    \"flavor\": \"aws\"\n  },\n  \"storage-credential\": {\n    \"type\": \"s3\",\n    \"credential-type\": \"access-key\",\n    \"aws-access-key-id\": \"...\",\n    \"aws-secret-access-key\": \"...\"\n  }\n}\n</code></pre> <p>We now create a new Warehouse by POSTing the request to the management API:</p> <pre><code>curl -X POST http://localhost:8181/management/v1/warehouse -H \"Content-Type: application/json\" -d @create-warehouse-request.json\n</code></pre> <p>If you want to use a different storage backend, see the Storage Guide for example configurations.</p>"}, {"location": "getting-started/#connect-compute", "title": "Connect Compute", "text": "<p>That's it - we can now use the catalog:</p> <pre><code>import pandas as pd\nimport pyspark\n\nSPARK_VERSION = pyspark.__version__\nSPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\nICEBERG_VERSION = \"1.6.1\"\n\n# if you use adls as storage backend, you need iceberg-azure instead of iceberg-aws-bundle\nconfiguration = {\n    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION},org.apache.iceberg:iceberg-azure-bundle:{ICEBERG_VERSION},org.apache.iceberg:iceberg-gcp-bundle:{ICEBERG_VERSION}\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": \"demo\",\n    \"spark.sql.catalog.demo\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.demo.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    \"spark.sql.catalog.demo.uri\": \"http://localhost:8181/catalog/\",\n    \"spark.sql.catalog.demo.token\": \"dummy\",\n    \"spark.sql.catalog.demo.warehouse\": \"my-warehouse\",\n}\nspark_conf = pyspark.SparkConf()\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\n\nspark.sql(\"USE demo\")\n\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS my_namespace\")\nprint(f\"\\n\\nCurrently the following namespace exist:\")\nprint(spark.sql(\"SHOW NAMESPACES\").toPandas())\nprint(\"\\n\\n\")\n\nsdf = spark.createDataFrame(\n    pd.DataFrame(\n        [[1, 1.2, \"foo\"], [2, 2.2, \"bar\"]], columns=[\"my_ints\", \"my_floats\", \"strings\"]\n    )\n)\n\nspark.sql(\"DROP TABLE IF EXISTS demo.my_namespace.my_table\")\nspark.sql(\n    \"CREATE TABLE demo.my_namespace.my_table (my_ints INT, my_floats DOUBLE, strings STRING) USING iceberg\"\n)\nsdf.writeTo(\"demo.my_namespace.my_table\").append()\nspark.table(\"demo.my_namespace.my_table\").show()\n</code></pre>"}, {"location": "support/", "title": "Community", "text": "<ul> <li> <p> Connect on Discord</p> <p>Connect with us on Discord to ask all your questions, stay up-to-date with the latest announcements and learn how others are using Lakekeeper.</p> </li> <li> <p> Report Issues &amp; Feature Request</p> <p>Open Feature Requests and report Issues on GitHub.</p> </li> <li> <p> Enterprise Support</p> <p>Get enterprise support for Lakekeeper in self-hosted or managed environments.</p> </li> </ul>"}, {"location": "about/code-of-conduct/", "title": "Code of Conduct", "text": "<p>Lakekeeper adheres to the Rust Code of Conduct. This describes the minimum behavior expected from all contributors. Instances of violations of the Code of Conduct can be reported by contacting the project team at moderation@vakamo.com.</p> <p>When you contribute code, you affirm that the contribution is your original work and that you license the work to the project under the project's open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project's open source license and warrant that you have the legal authority to do so.</p>"}, {"location": "about/license/", "title": "License", "text": ""}, {"location": "about/license/#included-projects", "title": "Included projects", "text": "<p>Themes documentation is based on <code>MkDocs</code>.</p> <ul> <li>MkDocs - View license.</li> </ul> <p>Many thanks to the authors and contributors of <code>MkDocs</code>!</p>"}, {"location": "about/license/#apache-license", "title": "Apache License", "text": "<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2025 Vakamo Inc.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"}, {"location": "about/logos/", "title": "Logos", "text": "<p>Below you can find the logos available for download. Click on the links to download the files.</p>"}, {"location": "about/logos/#available-logos", "title": "Available Logos", "text": "<p>Download All</p>"}, {"location": "about/release-notes/", "title": "Release Notes", "text": "<p>Please find following changes that affect the core Lakekeeper library crate. For more information, please also check our GitHub Releases.</p>"}, {"location": "about/release-notes/#changelog", "title": "Changelog", "text": ""}, {"location": "about/release-notes/#0102-2025-10-06", "title": "0.10.2 (2025-10-06)", "text": ""}, {"location": "about/release-notes/#bug-fixes", "title": "Bug Fixes", "text": "<ul> <li>Parse empty ProjectID headers as None (#1424) (8f067e7)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.10.2 (8d5171e)</li> </ul>"}, {"location": "about/release-notes/#0101-2025-10-05", "title": "0.10.1 (2025-10-05)", "text": ""}, {"location": "about/release-notes/#features", "title": "Features", "text": "<ul> <li>Add create table guard to cleanup location on Failure (#1418) (9fc98d8)</li> <li>debug: Add LAKEKEEPER__DEBUG__LOG_REQUEST_BODIES configuration (192e211)</li> <li>Expose client-managed X-Iceberg-Access-Delegation (#1407) (aa72949)</li> <li>implement <code>OpenFgaEntity</code> for <code>ServerId</code> (#1394) (9c2ae42)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_1", "title": "Bug Fixes", "text": "<ul> <li>Add support for <code>write.metadata.metrics.</code> Table Properties (#1422) (d45f473)</li> <li>Dynamic OpenAPI generation for registered Task Queues (192e211)</li> <li>Endpoint stats for Project Endpoints (#1412) (5f6e9c9)</li> <li>Return default initial Task Queue Config (192e211)</li> <li>Use FnOnce for queue &amp; service registration (192e211)</li> <li>Use FnOnce for queue &amp; service registration (47b03a0)</li> <li>Use higher consistency for some OpenFGA queries (#1408) (2a9ce24)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_1", "title": "Miscellaneous Chores", "text": "<ul> <li>Generate ID types with Macro (#1419) (47cef6a)</li> <li>release 0.10.1 (d58faec)</li> <li>remove unneded <code>#[allow(dead_code)]</code> (#1399) (dc821f4)</li> </ul>"}, {"location": "about/release-notes/#0100-2025-09-29", "title": "0.10.0 (2025-09-29)", "text": ""}, {"location": "about/release-notes/#breaking-changes", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>authz: extend OpenFGA tabular entities with WarehouseId (#1329)</li> <li>authz: add OpenFGA schema v4 and v3-&gt;v4 migration (#1314)</li> <li>add <code>warehouse_id</code> to PKs of all <code>table_*</code> and <code>view_*</code> tables (#1254)</li> <li>Require warehouse_id when checking permissions on views or tables via ID (#1340)</li> <li>Simplify Task interface, extend SpecializedTask interface (#1310)</li> <li>Move to Lakekeeper IO based on hyperscaler SDKs (#1285)</li> </ul>"}, {"location": "about/release-notes/#features_1", "title": "Features", "text": "<ul> <li>Add <code>RunNow</code> and <code>RunAt</code> to Controls for Tasks (#1322) (75f771e)</li> <li>add <code>warehouse_id</code> to PKs of all <code>table_*</code> and <code>view_*</code> tables (#1254) (f345db4)</li> <li>add endpoint for fuzzy search of tabulars (#1387) (6e7cce9)</li> <li>Add Entity Names to Tasks (#1353) (28cb613)</li> <li>Add loadTable Snapshots filter (#1385) (50b7b0f)</li> <li>Add optional STS session-tags in S3 STS requests for ABAC authorization (#1230) (c84390d)</li> <li>Add support for tasks affecting views (#1378) (62d9f5b)</li> <li>Add tasks heartbeat with Catalog state (#1324) (f13bd8a)</li> <li>authz: add OpenFGA schema v4 and v3-&gt;v4 migration (#1314) (f35c910)</li> <li>authz: Add specific Task Actions (#1379) (34993e4)</li> <li>authz: Deprecate Permission Endpoints for Tables without WarehouseIDs (#1334) (39bea3e)</li> <li>authz: extend OpenFGA tabular entities with WarehouseId (#1329) (8805030)</li> <li>Enable write.metadata.delete-after-commit.enabled by default (8fec31f)</li> <li>Enrich task metadata with concrete tabular type (#1383) (e7b4c63)</li> <li>Improve authorizer <code>are_allowed_*</code> batch check signature (#1315) (9f901c1)</li> <li>Introduce <code>RequestMetadata</code> for Internal Principals which bypass AuthZ (#1306) (c3c45ce)</li> <li>Make number of workers spawned for Expiration &amp; Purge configurable (#1395) (4a80257)</li> <li>Minor namespace join performance improvements (f3bccd8)</li> <li>Move to Lakekeeper IO based on hyperscaler SDKs (#1285) (1658ae6)</li> <li>Provide <code>ApiContext</code> to externally registered tasks (#1307) (7977b45)</li> <li>Random Server IDs (#1328) (4a84ce5)</li> <li>Remove serde_yaml &amp; serde_yml (#1341) (be020da)</li> <li>Require warehouse_id when checking permissions on views or tables via ID (#1340) (d659eea)</li> <li>Return Error Stacks for non-internal errors (8fec31f)</li> <li>Set ServerID during migration (#1348) (ce3ad5b)</li> <li>Simplify Task interface, extend SpecializedTask interface (#1310) (8ffb3f3)</li> <li>Simplify task interface, re-exports (8fec31f)</li> <li>Task Management APIs (1ee7bf2)</li> <li>V3 support (#1364) (0b895c1)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_2", "title": "Bug Fixes", "text": "<ul> <li>accept \"disable\" as PG sslmode as documented (#1303) (8cc32a6)</li> <li>case insensitivity of catalog endpoints (#1338) (602e611)</li> <li>clippy (11bb07b)</li> <li>Improve delete_warehouse error message for active tasks (a8e2e9c)</li> <li>PgSslMode not supported error string (54604c3)</li> <li>Potential Race Condition when undropping Tabulars (#1342) (71528bd)</li> <li>table rename authz and error handling (#1346) (be273cf)</li> <li>tabular-search: distance and docs for <code>searchterm is uuid</code> (#1393) (9d0ef16)</li> <li>Tasks endpoints in router (#1335) (8ac5ccf)</li> <li>Warehouse permissions are directly used for task retrieval &amp; control (#1347) (97d9853)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_2", "title": "Miscellaneous Chores", "text": "<ul> <li>authz: Enforce use of AuthZ decisions with #[must_use] (#1317) (e16f834)</li> <li>Move to parallel <code>unnests</code> in postgres (1ee7bf2)</li> <li>Set Get Task Permission to \"CanGetMetadata\" (#1376) (4e51f1f)</li> <li>test: use nextest filterset instead of needs_env_var (#1304) (51d310d)</li> <li>Use lakekeeper re-exports in bin (#1391) (93de241)</li> </ul>"}, {"location": "about/release-notes/#095-2025-08-14", "title": "0.9.5 (2025-08-14)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_3", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.9.5 (247c69f)</li> </ul>"}, {"location": "about/release-notes/#094-2025-08-06", "title": "0.9.4 (2025-08-06)", "text": ""}, {"location": "about/release-notes/#bug-fixes_3", "title": "Bug Fixes", "text": "<ul> <li><code>partition_statistics</code> deletion (#1269) (f323c58)</li> <li>View Versions default Namespace should be limited to its own Warehouse (#1259) (10bb9da)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_4", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.9.4 (0278948)</li> </ul>"}, {"location": "about/release-notes/#093-2025-07-18", "title": "0.9.3 (2025-07-18)", "text": ""}, {"location": "about/release-notes/#features_2", "title": "Features", "text": "<ul> <li>Add Tracing, Compression and Timeout Layers to UI serving endpoints (#1247) (3008211)</li> <li>Enable client side caching for S3 remote signing (#1226) (2004d17)</li> <li>Globally disable storage validation (#1239) (743d5b5)</li> <li>router: allow 'x-project-id' header in CORS (#1245) (3ca0e4f)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_4", "title": "Bug Fixes", "text": "<ul> <li>Create Warehouse Endpoint should respect X-Project-ID Header (#1238) (7fb01dd)</li> <li>GetAccessQuery should not require nested objects (#1242) (528ab5e)</li> <li>Skip port in determine_base_uri if host already contains it (#1248) (4e12628)</li> <li>Use camelCase for principalUser &amp; Role in GetAccessQuery (#1243) (83dce04)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_5", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.9.3 (44f6986)</li> </ul>"}, {"location": "about/release-notes/#092-2025-07-03", "title": "0.9.2 (2025-07-03)", "text": ""}, {"location": "about/release-notes/#features_3", "title": "Features", "text": "<ul> <li>Add \"https://kubernetes.default.svc.cluster.local\" to the accepted issuers (#1194) (cb53dc0)</li> <li>add params for DEFAULT and MAX page size of paginated queries (#1218) (a114271)</li> <li>authz: enable batch requests for authz checks (#1200) (bb43cd2)</li> <li>re-export iceberg (#1207) (89aaba1)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_5", "title": "Bug Fixes", "text": "<ul> <li>Add 0.9 task migration to changed migration due to 0.9.1 fix (#1198) (c630895)</li> <li>return MAX_PAGE_SIZE entries if pageToken is not specified (#1221) (a48a156)</li> <li>UI redirect without trailing slash did not respect x-forwarded-prefix header (#1205) (dd1d0c4)</li> <li>Undropped tabulars without expiration tasks (#1202) (bd00a76)</li> <li>UserOrRole should have String type for User variant (#1204) (c266f96)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_6", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.9.2 (4725c4a)</li> </ul>"}, {"location": "about/release-notes/#091-2025-06-19", "title": "0.9.1 (2025-06-19)", "text": ""}, {"location": "about/release-notes/#bug-fixes_6", "title": "Bug Fixes", "text": "<ul> <li>Migration error column \"scheduled_for\" of relation \"task\" contains null values (#1189) (72a16c2)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_7", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.9.1 (0f9fb6c)</li> </ul>"}, {"location": "about/release-notes/#090-2025-06-17", "title": "0.9.0 (2025-06-17)", "text": ""}, {"location": "about/release-notes/#breaking-changes_1", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>rename crate and binary to <code>lakekeeper</code> (#1146)</li> </ul>"}, {"location": "about/release-notes/#features_4", "title": "Features", "text": "<ul> <li>add <code>can_list_everything</code> action to <code>warehouse</code> (#1184) (8219c8d)</li> <li>Add actor to CloudEvent Metadata (#1171) (7414342)</li> <li>add quick check path for authorization in <code>list_...</code> fns (#1149) (2ebbae4)</li> <li>testing: add 'test-utils' feature to expose test helpers (#1169) (8c37156)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_7", "title": "Bug Fixes", "text": "<ul> <li>Error code for concurrent updates (#1161) (45e8cf5)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_8", "title": "Miscellaneous Chores", "text": "<ul> <li>rename crate and binary to <code>lakekeeper</code> (#1146) (d3cab68)</li> </ul>"}, {"location": "about/release-notes/#085-2025-05-09", "title": "0.8.5 (2025-05-09)", "text": ""}, {"location": "about/release-notes/#features_5", "title": "Features", "text": "<ul> <li>Optionally disable the use of x-forwarded- headers (2776f80)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_8", "title": "Bug Fixes", "text": "<ul> <li>Do not add standard ports (80, 443) if protocol matches (e5d8147)</li> <li>Format / Clippy (a3f9e25)</li> <li>migration health check (#1083) (b861920)</li> <li>pg17 search_path issue (#1069) (3e531a6)</li> <li>sequence number index cannot use stable function (#1072) (92b4603)</li> <li>Use X-Forwarded-Host header for response in /config Endpoint (2776f80)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_9", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.5 (118dc0b)</li> </ul>"}, {"location": "about/release-notes/#084-2025-05-02", "title": "0.8.4 (2025-05-02)", "text": ""}, {"location": "about/release-notes/#features_6", "title": "Features", "text": "<ul> <li>Add X-Forwarded- Headers to request trace (#1048) (f3bc2b5)</li> <li>Expand allowed characters for user-id to all UTF-8 except control (#1043) (15fddf5)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_9", "title": "Bug Fixes", "text": "<ul> <li>delete warehouse with open tasks should not work with pending/running tasks (#1052) (c327247)</li> <li>Missing prepared query after merge (#1065) (5a1f0c1)</li> <li>staged tables should be dropped with namespaces (#1062) (02714b5)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_10", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.4 (7a0b1c9)</li> </ul>"}, {"location": "about/release-notes/#083-2025-04-18", "title": "0.8.3 (2025-04-18)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_11", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.3 (21612cc)</li> </ul>"}, {"location": "about/release-notes/#082-2025-04-17", "title": "0.8.2 (2025-04-17)", "text": ""}, {"location": "about/release-notes/#features_7", "title": "Features", "text": "<ul> <li>Cloudflare R2 support (#1016) (6cdd408)</li> <li>Make \"s3.delete-enabled\" option configurable in Warehouses (c449e26)</li> <li>Remote signing for delete operations (c449e26)</li> <li>Support for AWS S3 Buckets with KMS (#1029) (922b86d)</li> <li>Support GCS Buckets with Hierarchical Namespaces (#1030) (ba14495)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_10", "title": "Bug Fixes", "text": "<ul> <li>Re-using old ViewVersions fails due to reused timestamps (#1019) (f8d76e9)</li> <li>Task Queues should not wait poll-interval after success (#1025) (beb62d5)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_12", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.2 (26572c8)</li> </ul>"}, {"location": "about/release-notes/#081-2025-04-13", "title": "0.8.1 (2025-04-13)", "text": ""}, {"location": "about/release-notes/#features_8", "title": "Features", "text": "<ul> <li>Add Protection for Tables, Views, Namespaces and Warehouses (fa8c678)</li> <li>Add recursive &amp; force deletes (fa8c678)</li> <li>Azure Managed Identities (#999) (faee35e)</li> <li>Best-Effort checks on unique Warehouse Locations within Projects (#996) (b571947)</li> <li>Faster Storage Profile validation (#989) (8278c5f)</li> <li>Recursive Deletes and Force Deletes (#891) (9762d52)</li> <li>Support for GCP Managed Identities (#1009) (9a8d36e)</li> <li>Support WASBS locations for ADLS (#1010) (008064a)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_11", "title": "Bug Fixes", "text": "<ul> <li>Missing Endpoints in Endpoint Statistic (76bf004)</li> <li>Properly deprecate /default-project endpoints. Use /project instead. (76bf004)</li> <li>undercount table counts in statistics / can't drop table (#990) (ba3dda0)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_13", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.1 (0c66884)</li> </ul>"}, {"location": "about/release-notes/#080-2025-04-08", "title": "0.8.0 (2025-04-08)", "text": ""}, {"location": "about/release-notes/#breaking-changes_2", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>drop duplicate sequence numbers in table snapshots &amp; add index (#952)</li> <li>Move APIs affecting the default project to /default-project according to OpenAPI spec (#878)</li> <li>Remove <code>project_id</code> from <code>RenameProjectRequest</code>. Use <code>/v1/project/{project_id}/rename</code> instead.</li> <li>Change Project ID type from UUID to String (alphanumeric + hyphen + underscore)</li> </ul>"}, {"location": "about/release-notes/#features_9", "title": "Features", "text": "<ul> <li>Accept nameless Application Clients for self-provisioning (Missing app_displayname in Entra-ID token) (#970) (f880c91)</li> <li>Add information about AWS System Identities for S3 storage profiles to Server Info (#972) (5b6aefc)</li> <li>add kafka support #271 (#937) (1fa2f09)</li> <li>Add support for legacy Kubernetes tokens (no audience) (#940) (e7daf01)</li> <li>AWS / S3 Managed Identities (#965) (7490047)</li> <li>Change Project ID type from UUID to String (alphanumeric + hyphen + underscore) (984381b)</li> <li>Migrate to OpenFGA Client (#876) (72ebee4)</li> <li>Re-add constraints for string project IDs (#960) (44b6248)</li> <li>Remove <code>project_id</code> from <code>RenameProjectRequest</code>. Use <code>/v1/project/{project_id}/rename</code> instead. (984381b)</li> <li>s3: make url-style detection configurable (#905) (69234fe)</li> <li>Split OpenFGA Model into components (#881) (e3b658b)</li> <li>stats: Endpoint call statistics (#818) (bf7bcde)</li> <li>view: allow view location updates for trino (#944) (0628fa8)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_12", "title": "Bug Fixes", "text": "<ul> <li>Consistency checks for READ Committed transaction level (#975) (6b852dd)</li> <li>drop duplicate sequence numbers in table snapshots &amp; add index (#952) (abb4882)</li> <li>Endpoint Statistics API, Consistently use kebab-case in API (#968) (5568a9d)</li> <li>error message if rename target namespace does not exist (8b60142)</li> <li>Move APIs affecting the default project to /default-project according to OpenAPI spec (#878) (fa4b26a)</li> <li>set method of endpoint statistics get to post (#951) (329f63d)</li> <li>stats: unique constraint violation on warehouse deletion (#977) (af66510)</li> <li>Table References not deleted (#956) (2ee3ac1)</li> <li>table snapshot refs with add+remove+set-current ref in one transaction (#945) (94da715)</li> <li>use correct image for kube auth test (#979) (a077d2d)</li> <li>views: set 404 instead of 403 on failed view deletion (#963) (d5c4cd6)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_14", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.8.0 (f1b8083)</li> </ul>"}, {"location": "about/release-notes/#074-2025-03-20", "title": "0.7.4 (2025-03-20)", "text": ""}, {"location": "about/release-notes/#features_10", "title": "Features", "text": "<ul> <li>az: support azure shared keys (#912) (e85bd91)</li> <li>configurable bind ip address (#922) (e545a26)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_13", "title": "Bug Fixes", "text": "<ul> <li>add log lines to determine if not found or action forbidden (#895) (66d1d05)</li> <li>az: relax sas token start time (#898) (45eb878)</li> <li>dropping S3 table does not accidentally delete sibling folders with same prefix anymore (#923) (39f60f4)</li> <li>forward sts role arn to s3-compatible storages when specified (#889) (bd4795a)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_15", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.7.4 (e51010a)</li> </ul>"}, {"location": "about/release-notes/#073-2025-03-04", "title": "0.7.3 (2025-03-04)", "text": ""}, {"location": "about/release-notes/#features_11", "title": "Features", "text": "<ul> <li>add EnumString to Catalog{resource}Action (efeb311)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_16", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.7.3 (630ee3a)</li> </ul>"}, {"location": "about/release-notes/#072-2025-02-27", "title": "0.7.2 (2025-02-27)", "text": ""}, {"location": "about/release-notes/#bug-fixes_14", "title": "Bug Fixes", "text": "<ul> <li>Authenticator order (OIDC before K8s), add K8s Authenticator Audiences (#864) (b894ba5)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_17", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.7.2 (b4c77ac)</li> </ul>"}, {"location": "about/release-notes/#071-2025-02-26", "title": "0.7.1 (2025-02-26)", "text": ""}, {"location": "about/release-notes/#features_12", "title": "Features", "text": "<ul> <li>Re-Export RequestMetadata under <code>api</code> (#850) (5a22149)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_15", "title": "Bug Fixes", "text": "<ul> <li>Client Credential Authentication for OpenFGA, allow to configure Scopes (#863) (720053b)</li> <li>more logging in validate warehouse (#860) (efc9eda)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_18", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.7.1 (05938cd)</li> </ul>"}, {"location": "about/release-notes/#070-2025-02-24", "title": "0.7.0 (2025-02-24)", "text": ""}, {"location": "about/release-notes/#features_13", "title": "Features", "text": "<ul> <li>Add Opt-In to S3 Variant prefixes (s3a, s3n) (#821) (b85b724)</li> <li>collect warehouse statistics (#811) (063066c)</li> <li>emit CloudEvent on undropTabulars #572 (#803) (9dd431a)</li> <li>Migrate Authentication to Limes, Support Unlimited Authenticators, Customizable Authentication (b72852d)</li> <li>tasks: add unit to poll interval config (#829) (c0edafa)</li> <li>use x-forwarded-for/host headers to generate links (#834) (89c0f8a)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_16", "title": "Bug Fixes", "text": "<ul> <li>deps: update rust crate rand to 0.9.0 (#785) (b9952de)</li> <li>HEAD Namespace missing in supported endpoints (#847) (f3e43fe)</li> <li>parsing of pg sslmode should be case-insensitive (#802) (1e3d001)</li> <li>s3: set path style access in s3 file_io (#796) (33e690f)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_19", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.7.0 (491940b)</li> </ul>"}, {"location": "about/release-notes/#062-2025-01-30", "title": "0.6.2 (2025-01-30)", "text": ""}, {"location": "about/release-notes/#features_14", "title": "Features", "text": "<ul> <li>Scope validation (#790) (65e664f)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_20", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.6.2 (0c7e181)</li> </ul>"}, {"location": "about/release-notes/#061-2025-01-27", "title": "0.6.1 (2025-01-27)", "text": ""}, {"location": "about/release-notes/#features_15", "title": "Features", "text": "<ul> <li>expose cloud-events tracing publisher on cli (#747) (798e85d)</li> <li>Register table endpoint (#775) (4b88f73)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_17", "title": "Bug Fixes", "text": "<ul> <li>clippy &amp; pin rust version (#758) (0899d4c)</li> <li>Table locations with same prefix (#780) (39eb3d2)</li> <li>test: use 0.3.0 for kube-auth test &amp; fix pyiceberg aws tests (#767) (e6b7b9c)</li> </ul>"}, {"location": "about/release-notes/#performance-improvements", "title": "Performance Improvements", "text": "<ul> <li>optimize load_table by refine SQL (#784) (2b87915)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_21", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.6.1 (a17f5c4)</li> </ul>"}, {"location": "about/release-notes/#060-2025-01-07", "title": "0.6.0 (2025-01-07)", "text": ""}, {"location": "about/release-notes/#features_16", "title": "Features", "text": "<ul> <li>Check Endpoint for single permissions (#706) (6a149a6)</li> <li>Lakekeeper Open Policy Agent Bridge with trino support (3735742)</li> <li>tests: run integration tests with iceberg versions: 1.5.2, 1.6.1, 1.7.1 (3f3b5ad)</li> <li>Update Lakekeeper UI to 0.4.0 (3735742)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_18", "title": "Bug Fixes", "text": "<ul> <li>credentials configs are never empty but are either null or an empty list (3f3b5ad)</li> <li>Default to purge drop for managed tables (#712) (676d995)</li> <li>Enable openfga integration tests (3735742)</li> <li>files of deleted tables not deleted for ADLS (#715) (d81677f)</li> <li>return proper error codes for invalid writes and reads of permission tuples (#727) (96c2d5e)</li> <li>use correct list of supported endpoints (3f3b5ad)</li> </ul>"}, {"location": "about/release-notes/#052-2024-12-17", "title": "0.5.2 (2024-12-17)", "text": ""}, {"location": "about/release-notes/#features_17", "title": "Features", "text": "<ul> <li>Support for Statistic Files (1e4fa38)</li> <li>tables: load table credentials (#675) (9fd272e)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_19", "title": "Bug Fixes", "text": "<ul> <li>Make BASE_URI trailing slash insensitive (#681) (4799ea7)</li> <li>Snapshots without schema (1e4fa38)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_22", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.5.2 (c5774b2)</li> </ul>"}, {"location": "about/release-notes/#051-2024-12-12", "title": "0.5.1 (2024-12-12)", "text": ""}, {"location": "about/release-notes/#features_18", "title": "Features", "text": "<ul> <li>openapi: document error models in openapi (#658) (2a67196)</li> <li>undrop  (#517) (658e757)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_20", "title": "Bug Fixes", "text": "<ul> <li>allow mixed-case properties (#660) (f435573)</li> <li>potential deadlock for views through uncommitted transactions (#638) (0dda8e3)</li> <li>potential deadlock in load-table (#636) (c22b0e0)</li> <li>remove unused relation from openfga schema (#659) (764ca5b)</li> <li>tokens of humans are wrongly identified as applications if \"appid\" claim is present (#647) (bc6b475)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_23", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.5.1 (f8aa87c)</li> </ul>"}, {"location": "about/release-notes/#050-2024-12-06", "title": "0.5.0 (2024-12-06)", "text": ""}, {"location": "about/release-notes/#breaking-changes_3", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>Rename S3 minio flavor to s3-compat (#630)</li> <li>Change default port from 8080 to 8181</li> <li>Default to single-tenant / single-project with NIL Project-ID</li> </ul>"}, {"location": "about/release-notes/#features_19", "title": "Features", "text": "<ul> <li>Add iceberg openapi to swagger (#431) (bb3d12f)</li> <li>Add Iceberg REST Spec to swagger (2eaa10e)</li> <li>add kafka support #271 (#340) (7973586)</li> <li>Add namespace_id filter to list deleted tabulars (#443) (cc82736)</li> <li>Add operator role (#543) (bcddb60)</li> <li>Allow configuration of additional Issuer URLs (b712cf0)</li> <li>Allow configuration of multiple Audiences (b712cf0)</li> <li>Change default port from 8080 to 8181 (b712cf0)</li> <li>Create default Project on Bootstrap (2eaa10e)</li> <li>Default to hard deletion (#507) (5d794aa)</li> <li>Default to single-tenant / single-project with NIL Project-ID (2eaa10e)</li> <li>docs (#605) (c1d2348)</li> <li>Embedded UI (#622) (332f3b8)</li> <li>Enable K8s Auth explicitly (#594) (3773141)</li> <li>Extend user search to email field (#477) (9f9f42b)</li> <li>Fine Grained Access Controls with OpenFGA (2eaa10e)</li> <li>Generated TS Client (#453) (24bfccf)</li> <li>Hierarchical Namespaces (2eaa10e)</li> <li>improve latency against aws by reusing http clients (#540) (8c384f7)</li> <li>OIDC Audience validation (#607) (052bb3f)</li> <li>Optionally return uuids for Iceberg APIs (2eaa10e)</li> <li>pagination without empty pages (#450) (c88a59d)</li> <li>Project Management APIs (2eaa10e)</li> <li>Provide inherited managed access via API (#619) (e7b0394)</li> <li>Rename S3 minio flavor to s3-compat (#630) (acb7419)</li> <li>Server Info Endpoint (2eaa10e)</li> <li>split table metadata into tables (#478) (942fa97)</li> <li>support kubernetes service-accounts (#538) (2982210)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_21", "title": "Bug Fixes", "text": "<ul> <li>aws s3 signer (#493) (b7ad8f4)</li> <li>aws: deal with closed connections via retries (#569) (bbda2c4)</li> <li>azure connection reset (#553) (5d4b041)</li> <li>Bootstrap should return HTTP Code 204 (#597) (25d1d4e)</li> <li>Delete Namespaces with children should not be possible (#482) (7ffd864)</li> <li>flaky aws tests (#545) (f4d46b2)</li> <li>Include Deletion Profile in GetWarehouseResponse (#514) (54a6420)</li> <li>List Namespaces - Top level NS list should only contain top level Namespaces (#512) (795d4f0)</li> <li>list-projects for non admins (#546) (d0066b8)</li> <li>management: deleted tabulars endpoint should not contain underscore (#556) (b15a8fe)</li> <li>only log table load failed when it actually happened (#626) (be5f58c)</li> <li>openapi: Fix Soft-Deletion expiration seconds type (#509) (322a1a0)</li> <li>pagination (#604) (0be19ed)</li> <li>permissions API Parameters (#516) (5133752)</li> <li>prepend a version count to metadata files (#524) (0d9d06f)</li> <li>recreate user (#599) (1194cb0)</li> <li>run metrics router (#628) (f6b47e5)</li> <li>set pool idle timeout to &lt;20 not keepalive timeout (#551) (2ae5b8d)</li> <li>tests: give openfga a bit of time to delete things (#557) (71daf6f)</li> <li>tests: use a shared runtime for tests that share a static reqwest client (#555) (90c6880)</li> <li>Warehouse managed-access in openapi spec (#610) (c860506)</li> <li>WarehouseAdmin renamed to DataAdmin (#515) (7ec4c01)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_24", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.5.0 (b1b2ee6)</li> </ul>"}, {"location": "about/release-notes/#043-2024-11-13", "title": "0.4.3 (2024-11-13)", "text": ""}, {"location": "about/release-notes/#bug-fixes_22", "title": "Bug Fixes", "text": "<ul> <li>aws s3 signer (#493) (dea4a57)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_25", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.4.3 (e577ab2)</li> </ul>"}, {"location": "about/release-notes/#042-2024-10-28", "title": "0.4.2 (2024-10-28)", "text": ""}, {"location": "about/release-notes/#features_20", "title": "Features", "text": "<ul> <li>enable native-tls-root-certs (af26004)</li> <li>improve azure latency by reusing http clients (af26004)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_26", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.4.2 (1d8c469)</li> </ul>"}, {"location": "about/release-notes/#041-2024-10-15", "title": "0.4.1 (2024-10-15)", "text": ""}, {"location": "about/release-notes/#bug-fixes_23", "title": "Bug Fixes", "text": "<ul> <li>bug in join for listing view representations (d2f1d7a)</li> <li>gcs integration test are now running in ci (d2f1d7a)</li> <li>increase keycloak timeout in integration tests (d2f1d7a)</li> <li>purge tests are now properly executed in ci (d2f1d7a)</li> </ul>"}, {"location": "about/release-notes/#040-2024-10-03", "title": "0.4.0 (2024-10-03)", "text": ""}, {"location": "about/release-notes/#breaking-changes_4", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>Rename TIP to Lakekeeper (#372)</li> </ul>"}, {"location": "about/release-notes/#features_21", "title": "Features", "text": "<ul> <li>cache: cache metadata location in signer (#334) (fa0863c)</li> <li>catalog: expiration queue configuration (#330) (fd96861)</li> <li>catalog: Soft-deletions &amp; tabular cleanup queues (#310) (1de63b3)</li> <li>list soft deletions (#302) (0a01eaf)</li> <li>make sure table locations are unique (#335) (543db50)</li> <li>New TableMetadataBuilder with: ID Reassignments, Metadata expiry, safe binding... (#387) (e5c1c77)</li> <li>Rename TIP to Lakekeeper (#372) (57df07e)</li> <li>storage: support for google cloud storage (gcs) (#361) (ebb4e27)</li> <li>tabular: soft-delete &amp; drop purge (#287) (475db44)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_24", "title": "Bug Fixes", "text": "<ul> <li>make conditional compilation of tests depend on var content (#311) (79036db)</li> <li>replace pretty debug prints with properly formatted errors (#327) (efe9fe9)</li> </ul>"}, {"location": "about/release-notes/#030-2024-08-26", "title": "0.3.0 (2024-08-26)", "text": ""}, {"location": "about/release-notes/#breaking-changes_5", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>dots can no longer be used in namespace names (#257)</li> </ul>"}, {"location": "about/release-notes/#features_22", "title": "Features", "text": "<ul> <li>Add support for custom Locations for Namespaces &amp; Tables (1d2ac6f)</li> <li>aws: sts credentials for s3 (#236) (dbf775b)</li> <li>compression-codec: Support setting and altering write.metadata.compression-codec (#235) (f4fb4cb)</li> <li>storage: add ability to narrow token permissions (#249) (ba9f046)</li> <li>storage: adls (#223) (fd11428)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_25", "title": "Bug Fixes", "text": "<ul> <li>dots can no longer be used in namespace names (#257) (8ac52e0)</li> <li>kv2: extend docs &amp; fix mismatch between docs and expected env values (#224) (be3e3e6)</li> </ul>"}, {"location": "about/release-notes/#021-2024-07-29", "title": "0.2.1 (2024-07-29)", "text": ""}, {"location": "about/release-notes/#features_23", "title": "Features", "text": "<ul> <li>db: Add Encryption Secret for postgres SecretStore to README &amp; warn on startup (#217) (933409d)</li> <li>secrets: Secret Backend configuration is now case insensitive (#215) (99b19ab)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_26", "title": "Bug Fixes", "text": "<ul> <li>examples: Fix <code>ICEBERG_REST__BASE_URI</code> (33f213b)</li> <li>s3signing: Add S3 remote signing \"content-md5\" for pyiceberg compatability (33f213b)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_27", "title": "Miscellaneous Chores", "text": "<ul> <li>release 0.2.1 (587ea12)</li> </ul>"}, {"location": "about/release-notes/#020-2024-07-26", "title": "0.2.0 (2024-07-26)", "text": ""}, {"location": "about/release-notes/#breaking-changes_6", "title": "\u26a0 BREAKING CHANGES", "text": "<ul> <li>Catalog base URL should not contain /catalog suffix (#208)</li> <li>views: split off tabular from table to prepare for views</li> </ul>"}, {"location": "about/release-notes/#features_24", "title": "Features", "text": "<ul> <li>health: Service health checks (#181) (3bf4d4c)</li> <li>pagination: add pagination for namespaces &amp; tables &amp; views (#186) (37b1dbd)</li> <li>prometheus: add prometheus axum metrics (#185) (d60d84a)</li> <li>secrets: add support for kv2 secret storage (#192) (a86b13c)</li> <li>server: make listenport configurable (#183) (9ffe0c2)</li> <li>views: authz interface for views &amp; view-ident resolve (#141) (c5e1f99)</li> <li>views: commit views (#146) (0f6310b)</li> <li>views: create + load view (#142) (328cf33)</li> <li>views: exists (#149) (fdb5013)</li> <li>views: list-views (5917a5e)</li> <li>views: rename views (#148) (4aaaa7d)</li> <li>views: split off tabular from table to prepare for views (f62b329)</li> </ul>"}, {"location": "about/release-notes/#bug-fixes_27", "title": "Bug Fixes", "text": "<ul> <li>Catalog base URL should not contain /catalog suffix (#208) (6aabaa9)</li> <li>db: add wait-for-db command (#196) (c1cd069)</li> <li>remove unused cfg-attributes (#203) (b6d17c4)</li> <li>tables: deny \"write.metadata\" &amp; \"write.data.path\" table properties  (#197) (4b2191e)</li> </ul>"}, {"location": "about/release-notes/#010-2024-06-17", "title": "0.1.0 (2024-06-17)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_28", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.1.0 (a5def9a)</li> </ul>"}, {"location": "about/release-notes/#010-rc3-2024-06-17", "title": "0.1.0-rc3 (2024-06-17)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_29", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.1.0-rc3 (9b0d219)</li> </ul>"}, {"location": "about/release-notes/#010-rc2-2024-06-17", "title": "0.1.0-rc2 (2024-06-17)", "text": ""}, {"location": "about/release-notes/#bug-fixes_28", "title": "Bug Fixes", "text": "<ul> <li>add view router (#116) (0745cc8)</li> </ul>"}, {"location": "about/release-notes/#miscellaneous-chores_30", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.1.0-rc2 (9bc25ef)</li> </ul>"}, {"location": "about/release-notes/#010-rc1-2024-06-16", "title": "0.1.0-rc1 (2024-06-16)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_31", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.1.0-rc1 (ba6e5d5)</li> </ul>"}, {"location": "about/release-notes/#002-rc1-2024-06-16", "title": "0.0.2-rc1 (2024-06-16)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_32", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.0.2-rc1 (eb34b9c)</li> </ul>"}, {"location": "about/release-notes/#001-2024-06-15", "title": "0.0.1 (2024-06-15)", "text": ""}, {"location": "about/release-notes/#miscellaneous-chores_33", "title": "Miscellaneous Chores", "text": "<ul> <li>\ud83d\ude80 Release 0.0.1 (c52ddec)</li> </ul>"}, {"location": "docs/0.10.x/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.10.x/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding Github Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/0.10.x/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/0.10.x/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/0.10.x/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.10.x/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.10.x/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.10.x/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.10.x/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/0.10.x/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.10.x/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.10.x/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/0.10.x/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/0.10.x/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/0.10.x/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> The Lakekeeper Helm Chart creates the required binding by default.</p>"}, {"location": "docs/0.10.x/authorization/", "title": "Authorization", "text": "<p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA enables a powerful permission model with bi-directional inheritance, essential for managing modern lakehouses with hierarchical namespaces. Our model balances usability and control for administrators. In addition to OpenFGA, Lakekeeper's OPA bridge provides an additional translation layer that allows query engines such as trino to access Lakekeeper's permissions via Open Policy Agent (OPA). Please find more information in the OPA Bridge Guide.</p> <p>Please check the Authorization Configuration for details on enabling Authorization with Lakekeeper.</p>"}, {"location": "docs/0.10.x/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.10.x/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.10.x/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.10.x/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.10.x/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.10.x/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"}, {"location": "docs/0.10.x/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.10.x/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.10.x/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.10.x/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.10.x/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.10.x/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.10.x/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.10.x/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.10.x/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.10.x/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.10.x/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.10.x/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.10.x/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.10.x/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.10.x/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.10.x/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/0.10.x/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.10.x/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.10.x/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.10.x/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.10.x/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.10.x/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"}, {"location": "docs/0.10.x/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/0.10.x/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.10.x/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/0.10.x/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/0.10.x/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/0.10.x/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/0.10.x/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/0.10.x/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.10.x/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/0.10.x/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/0.10.x/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/0.10.x/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/0.10.x/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/0.10.x/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.10.x/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option."}, {"location": "docs/0.10.x/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/0.10.x/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/0.10.x/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/0.10.x/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/0.10.x/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/0.10.x/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> is chosen, you need to provide additional parameters. The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>] <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/0.10.x/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/0.10.x/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/0.10.x/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.10.x/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/0.10.x/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.10.x/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/0.10.x/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.10.x/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:16\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code. You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/0.10.x/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/0.10.x/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p>"}, {"location": "docs/0.10.x/developer-guide/#lakekeeper", "title": "lakekeeper", "text": "<p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol>"}, {"location": "docs/0.10.x/developer-guide/#lakekeeper-bin", "title": "lakekeeper-bin", "text": "<p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/0.10.x/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/0.10.x/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/0.10.x/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/0.10.x/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/0.10.x/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.10.x/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/0.10.x/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/0.10.x/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.10.x/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/0.10.x/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on Github.</p>"}, {"location": "docs/0.10.x/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/0.10.x/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.10.x/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.10.x/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/0.10.x/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/0.10.x/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/0.10.x/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.10.x/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/0.10.x/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/0.10.x/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/0.10.x/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/0.10.x/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/0.10.x/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/0.10.x/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/0.10.x/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/0.10.x/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/0.10.x/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/0.10.x/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/0.10.x/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p>"}, {"location": "docs/0.10.x/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/0.10.x/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with Minio &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Minio, Rook Ceph and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>Remote signing relies on identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/0.10.x/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key."}, {"location": "docs/0.10.x/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/0.10.x/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/0.10.x/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/0.10.x/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/0.10.x/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.10.x/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. eu&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code></p>"}, {"location": "docs/0.10.x/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/0.10.x/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.10.x/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/0.10.x/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/0.10.x/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/0.10.x/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/0.10.x/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/0.10.x/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/0.10.x/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/0.10.x/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p>"}, {"location": "docs/0.10.x/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/0.10.x/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/0.10.x/api/management/", "title": "Management", "text": ""}, {"location": "docs/0.10.x/docs/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.10.x/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding Github Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/0.10.x/docs/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/0.10.x/docs/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/0.10.x/docs/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.10.x/docs/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.10.x/docs/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.10.x/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.10.x/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/0.10.x/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.10.x/docs/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.10.x/docs/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/0.10.x/docs/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/0.10.x/docs/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/0.10.x/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> The Lakekeeper Helm Chart creates the required binding by default.</p>"}, {"location": "docs/0.10.x/docs/authorization/", "title": "Authorization", "text": "<p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA enables a powerful permission model with bi-directional inheritance, essential for managing modern lakehouses with hierarchical namespaces. Our model balances usability and control for administrators. In addition to OpenFGA, Lakekeeper's OPA bridge provides an additional translation layer that allows query engines such as trino to access Lakekeeper's permissions via Open Policy Agent (OPA). Please find more information in the OPA Bridge Guide.</p> <p>Please check the Authorization Configuration for details on enabling Authorization with Lakekeeper.</p>"}, {"location": "docs/0.10.x/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.10.x/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.10.x/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.10.x/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.10.x/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.10.x/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"}, {"location": "docs/0.10.x/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.10.x/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.10.x/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.10.x/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.10.x/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.10.x/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.10.x/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.10.x/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.10.x/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.10.x/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.10.x/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.10.x/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.10.x/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.10.x/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.10.x/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.10.x/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/0.10.x/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.10.x/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.10.x/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.10.x/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.10.x/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.10.x/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"}, {"location": "docs/0.10.x/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/0.10.x/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.10.x/docs/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/0.10.x/docs/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/0.10.x/docs/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/0.10.x/docs/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/0.10.x/docs/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/0.10.x/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.10.x/docs/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/0.10.x/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/0.10.x/docs/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/0.10.x/docs/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/0.10.x/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/0.10.x/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.10.x/docs/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option."}, {"location": "docs/0.10.x/docs/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/0.10.x/docs/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/0.10.x/docs/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/0.10.x/docs/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/0.10.x/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/0.10.x/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> is chosen, you need to provide additional parameters. The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>] <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/0.10.x/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/0.10.x/docs/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/0.10.x/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.10.x/docs/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/0.10.x/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:16\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code. You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/0.10.x/docs/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p>"}, {"location": "docs/0.10.x/docs/developer-guide/#lakekeeper", "title": "lakekeeper", "text": "<p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol>"}, {"location": "docs/0.10.x/docs/developer-guide/#lakekeeper-bin", "title": "lakekeeper-bin", "text": "<p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.10.x/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/0.10.x/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/0.10.x/docs/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.10.x/docs/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/0.10.x/docs/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on Github.</p>"}, {"location": "docs/0.10.x/docs/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/0.10.x/docs/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.10.x/docs/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.10.x/docs/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/0.10.x/docs/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/0.10.x/docs/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/0.10.x/docs/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.10.x/docs/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/0.10.x/docs/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/0.10.x/docs/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/0.10.x/docs/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/0.10.x/docs/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/0.10.x/docs/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/0.10.x/docs/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/0.10.x/docs/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/0.10.x/docs/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/0.10.x/docs/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/0.10.x/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/0.10.x/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p>"}, {"location": "docs/0.10.x/docs/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/0.10.x/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with Minio &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Minio, Rook Ceph and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>Remote signing relies on identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/0.10.x/docs/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key."}, {"location": "docs/0.10.x/docs/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/0.10.x/docs/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/0.10.x/docs/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/0.10.x/docs/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/0.10.x/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.10.x/docs/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. eu&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code></p>"}, {"location": "docs/0.10.x/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/0.10.x/docs/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.10.x/docs/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/0.10.x/docs/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/0.10.x/docs/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/0.10.x/docs/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/0.10.x/docs/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/0.10.x/docs/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/0.10.x/docs/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/0.10.x/docs/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p>"}, {"location": "docs/0.10.x/docs/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/0.10.x/docs/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/0.10.x/docs/api/management/", "title": "Management", "text": ""}, {"location": "docs/0.11.x/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.11.x/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding GitHub Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/0.11.x/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/0.11.x/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/0.11.x/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.11.x/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.11.x/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.11.x/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.11.x/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/0.11.x/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.11.x/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.11.x/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/0.11.x/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/0.11.x/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/0.11.x/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> <p>The Lakekeeper Helm Chart creates the required binding by default.</p> <p>Applications running in Kubernetes pods can now authenticate using the service account token, which is typically mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Simply read this token and include it in the <code>Authorization</code> header.</p> <p>Example with CURL: <pre><code>curl -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     http://my-lakekeeper:8181/catalog/v1/config\n</code></pre></p> <p>Example with Spark: <pre><code>spark-submit \\\n  --conf spark.sql.catalog.lakekeeper.token=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n  --conf spark.sql.catalog.lakekeeper.uri=\"http://my-lakekeeper:8181/catalog\" \\\n  my-spark-job.py\n</code></pre></p> <p>User identities appear in Lakekeeper as <code>k8s~&lt;namespace&gt;~&lt;service-account-name&gt;</code>.</p>"}, {"location": "docs/0.11.x/authorization/", "title": "Authorization", "text": ""}, {"location": "docs/0.11.x/authorization/#overview", "title": "Overview", "text": "<p>Authentication verifies who you are, while authorization determines what you can do.</p> <p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper currently supports the following Authorizers:</p> <ul> <li>AllowAll: A simple authorizer that allows all requests. This is mainly intended for development and testing purposes.</li> <li>OpenFGA: A fine-grained authorization system based on the CNCF project OpenFGA. Please find more information in the Authorization with OpenFGA section. OpenFGA requires an additional OpenFGA service to be deployed (this is included in our self-contained examples and our helm charts).</li> <li>Cedar: An enterprise-grade policy-based authorization system based on Cedar. The cedar authorizer is built into Lakekeeper and requires no additional external services. Please find more information in the Authorization with Cedar section.</li> <li>Custom: Lakekeeper supports custom authorizers via the <code>Authorizer</code> trait.</li> </ul>"}, {"location": "docs/0.11.x/authorization/#authorization-with-openfga", "title": "Authorization with OpenFGA", "text": "<p>Lakekeeper can use OpenFGA to store and evaluate permissions. OpenFGA provides bi-directional inheritance, which is key for managing hierarchical namespaces in modern lakehouses. For query engines like Trino, Lakekeeper's OPA bridge translates OpenFGA permissions into Open Policy Agent (OPA) format. See the OPA Bridge Guide for details.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/0.11.x/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on GitHub. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.11.x/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.11.x/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.11.x/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.11.x/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.11.x/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. They can delegate the <code>data_admin</code> role they already hold (for example to team members), but they do not have general grant or ownership administration capabilities.</p>"}, {"location": "docs/0.11.x/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.11.x/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.11.x/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.11.x/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.11.x/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.11.x/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.11.x/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.11.x/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.11.x/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>Top-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.11.x/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.11.x/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.11.x/authorization/#openfga-in-production", "title": "OpenFGA in Production", "text": "<p>When deploying OpenFGA in production environments, ensure you follow the OpenFGA Production Checklist.</p> <p>Lakekeeper includes Query Consistency specifications with each authorization request to OpenFGA. For most operations, <code>MINIMIZE_LATENCY</code> consistency provides optimal performance while maintaining sufficient data consistency guarantees.</p> <p>For medium to large-scale deployments, we strongly recommend enabling caching in OpenFGA and increasing the database connection pool limits. These optimizations significantly reduce database load and improve authorization latency. Configure the following environment variables in OpenFGA (written for version 1.10). You may increase the number of connections further if your database deployment can handle additional connections:</p> <pre><code>OPENFGA_DATASTORE_MAX_OPEN_CONNS=200\nOPENFGA_DATASTORE_MAX_IDLE_CONNS=100\nOPENFGA_CACHE_CONTROLLER_ENABLED=true\nOPENFGA_CHECK_QUERY_CACHE_ENABLED=true\nOPENFGA_CHECK_ITERATOR_CACHE_ENABLED=true\n</code></pre>"}, {"location": "docs/0.11.x/authorization/#authorization-with-cedar", "title": "Authorization with Cedar", "text": "<p>Cedar is an enterprise-grade, policy-based authorization system built into Lakekeeper that requires no external services. Cedar uses a declarative policy language to define access controls, making it ideal for organizations that prefer infrastructure-as-code approaches to authorization management.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/0.11.x/authorization/#schema-and-entity-model", "title": "Schema and Entity Model", "text": "<p>For each authorization request, Lakekeeper provides the complete entity hierarchy from the requested resource up to the server level. This ensures policies have full context for making authorization decisions.</p> <p>When a user queries table <code>ns1.ns2.table1</code> in warehouse <code>wh-1</code> within project <code>my-project</code>, Cedar receives the following entities:</p> <ul> <li><code>Server</code> (root)</li> <li><code>Project::\"my-project\"</code></li> <li><code>Warehouse::\"wh-1\"</code> (parent: <code>my-project</code>)</li> <li><code>Namespace::\"ns1\"</code> (parent: <code>wh-1</code>)</li> <li><code>Namespace::\"ns2\"</code> (parent: <code>ns1</code>)</li> <li><code>Table::\"table1\"</code> (parent: <code>ns2</code>)</li> </ul> <p>This hierarchical context allows policies to reference any level in the path. For example, you can write policies that grant access based on the warehouse name, namespace hierarchy, or specific table properties.</p> <p>The Lakekeeper Cedar schema defines all available entity types, attributes, and actions. All loaded entities and policies are validated against this schema on startup and refresh. You can download the schema here: lakekeeper.cedarschema or find it on GitHub.</p> <p>Important: Lakekeeper does not provide Roles as built-in entities. Roles must be defined as custom entities in your entity JSON files.</p>"}, {"location": "docs/0.11.x/authorization/#policy-examples", "title": "Policy Examples", "text": "<p>Grant admin access to a specific user: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action,\n    resource\n);\n</code></pre></p> <p>Role-based warehouse access: <pre><code>// Grant full access to all entities in a warehouse with name \"wh-1\"\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"NamespaceActions\",\n               Lakekeeper::Action::\"TableActions\",\n               Lakekeeper::Action::\"ViewActions\"],\n    resource\n)\nwhen { resource.warehouse.name == \"wh-1\" };\n\n// Allow modification of the warehouse itself\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"WarehouseModifyActions\"],\n    resource\n)\nwhen { resource.name == \"wh-1\" };\n</code></pre></p> <p>Table read access for all tables in the <code>analytics</code> namespace of warehouse <code>wh-1</code>: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action in [Lakekeeper::Action::\"TableSelectActions\"],\n    resource\n)\nwhen {\n    resource.namespace.name == \"analytics\" &amp;&amp;\n    resource.warehouse.name == \"wh-1\"\n};\n</code></pre></p>"}, {"location": "docs/0.11.x/authorization/#entity-definition-example", "title": "Entity Definition Example", "text": "<p>Define roles and assign users to them using JSON entity files:</p> <pre><code>[\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::User\",\n            \"id\": \"oidc~90471f73-e338-4032-9a6b-1e021cc3cb1e\"\n        },\n        \"attrs\": {\n            \"display_name\": \"machine-user-1\"\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"data-engineering\"\n            }\n        ]\n    },\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::Role\",\n            \"id\": \"data-engineering\"\n        },\n        \"attrs\": {\n            \"name\": \"DataEngineering\",\n            \"project\": {\n                \"__entity\": {\n                    \"type\": \"Lakekeeper::Project\",\n                    \"id\": \"00000000-0000-0000-0000-000000000000\"\n                }\n            }\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"warehouse-1-admins\"\n            }\n        ]\n    }\n]\n</code></pre>"}, {"location": "docs/0.11.x/authorization/#policy-and-entity-management", "title": "Policy and Entity Management", "text": "<p>Startup Behavior:</p> <ul> <li>All policy and entity files are loaded and validated against the Cedar schema</li> <li>If any file is unreadable or invalid, Lakekeeper fails to start with an error</li> </ul> <p>This ensures that authorization policies are always valid before serving requests</p> <p>Refresh Behavior: Configure automatic policy refresh using <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> (default: 5 seconds):</p> <ol> <li>Change Detection: Lightweight checks monitor ConfigMap versions and file timestamps</li> <li>Reload on Change: Modified entity or policy files trigger a full reload of all files to guarantee consistency</li> <li>Atomic Updates: The in-memory store is only updated if all files reload successfully</li> <li>Error Handling: If any reload fails, the previous configuration is retained, an error is logged, and health checks report unhealthy status</li> </ol> <p>This approach ensures that authorization policies remain consistent and that partial updates never compromise security.</p>"}, {"location": "docs/0.11.x/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.11.x/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.11.x/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper supports different Authorizers. Please refer to the Authorization Documentation for more information.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.11.x/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.11.x/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/0.11.x/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.11.x/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.11.x/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.11.x/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.11.x/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.11.x/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding GitHub Issue if this would be of interest to you.</p>"}, {"location": "docs/0.11.x/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/0.11.x/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.11.x/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/0.11.x/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/0.11.x/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/0.11.x/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/0.11.x/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/0.11.x/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.11.x/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/0.11.x/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/0.11.x/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/0.11.x/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/0.11.x/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/0.11.x/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.11.x/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option. <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> 2 Number of workers spawned that work on expire Snapshots tasks. See Expire Snapshots Docs for more information."}, {"location": "docs/0.11.x/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/0.11.x/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/0.11.x/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/0.11.x/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/0.11.x/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> <code>resource_access.lakekeeper.roles</code> Specify the claim to use in provided JWT tokens to extract roles. The field should contain an array of strings or a single string. Supports nested claims using dot notation, e.g., \"resource_access.account.roles\". Currently only has an effect when using the Cedar Authorizer. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/0.11.x/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> or <code>cedar</code> is chosen, additional parameters are required (see below). The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>, <code>cedar</code>]"}, {"location": "docs/0.11.x/configuration/#openfga", "title": "OpenFGA", "text": "Variable Example Description <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/0.11.x/configuration/#cedar", "title": "Cedar", "text": "Variable Example Description <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__LOCAL_FILES</code> <code>[/path/to/policies1.cedar,/path/to/policies2.cedar]</code> List of local file paths containing Cedar policies in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__LOCAL_FILES</code> <code>[/path/to/entities1.json,/path/to/entities2.json]</code> List of local JSON file paths containing additional Cedar entities (typically roles). <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedar</code> is treated as a policy source in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedarentities.json</code> is treated as an entity source. <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> <code>5</code> Refresh interval in seconds for reloading policies and entities from Kubernetes ConfigMaps and local files. Default: <code>5</code> seconds. See Cedar Authorization for more information. <code>LAKEKEEPER__CEDAR__EXTERNALLY_MANAGED_USER_AND_ROLES</code> <code>false</code> When set to <code>true</code>, Lakekeeper expects all roles and users to be managed externally via entities.json and does not extract <code>Lakekeeper::Role</code> or <code>Lakekeeper::User</code> entities from the user's token. When set to <code>false</code> (default), Lakekeeper automatically provides <code>Lakekeeper::Role</code> and <code>Lakekeeper::User</code> entities to Cedar based on information extracted from the user's token. When set to <code>false</code>, ensure <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> is configured to specify which claim in the token contains role information. <code>LAKEKEEPER__CEDAR__SCHEMA_FILE</code> <code>/path/to/custom/schema.cedarschema</code> Optional path to a custom Cedar schema file. If provided, this schema will be used instead of the embedded default schema. Useful for extending or customizing the Cedar schema. Compatibility with the Lakekeeper schema must be ensured for all entities provided by Lakekeeper (Server, Project, Namespace, Table, View. User &amp; Role if externally managed roles is <code>false</code>). <p>Debug configurations for Cedar</p> Variable Example Description <code>LAKEKEEPER__CEDAR__DEBUG__LOG_ENTITIES</code> <code>false</code> If <code>true</code>, logs all internal entities (excluding externally managed entities) for each authorization request at debug level. This is useful for debugging authorization issues but can be verbose and impacts performance. Logging only occurs when both this flag is <code>true</code> AND debug logging is enabled (<code>RUST_LOG=debug</code>). Default: <code>false</code>."}, {"location": "docs/0.11.x/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/0.11.x/configuration/#caching", "title": "Caching", "text": "<p>Lakekeeper uses in-memory caches to speed up certain operations.</p> <p>Short-Term Credentials (STC) Cache</p> <p>When Lakekeeper vends short-term credentials for cloud storage access (S3 STS, Azure SAS tokens, or GCP access tokens), these credentials can be cached to reduce load on cloud identity services and improve response times.</p> Variable Example Description <code>LAKEKEEPER__CACHE__STC__ENABLED</code> <code>true</code> Enable or disable the short-term credentials cache. Default: <code>true</code> <code>LAKEKEEPER__CACHE__STC__CAPACITY</code> <code>10000</code> Maximum number of credential entries to cache. Default: <code>10000</code> <p>Expiry Mechanism: Cached credentials automatically expire based on the validity period of the underlying cloud credentials. Lakekeeper caches credentials for half their lifetime (e.g., if GCP STS returns credentials valid for 1 hour, they're cached for 30 minutes) with a maximum cache duration of 1 hour. This ensures credentials remain fresh while reducing unnecessary identity service calls.</p> <p>Metrics: The STC cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_stc_cache_size{cache_type=\"stc\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_stc_cache_hits_total{cache_type=\"stc\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_stc_cache_misses_total{cache_type=\"stc\"}</code>: Total number of cache misses</li> </ul> <p>Warehouse Cache</p> <p>Caches warehouse metadata to reduce database queries for warehouse lookups.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__WAREHOUSE__ENABLED</code> boolean <code>true</code> Enable/disable warehouse caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__CAPACITY</code> integer <code>1000</code> Maximum number of warehouses to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to Storage Profile may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. Warehouse metadata is guaranteed to be fresh for load table &amp; view operations also for multi-worker deployments.</p> <p>Metrics: The Warehouse cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_warehouse_cache_size{cache_type=\"warehouse\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_warehouse_cache_hits_total{cache_type=\"warehouse\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_warehouse_cache_misses_total{cache_type=\"warehouse\"}</code>: Total number of cache misses</li> </ul> <p>Namespace Cache</p> <p>Caches namespace metadata and hierarchies to reduce database queries for namespace lookups. Namespace lookups are also required for table &amp; view operations.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__NAMESPACE__ENABLED</code> boolean <code>true</code> Enable/disable namespace caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__NAMESPACE__CAPACITY</code> integer <code>1000</code> Maximum number of namespaces to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__NAMESPACE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to namespace properties may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. The namespace cache stores both individual namespaces and their parent hierarchies for efficient lookups.</p> <p>Metrics: The Namespace cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_namespace_cache_size{cache_type=\"namespace\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_namespace_cache_hits_total{cache_type=\"namespace\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_namespace_cache_misses_total{cache_type=\"namespace\"}</code>: Total number of cache misses</li> </ul> <p>Secrets Cache</p> <p>Caches storage secrets to reduce load on the secret store. Since Lakekeeper never updates secrets, long TTLs can significantly increase resilience against secret store outages, especially when the secret store is external to the main database backend.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__SECRETS__ENABLED</code> boolean <code>true</code> Enable/disable secrets caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__SECRETS__CAPACITY</code> integer <code>500</code> Maximum number of secrets to cache. Default: <code>500</code> <code>LAKEKEEPER__CACHE__SECRETS__TIME_TO_LIVE_SECS</code> integer <code>600</code> Time-to-live for cache entries in seconds. Default: <code>600</code> (10 minutes) <p>Metrics: The Secrets cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_secrets_cache_size{cache_type=\"secrets\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_secrets_cache_hits_total{cache_type=\"secrets\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_secrets_cache_misses_total{cache_type=\"secrets\"}</code>: Total number of cache misses</li> </ul>"}, {"location": "docs/0.11.x/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/0.11.x/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.11.x/configuration/#debug", "title": "Debug", "text": "<p>Lakekeeper provides debugging options to help troubleshoot issues during development. These options should not be enabled in production environments as they can expose sensitive data and impact performance.</p> Variable Example Description <code>LAKEKEEPER__DEBUG__LOG_REQUEST_BODIES</code> <code>true</code> If set to <code>true</code>, Lakekeeper will log all incoming and outgoing request bodies at debug level. This is useful for debugging API interactions but should never be enabled in production as it can expose sensitive data (credentials, tokens, etc.) and significantly impact performance. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__MIGRATE_BEFORE_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper waits for the DB (30s) and runs migrations when <code>serve</code> is called. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__AUTO_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper will automatically start the server when no subcommand is provided (i.e., when running the binary without arguments). This is useful for development environments to quickly start the server without explicitly specifying the <code>serve</code> command. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__EXTENDED_LOGS</code> <code>false</code> Controls whether file names and line numbers are included in JSON log output. When set to <code>false</code>, these fields are omitted for cleaner logs. When set to <code>true</code>, each log entry includes <code>filename</code> and <code>line_number</code> fields for easier debugging. Default: <code>false</code> <p>Warning: Debug options can expose sensitive information in logs and should only be used in secure development environments.</p>"}, {"location": "docs/0.11.x/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/0.11.x/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.11.x/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/0.11.x/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.11.x/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:17\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db (make sure you have sqlx installed `cargo install sqlx-cli`)\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code (make sure you have cargo-sort installed, `cargo install cargo-sort`)\n# You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/0.11.x/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/0.11.x/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p> <p>lakekeeper</p> <p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol> <p>lakekeeper-bin</p> <p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/0.11.x/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/0.11.x/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/0.11.x/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/0.11.x/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/0.11.x/developer-guide/#schema-qualification-warning", "title": "\u26a0\ufe0f Schema Qualification Warning", "text": "<p>IMPORTANT: When adding new migrations, do NOT schema qualify references to any database objects. Schema qualification will break deployments that place the application in a schema different than the public one.</p> <p>\u274c Incorrect - Do NOT do this: <pre><code>-- This will break deployments in non-public schemas\nCREATE TABLE public.my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO public.my_new_table (name) VALUES ('example');\n\nALTER TABLE public.existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>\u2705 Correct - Do this instead: <pre><code>-- This will work in any schema\nCREATE TABLE my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO my_new_table (name) VALUES ('example');\n\nALTER TABLE existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>The migration system will automatically apply the migration in the correct schema context, so explicit schema qualification is unnecessary and will cause issues in deployments where Lakekeeper is deployed to a custom schema.</p>"}, {"location": "docs/0.11.x/developer-guide/#inspecting-the-db", "title": "Inspecting the db", "text": "<p>The db schema is the result of all migrations applied in order. To inspect it you can:</p> <pre><code># Assumes you set up the db as described above\n\n# Get a shell in the db's container\ndocker exec -it postgres-16 /bin/bash\n\n# Then you can connect to the db\npsql \"postgresql://postgres:postgres@localhost:5432/postgres\"\n# And inspect it, for instance by describing views or tables\n\\d+ active_tabulars\n\n# Or you can dump the entire schema\npg_dump --schema-only \"postgresql://postgres:postgres@localhost:5432/postgres\" &gt; /home/lakekeeper_schema.sql\n# Copy it out of the container and then inspect it or pass it as context to LLMs\ndocker cp postgres-16:/home/lakekeeper_schema.sql .\n</code></pre>"}, {"location": "docs/0.11.x/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.11.x/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/0.11.x/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/0.11.x/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.11.x/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/0.11.x/developer-guide/#building-the-docs-locally", "title": "Building the docs locally", "text": "<pre><code>cd site\njust serve\n</code></pre>"}, {"location": "docs/0.11.x/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on GitHub.</p>"}, {"location": "docs/0.11.x/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/0.11.x/engines/#duckdb-wasm", "title": "DuckDB WASM", "text": "<p>DuckDB WASM allows you to query Lakekeeper directly from your browser. If you are using the Lakekeeper UI, DuckDB WASM is pre-configured. To use DuckDB WASM from the Lakekeeper UI, there are two important requirements due to browser security restrictions:</p> <p>Requirements:</p> <ol> <li>Same-Origin Access: The S3 endpoint must be accessible from your browser at the same URL/origin that Lakekeeper uses to access it. For example, if Lakekeeper accesses S3 at <code>http://my-s3-endpoint:9000</code>, your browser must also be able to reach it at <code>http://my-s3-endpoint:9000</code>. This means the Docker Compose examples won't work with DuckDB WASM out of the box, as the S3 endpoint is typically only accessible within the Docker network, while your browser is not in this network.</li> <li>CORS Policy: Your S3 storage must be configured with a CORS policy that allows requests from the Lakekeeper origin. See the CORS Configuration guide for setup instructions.</li> </ol>"}, {"location": "docs/0.11.x/engines/#duckdb", "title": "DuckDB", "text": "<p>Basic setup in DuckDB:</p> <pre><code>import duckdb\n\nCATALOG_URL = \"http://localhost:8181/catalog\"\nWAREHOUSE = \"my_warehouse\"\n\n# Required if OAuth2 authentication is enabled for Lakekeeper\nCLIENT_ID = \"your-client-id\"\nCLIENT_SECRET = \"your-client-secret\"\nKEYCLOAK_TOKEN_ENDPOINT = \"http://your-idp/realms/iceberg/protocol/openid-connect/token\"\n\n# Install and load Iceberg extension\nduckdb.sql(\"INSTALL ICEBERG;\")\nduckdb.sql(\"LOAD ICEBERG;\")\n\n# Create secret for authentication\nduckdb.sql(f\"\"\"\n    CREATE SECRET lakekeeper_secret (\n        TYPE ICEBERG,\n        CLIENT_ID '{CLIENT_ID}',\n        CLIENT_SECRET '{CLIENT_SECRET}',\n        OAUTH2_SCOPE 'lakekeeper',\n        OAUTH2_SERVER_URI '{KEYCLOAK_TOKEN_ENDPOINT}'\n    )\n\"\"\")\n\n# Attach catalog\nduckdb.sql(f\"\"\"\n    ATTACH '{WAREHOUSE}' AS my_datalake (\n        TYPE ICEBERG,\n        ENDPOINT '{CATALOG_URL}',\n        SECRET lakekeeper_secret\n    )\n\"\"\")\n\n# Query tables\nduckdb.sql(\"SELECT * FROM my_datalake.my_namespace.my_table\").show()\n</code></pre>"}, {"location": "docs/0.11.x/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.11.x/engines/#starburst", "title": "Starburst", "text": "<p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in Starburst:</p> S3-CompatibleAzureGCS <p>Starburst supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <p>Please find additional configuration Options in the Starburst docs.    </p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Starburst.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for GCS, so that GCS credentials must be specified in the connector.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.11.x/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.11.x/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/0.11.x/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/0.11.x/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/0.11.x/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.11.x/engines/#risingwave", "title": "RisingWave", "text": "<p>RisingWave is a distributed SQL streaming database that is wire-compatible with PostgreSQL, designed for real-time data ingestion, processing, and querying. Unlike many other query engines that use a <code>CATALOG</code> abstraction, RisingWave connects to Lakekeeper through a <code>CONNECTION</code> object, which allows it to use Iceberg tables for sources, sinks, and internal tables.</p> <p>For a hands-on example, a Docker Compose setup is available in the RisingWave repository. You can find detailed deployment instructions in the official RisingWave documentation.</p> <p>Once you have both services running, you can create a <code>CONNECTION</code> in RisingWave to connect to Lakekeeper. The following is an example configuration. As parameters may change over time, please refer to the official RisingWave documentation for the most up-to-date and complete configuration options.</p> <pre><code>CREATE CONNECTION lakekeeper_catalog_conn\nWITH (\n    type = 'iceberg',\n    catalog.type = 'rest',\n    catalog.uri = 'http://lakekeeper:8181/catalog/',\n    warehouse.path = 'risingwave-warehouse',\n    s3.access.key = 'hummockadmin',\n    s3.secret.key = 'hummockadmin',\n    s3.path.style.access = 'true',\n    s3.endpoint = 'http://minio-0:9301',\n    s3.region = 'us-east-1'\n);\n</code></pre> <p>After creating the connection, you must set it as the default for your session to create and query internal Iceberg tables. The <code>SET</code> command applies the change to the current session only, while <code>ALTER SYSTEM</code> makes it persistent across restarts.</p> <pre><code>-- Set for the current session\nSET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n\n-- Set persistent for the system\nALTER SYSTEM SET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n</code></pre>"}, {"location": "docs/0.11.x/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/0.11.x/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/0.11.x/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/0.11.x/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/0.11.x/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/0.11.x/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/0.11.x/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/0.11.x/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/0.11.x/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/0.11.x/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/0.11.x/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For medium or large deployments, ensure the <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> and <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> are set to a higher value to allow Lakekeeper to use more connections to the database.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>When using OpenFGA, make sure that Caching is enabled. Check the OpenFGA in Production section for more information.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/0.11.x/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p> <p>// ...existing code...</p>"}, {"location": "docs/0.11.x/storage/#disabling-credential-vending-remote-signing", "title": "Disabling Credential Vending &amp; Remote Signing", "text": "<p>Lakekeeper provides multiple ways to control how credentials and remote signing information are provided to clients.</p> <p>You can disable credential vending and remote signing on a per-warehouse basis using storage profile settings. For S3 warehouses, set <code>remote-signing-enabled</code> to <code>false</code> to disable remote signing and <code>sts-enabled</code> to <code>false</code> to disable STS vended credentials. For Azure ADLS warehouses, set <code>sas-enabled</code> to <code>false</code> to disable SAS token generation. For GCS warehouses, set <code>sts-enabled</code> to <code>false</code> to disable STS token generation. When these options are disabled at the storage profile level, clients will not receive the corresponding credentials or signing information for that warehouse, regardless of the request headers. Lakekeeper downscopes vended credentials for all supported storages to the location of the table being accessed and ensures that there are no overlapping table locations within a warehouse.</p> <p>Clients can also control credential delegation per request using the <code>X-Iceberg-Access-Delegation</code> header. Lakekeeper supports the standard Iceberg REST spec values (<code>vended-credentials</code> and <code>remote-signing</code>), plus a special <code>client-managed</code> value. When set to <code>client-managed</code>, no credentials or signing information are returned, regardless of storage profile configuration. This allows clients to use their own credentials for direct storage access.</p>"}, {"location": "docs/0.11.x/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/0.11.x/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with S3-compatible storages &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Rook Ceph Rados, NetApp StorageGRID 12.0 or newer, Minio and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>When a client requests table configuration, Lakekeeper selects between remote signing and vended credentials based on the <code>X-Iceberg-Access-Delegation</code> header and storage profile settings:</p> <ul> <li>If the header is set to <code>client-managed</code>, neither credentials nor signing information are returned</li> <li>If the header specifies <code>vended-credentials</code> or <code>remote-signing</code>, that method is used if enabled in the storage profile</li> <li>If both methods are requested or neither is specified, Lakekeeper attempts to provide vended credentials first (if STS is enabled), then falls back to remote signing (if enabled)</li> <li>If both methods are disabled at the storage profile level, no credentials are returned regardless of the header value</li> </ul> <p>For maximum client compatibility, we recommend enabling both STS and remote signing when your S3 storage supports it.</p> <p>For some older remote signing clients that cannot handle table-specific remote signing endpoint locations, Lakekeeper needs to identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/0.11.x/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>remote-signing-enabled</code> Boolean No <code>true</code> Whether to enable remote signing for S3 requests. When disabled, clients cannot use remote signing for this storage profile even if STS is disabled. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key. <code>legacy-md5-behavior</code> Boolean No <code>false</code> A flag to enable the legacy behavior of using MD5 checksums for operations that require checksums."}, {"location": "docs/0.11.x/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/0.11.x/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/0.11.x/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/0.11.x/storage/#cors-configuration", "title": "CORS Configuration", "text": "<p>For browser-based access to S3 buckets (required for DuckDB WASM), you need to configure CORS (Cross-Origin Resource Sharing) on your S3 bucket.</p> <p>To configure CORS for your S3 bucket:</p> <ol> <li>In the AWS S3 Configuration Menu, klick on the name of your bucket</li> <li>Choose Permissions Tab</li> <li>In the Cross-origin resource sharing (CORS) section, choose Edit</li> <li>In the CORS configuration editor text box, type or copy and paste a new CORS configuration, or edit an existing configuration. The CORS configuration is a JSON file. The text that you type in the editor must be valid JSON. See below for an example.</li> <li>Choose Save changes</li> </ol> <p>Example CORS policy:</p> <pre><code>[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"POST\",\n            \"PUT\",\n            \"DELETE\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"https://lakekeeper.example.com\"\n        ],\n        \"ExposeHeaders\": []\n    }\n]\n</code></pre> <p>Replace <code>https://lakekeeper.example.com</code> with the origin where your Lakekeeper instance is hosted.</p>"}, {"location": "docs/0.11.x/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/0.11.x/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.11.x/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. weur&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code>. Use a Data Location Hint as region.</p>"}, {"location": "docs/0.11.x/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/0.11.x/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>sas-enabled</code> Boolean No <code>true</code> Whether to enable SAS (Shared Access Signature) token generation for Azure Data Lake Storage. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.11.x/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/0.11.x/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/0.11.x/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>sts-enabled</code> Boolean No <code>true</code> Whether to enable STS (Security Token Service) downscoped token generation for GCS. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/0.11.x/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/0.11.x/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/0.11.x/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/0.11.x/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/0.11.x/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p> <p>Link to Expire Snapshots</p>"}, {"location": "docs/0.11.x/table-maintenance/#expire-snapshots", "title": "Expire Snapshots", "text": "<p>Lakekeeper automatically expires old table snapshots based on configurable age and retention policies. This helps manage storage costs and performance by removing outdated snapshot metadata and associated data files.</p> <p>Expire snapshots can be configured per warehouse and optionally overridden at the table level using Iceberg table properties.</p>"}, {"location": "docs/0.11.x/table-maintenance/#configuration", "title": "Configuration", "text": "<p>Configuration can be set via the Management UI or REST API endpoints:</p> <ul> <li>GET <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> <li>POST <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> </ul> Parameter Type Default Description <code>enable-expire-snapshots</code> boolean <code>false</code> Enable automatic snapshot expiration for all tables in the warehouse. Can be overridden per table with <code>lakekeeper.history.expire.enabled</code> <code>max-snapshot-age-ms</code> integer <code>432000000</code> (5 days) Maximum age of snapshots in milliseconds before expiration. Override per table with <code>history.expire.max-snapshot-age-ms</code> <code>min-snapshots-to-keep</code> integer <code>1</code> Minimum snapshots to retain on each table branch. Override per table with <code>history.expire.min-snapshots-to-keep</code> <code>min-snapshots-to-expire</code> integer <code>20</code> Minimum snapshots required before expiration job is scheduled (prevents expensive jobs for few snapshots). Override per table with <code>lakekeeper.history.expire.min-snapshots-to-expire</code> <code>max-ref-age-ms</code> integer <code>9223372036854775807</code> (no limit) Maximum age for snapshot references (except main branch). Main branch references never expire"}, {"location": "docs/0.11.x/table-maintenance/#table-level-overrides", "title": "Table-Level Overrides", "text": "<p>Individual tables can override warehouse settings using these Iceberg table properties:</p> <ul> <li><code>lakekeeper.history.expire.enabled</code> - Enable/disable for specific table</li> <li><code>history.expire.max-snapshot-age-ms</code> - Custom max age for table snapshots  </li> <li><code>history.expire.min-snapshots-to-keep</code> - Custom minimum retention for table</li> <li><code>lakekeeper.history.expire.min-snapshots-to-expire</code> - Custom threshold for table</li> </ul> <p>Note</p> <p>Tables with <code>gc.enabled=false</code> are excluded from automatic expiration regardless of other settings.</p>"}, {"location": "docs/0.11.x/table-maintenance/#production-deployment", "title": "Production Deployment", "text": "<p>For production workloads, we recommend running expire snapshots workers in dedicated pods to avoid impacting REST API performance. This can be achieved by:</p> <ol> <li>API pods: Set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS=0</code> to disable workers</li> <li>Worker pods: Use default worker configuration (2 workers) to handle expire snapshots tasks or set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> to desired number of workers</li> </ol>"}, {"location": "docs/0.11.x/table-maintenance/#task-scheduling", "title": "Task Scheduling", "text": "<p>Expire snapshots tasks are intelligently scheduled immediately after table commits when needed, eliminating the overhead of cron-based polling. This ensures timely cleanup while maintaining optimal performance.</p>"}, {"location": "docs/0.11.x/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/0.11.x/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/0.11.x/api/management-plus/", "title": "Management <span class=\"lkp\"></span>", "text": ""}, {"location": "docs/0.11.x/api/management/", "title": "Management (Core)", "text": ""}, {"location": "docs/0.11.x/docs/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.11.x/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding GitHub Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/0.11.x/docs/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/0.11.x/docs/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/0.11.x/docs/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.11.x/docs/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.11.x/docs/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.11.x/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.11.x/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/0.11.x/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.11.x/docs/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.11.x/docs/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/0.11.x/docs/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/0.11.x/docs/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/0.11.x/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> <p>The Lakekeeper Helm Chart creates the required binding by default.</p> <p>Applications running in Kubernetes pods can now authenticate using the service account token, which is typically mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Simply read this token and include it in the <code>Authorization</code> header.</p> <p>Example with CURL: <pre><code>curl -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     http://my-lakekeeper:8181/catalog/v1/config\n</code></pre></p> <p>Example with Spark: <pre><code>spark-submit \\\n  --conf spark.sql.catalog.lakekeeper.token=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n  --conf spark.sql.catalog.lakekeeper.uri=\"http://my-lakekeeper:8181/catalog\" \\\n  my-spark-job.py\n</code></pre></p> <p>User identities appear in Lakekeeper as <code>k8s~&lt;namespace&gt;~&lt;service-account-name&gt;</code>.</p>"}, {"location": "docs/0.11.x/docs/authorization/", "title": "Authorization", "text": ""}, {"location": "docs/0.11.x/docs/authorization/#overview", "title": "Overview", "text": "<p>Authentication verifies who you are, while authorization determines what you can do.</p> <p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper currently supports the following Authorizers:</p> <ul> <li>AllowAll: A simple authorizer that allows all requests. This is mainly intended for development and testing purposes.</li> <li>OpenFGA: A fine-grained authorization system based on the CNCF project OpenFGA. Please find more information in the Authorization with OpenFGA section. OpenFGA requires an additional OpenFGA service to be deployed (this is included in our self-contained examples and our helm charts).</li> <li>Cedar: An enterprise-grade policy-based authorization system based on Cedar. The cedar authorizer is built into Lakekeeper and requires no additional external services. Please find more information in the Authorization with Cedar section.</li> <li>Custom: Lakekeeper supports custom authorizers via the <code>Authorizer</code> trait.</li> </ul>"}, {"location": "docs/0.11.x/docs/authorization/#authorization-with-openfga", "title": "Authorization with OpenFGA", "text": "<p>Lakekeeper can use OpenFGA to store and evaluate permissions. OpenFGA provides bi-directional inheritance, which is key for managing hierarchical namespaces in modern lakehouses. For query engines like Trino, Lakekeeper's OPA bridge translates OpenFGA permissions into Open Policy Agent (OPA) format. See the OPA Bridge Guide for details.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/0.11.x/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on GitHub. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.11.x/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.11.x/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.11.x/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.11.x/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.11.x/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. They can delegate the <code>data_admin</code> role they already hold (for example to team members), but they do not have general grant or ownership administration capabilities.</p>"}, {"location": "docs/0.11.x/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.11.x/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.11.x/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.11.x/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.11.x/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.11.x/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.11.x/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.11.x/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.11.x/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>Top-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.11.x/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.11.x/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.11.x/docs/authorization/#openfga-in-production", "title": "OpenFGA in Production", "text": "<p>When deploying OpenFGA in production environments, ensure you follow the OpenFGA Production Checklist.</p> <p>Lakekeeper includes Query Consistency specifications with each authorization request to OpenFGA. For most operations, <code>MINIMIZE_LATENCY</code> consistency provides optimal performance while maintaining sufficient data consistency guarantees.</p> <p>For medium to large-scale deployments, we strongly recommend enabling caching in OpenFGA and increasing the database connection pool limits. These optimizations significantly reduce database load and improve authorization latency. Configure the following environment variables in OpenFGA (written for version 1.10). You may increase the number of connections further if your database deployment can handle additional connections:</p> <pre><code>OPENFGA_DATASTORE_MAX_OPEN_CONNS=200\nOPENFGA_DATASTORE_MAX_IDLE_CONNS=100\nOPENFGA_CACHE_CONTROLLER_ENABLED=true\nOPENFGA_CHECK_QUERY_CACHE_ENABLED=true\nOPENFGA_CHECK_ITERATOR_CACHE_ENABLED=true\n</code></pre>"}, {"location": "docs/0.11.x/docs/authorization/#authorization-with-cedar", "title": "Authorization with Cedar", "text": "<p>Cedar is an enterprise-grade, policy-based authorization system built into Lakekeeper that requires no external services. Cedar uses a declarative policy language to define access controls, making it ideal for organizations that prefer infrastructure-as-code approaches to authorization management.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/0.11.x/docs/authorization/#schema-and-entity-model", "title": "Schema and Entity Model", "text": "<p>For each authorization request, Lakekeeper provides the complete entity hierarchy from the requested resource up to the server level. This ensures policies have full context for making authorization decisions.</p> <p>When a user queries table <code>ns1.ns2.table1</code> in warehouse <code>wh-1</code> within project <code>my-project</code>, Cedar receives the following entities:</p> <ul> <li><code>Server</code> (root)</li> <li><code>Project::\"my-project\"</code></li> <li><code>Warehouse::\"wh-1\"</code> (parent: <code>my-project</code>)</li> <li><code>Namespace::\"ns1\"</code> (parent: <code>wh-1</code>)</li> <li><code>Namespace::\"ns2\"</code> (parent: <code>ns1</code>)</li> <li><code>Table::\"table1\"</code> (parent: <code>ns2</code>)</li> </ul> <p>This hierarchical context allows policies to reference any level in the path. For example, you can write policies that grant access based on the warehouse name, namespace hierarchy, or specific table properties.</p> <p>The Lakekeeper Cedar schema defines all available entity types, attributes, and actions. All loaded entities and policies are validated against this schema on startup and refresh. You can download the schema here: lakekeeper.cedarschema or find it on GitHub.</p> <p>Important: Lakekeeper does not provide Roles as built-in entities. Roles must be defined as custom entities in your entity JSON files.</p>"}, {"location": "docs/0.11.x/docs/authorization/#policy-examples", "title": "Policy Examples", "text": "<p>Grant admin access to a specific user: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action,\n    resource\n);\n</code></pre></p> <p>Role-based warehouse access: <pre><code>// Grant full access to all entities in a warehouse with name \"wh-1\"\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"NamespaceActions\",\n               Lakekeeper::Action::\"TableActions\",\n               Lakekeeper::Action::\"ViewActions\"],\n    resource\n)\nwhen { resource.warehouse.name == \"wh-1\" };\n\n// Allow modification of the warehouse itself\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"WarehouseModifyActions\"],\n    resource\n)\nwhen { resource.name == \"wh-1\" };\n</code></pre></p> <p>Table read access for all tables in the <code>analytics</code> namespace of warehouse <code>wh-1</code>: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action in [Lakekeeper::Action::\"TableSelectActions\"],\n    resource\n)\nwhen {\n    resource.namespace.name == \"analytics\" &amp;&amp;\n    resource.warehouse.name == \"wh-1\"\n};\n</code></pre></p>"}, {"location": "docs/0.11.x/docs/authorization/#entity-definition-example", "title": "Entity Definition Example", "text": "<p>Define roles and assign users to them using JSON entity files:</p> <pre><code>[\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::User\",\n            \"id\": \"oidc~90471f73-e338-4032-9a6b-1e021cc3cb1e\"\n        },\n        \"attrs\": {\n            \"display_name\": \"machine-user-1\"\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"data-engineering\"\n            }\n        ]\n    },\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::Role\",\n            \"id\": \"data-engineering\"\n        },\n        \"attrs\": {\n            \"name\": \"DataEngineering\",\n            \"project\": {\n                \"__entity\": {\n                    \"type\": \"Lakekeeper::Project\",\n                    \"id\": \"00000000-0000-0000-0000-000000000000\"\n                }\n            }\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"warehouse-1-admins\"\n            }\n        ]\n    }\n]\n</code></pre>"}, {"location": "docs/0.11.x/docs/authorization/#policy-and-entity-management", "title": "Policy and Entity Management", "text": "<p>Startup Behavior:</p> <ul> <li>All policy and entity files are loaded and validated against the Cedar schema</li> <li>If any file is unreadable or invalid, Lakekeeper fails to start with an error</li> </ul> <p>This ensures that authorization policies are always valid before serving requests</p> <p>Refresh Behavior: Configure automatic policy refresh using <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> (default: 5 seconds):</p> <ol> <li>Change Detection: Lightweight checks monitor ConfigMap versions and file timestamps</li> <li>Reload on Change: Modified entity or policy files trigger a full reload of all files to guarantee consistency</li> <li>Atomic Updates: The in-memory store is only updated if all files reload successfully</li> <li>Error Handling: If any reload fails, the previous configuration is retained, an error is logged, and health checks report unhealthy status</li> </ol> <p>This approach ensures that authorization policies remain consistent and that partial updates never compromise security.</p>"}, {"location": "docs/0.11.x/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.11.x/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.11.x/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper supports different Authorizers. Please refer to the Authorization Documentation for more information.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.11.x/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.11.x/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/0.11.x/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.11.x/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.11.x/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.11.x/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.11.x/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.11.x/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding GitHub Issue if this would be of interest to you.</p>"}, {"location": "docs/0.11.x/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/0.11.x/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.11.x/docs/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/0.11.x/docs/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/0.11.x/docs/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/0.11.x/docs/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/0.11.x/docs/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/0.11.x/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.11.x/docs/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/0.11.x/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/0.11.x/docs/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/0.11.x/docs/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/0.11.x/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/0.11.x/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.11.x/docs/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option. <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> 2 Number of workers spawned that work on expire Snapshots tasks. See Expire Snapshots Docs for more information."}, {"location": "docs/0.11.x/docs/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/0.11.x/docs/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/0.11.x/docs/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/0.11.x/docs/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/0.11.x/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> <code>resource_access.lakekeeper.roles</code> Specify the claim to use in provided JWT tokens to extract roles. The field should contain an array of strings or a single string. Supports nested claims using dot notation, e.g., \"resource_access.account.roles\". Currently only has an effect when using the Cedar Authorizer. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/0.11.x/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> or <code>cedar</code> is chosen, additional parameters are required (see below). The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>, <code>cedar</code>]"}, {"location": "docs/0.11.x/docs/configuration/#openfga", "title": "OpenFGA", "text": "Variable Example Description <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/0.11.x/docs/configuration/#cedar", "title": "Cedar", "text": "Variable Example Description <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__LOCAL_FILES</code> <code>[/path/to/policies1.cedar,/path/to/policies2.cedar]</code> List of local file paths containing Cedar policies in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__LOCAL_FILES</code> <code>[/path/to/entities1.json,/path/to/entities2.json]</code> List of local JSON file paths containing additional Cedar entities (typically roles). <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedar</code> is treated as a policy source in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedarentities.json</code> is treated as an entity source. <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> <code>5</code> Refresh interval in seconds for reloading policies and entities from Kubernetes ConfigMaps and local files. Default: <code>5</code> seconds. See Cedar Authorization for more information. <code>LAKEKEEPER__CEDAR__EXTERNALLY_MANAGED_USER_AND_ROLES</code> <code>false</code> When set to <code>true</code>, Lakekeeper expects all roles and users to be managed externally via entities.json and does not extract <code>Lakekeeper::Role</code> or <code>Lakekeeper::User</code> entities from the user's token. When set to <code>false</code> (default), Lakekeeper automatically provides <code>Lakekeeper::Role</code> and <code>Lakekeeper::User</code> entities to Cedar based on information extracted from the user's token. When set to <code>false</code>, ensure <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> is configured to specify which claim in the token contains role information. <code>LAKEKEEPER__CEDAR__SCHEMA_FILE</code> <code>/path/to/custom/schema.cedarschema</code> Optional path to a custom Cedar schema file. If provided, this schema will be used instead of the embedded default schema. Useful for extending or customizing the Cedar schema. Compatibility with the Lakekeeper schema must be ensured for all entities provided by Lakekeeper (Server, Project, Namespace, Table, View. User &amp; Role if externally managed roles is <code>false</code>). <p>Debug configurations for Cedar</p> Variable Example Description <code>LAKEKEEPER__CEDAR__DEBUG__LOG_ENTITIES</code> <code>false</code> If <code>true</code>, logs all internal entities (excluding externally managed entities) for each authorization request at debug level. This is useful for debugging authorization issues but can be verbose and impacts performance. Logging only occurs when both this flag is <code>true</code> AND debug logging is enabled (<code>RUST_LOG=debug</code>). Default: <code>false</code>."}, {"location": "docs/0.11.x/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/0.11.x/docs/configuration/#caching", "title": "Caching", "text": "<p>Lakekeeper uses in-memory caches to speed up certain operations.</p> <p>Short-Term Credentials (STC) Cache</p> <p>When Lakekeeper vends short-term credentials for cloud storage access (S3 STS, Azure SAS tokens, or GCP access tokens), these credentials can be cached to reduce load on cloud identity services and improve response times.</p> Variable Example Description <code>LAKEKEEPER__CACHE__STC__ENABLED</code> <code>true</code> Enable or disable the short-term credentials cache. Default: <code>true</code> <code>LAKEKEEPER__CACHE__STC__CAPACITY</code> <code>10000</code> Maximum number of credential entries to cache. Default: <code>10000</code> <p>Expiry Mechanism: Cached credentials automatically expire based on the validity period of the underlying cloud credentials. Lakekeeper caches credentials for half their lifetime (e.g., if GCP STS returns credentials valid for 1 hour, they're cached for 30 minutes) with a maximum cache duration of 1 hour. This ensures credentials remain fresh while reducing unnecessary identity service calls.</p> <p>Metrics: The STC cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_stc_cache_size{cache_type=\"stc\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_stc_cache_hits_total{cache_type=\"stc\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_stc_cache_misses_total{cache_type=\"stc\"}</code>: Total number of cache misses</li> </ul> <p>Warehouse Cache</p> <p>Caches warehouse metadata to reduce database queries for warehouse lookups.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__WAREHOUSE__ENABLED</code> boolean <code>true</code> Enable/disable warehouse caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__CAPACITY</code> integer <code>1000</code> Maximum number of warehouses to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to Storage Profile may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. Warehouse metadata is guaranteed to be fresh for load table &amp; view operations also for multi-worker deployments.</p> <p>Metrics: The Warehouse cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_warehouse_cache_size{cache_type=\"warehouse\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_warehouse_cache_hits_total{cache_type=\"warehouse\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_warehouse_cache_misses_total{cache_type=\"warehouse\"}</code>: Total number of cache misses</li> </ul> <p>Namespace Cache</p> <p>Caches namespace metadata and hierarchies to reduce database queries for namespace lookups. Namespace lookups are also required for table &amp; view operations.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__NAMESPACE__ENABLED</code> boolean <code>true</code> Enable/disable namespace caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__NAMESPACE__CAPACITY</code> integer <code>1000</code> Maximum number of namespaces to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__NAMESPACE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to namespace properties may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. The namespace cache stores both individual namespaces and their parent hierarchies for efficient lookups.</p> <p>Metrics: The Namespace cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_namespace_cache_size{cache_type=\"namespace\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_namespace_cache_hits_total{cache_type=\"namespace\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_namespace_cache_misses_total{cache_type=\"namespace\"}</code>: Total number of cache misses</li> </ul> <p>Secrets Cache</p> <p>Caches storage secrets to reduce load on the secret store. Since Lakekeeper never updates secrets, long TTLs can significantly increase resilience against secret store outages, especially when the secret store is external to the main database backend.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__SECRETS__ENABLED</code> boolean <code>true</code> Enable/disable secrets caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__SECRETS__CAPACITY</code> integer <code>500</code> Maximum number of secrets to cache. Default: <code>500</code> <code>LAKEKEEPER__CACHE__SECRETS__TIME_TO_LIVE_SECS</code> integer <code>600</code> Time-to-live for cache entries in seconds. Default: <code>600</code> (10 minutes) <p>Metrics: The Secrets cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_secrets_cache_size{cache_type=\"secrets\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_secrets_cache_hits_total{cache_type=\"secrets\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_secrets_cache_misses_total{cache_type=\"secrets\"}</code>: Total number of cache misses</li> </ul>"}, {"location": "docs/0.11.x/docs/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/0.11.x/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.11.x/docs/configuration/#debug", "title": "Debug", "text": "<p>Lakekeeper provides debugging options to help troubleshoot issues during development. These options should not be enabled in production environments as they can expose sensitive data and impact performance.</p> Variable Example Description <code>LAKEKEEPER__DEBUG__LOG_REQUEST_BODIES</code> <code>true</code> If set to <code>true</code>, Lakekeeper will log all incoming and outgoing request bodies at debug level. This is useful for debugging API interactions but should never be enabled in production as it can expose sensitive data (credentials, tokens, etc.) and significantly impact performance. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__MIGRATE_BEFORE_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper waits for the DB (30s) and runs migrations when <code>serve</code> is called. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__AUTO_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper will automatically start the server when no subcommand is provided (i.e., when running the binary without arguments). This is useful for development environments to quickly start the server without explicitly specifying the <code>serve</code> command. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__EXTENDED_LOGS</code> <code>false</code> Controls whether file names and line numbers are included in JSON log output. When set to <code>false</code>, these fields are omitted for cleaner logs. When set to <code>true</code>, each log entry includes <code>filename</code> and <code>line_number</code> fields for easier debugging. Default: <code>false</code> <p>Warning: Debug options can expose sensitive information in logs and should only be used in secure development environments.</p>"}, {"location": "docs/0.11.x/docs/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/0.11.x/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:17\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db (make sure you have sqlx installed `cargo install sqlx-cli`)\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code (make sure you have cargo-sort installed, `cargo install cargo-sort`)\n# You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/0.11.x/docs/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p> <p>lakekeeper</p> <p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol> <p>lakekeeper-bin</p> <p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#schema-qualification-warning", "title": "\u26a0\ufe0f Schema Qualification Warning", "text": "<p>IMPORTANT: When adding new migrations, do NOT schema qualify references to any database objects. Schema qualification will break deployments that place the application in a schema different than the public one.</p> <p>\u274c Incorrect - Do NOT do this: <pre><code>-- This will break deployments in non-public schemas\nCREATE TABLE public.my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO public.my_new_table (name) VALUES ('example');\n\nALTER TABLE public.existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>\u2705 Correct - Do this instead: <pre><code>-- This will work in any schema\nCREATE TABLE my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO my_new_table (name) VALUES ('example');\n\nALTER TABLE existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>The migration system will automatically apply the migration in the correct schema context, so explicit schema qualification is unnecessary and will cause issues in deployments where Lakekeeper is deployed to a custom schema.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#inspecting-the-db", "title": "Inspecting the db", "text": "<p>The db schema is the result of all migrations applied in order. To inspect it you can:</p> <pre><code># Assumes you set up the db as described above\n\n# Get a shell in the db's container\ndocker exec -it postgres-16 /bin/bash\n\n# Then you can connect to the db\npsql \"postgresql://postgres:postgres@localhost:5432/postgres\"\n# And inspect it, for instance by describing views or tables\n\\d+ active_tabulars\n\n# Or you can dump the entire schema\npg_dump --schema-only \"postgresql://postgres:postgres@localhost:5432/postgres\" &gt; /home/lakekeeper_schema.sql\n# Copy it out of the container and then inspect it or pass it as context to LLMs\ndocker cp postgres-16:/home/lakekeeper_schema.sql .\n</code></pre>"}, {"location": "docs/0.11.x/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.11.x/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/0.11.x/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/0.11.x/docs/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/0.11.x/docs/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/0.11.x/docs/developer-guide/#building-the-docs-locally", "title": "Building the docs locally", "text": "<pre><code>cd site\njust serve\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on GitHub.</p>"}, {"location": "docs/0.11.x/docs/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/0.11.x/docs/engines/#duckdb-wasm", "title": "DuckDB WASM", "text": "<p>DuckDB WASM allows you to query Lakekeeper directly from your browser. If you are using the Lakekeeper UI, DuckDB WASM is pre-configured. To use DuckDB WASM from the Lakekeeper UI, there are two important requirements due to browser security restrictions:</p> <p>Requirements:</p> <ol> <li>Same-Origin Access: The S3 endpoint must be accessible from your browser at the same URL/origin that Lakekeeper uses to access it. For example, if Lakekeeper accesses S3 at <code>http://my-s3-endpoint:9000</code>, your browser must also be able to reach it at <code>http://my-s3-endpoint:9000</code>. This means the Docker Compose examples won't work with DuckDB WASM out of the box, as the S3 endpoint is typically only accessible within the Docker network, while your browser is not in this network.</li> <li>CORS Policy: Your S3 storage must be configured with a CORS policy that allows requests from the Lakekeeper origin. See the CORS Configuration guide for setup instructions.</li> </ol>"}, {"location": "docs/0.11.x/docs/engines/#duckdb", "title": "DuckDB", "text": "<p>Basic setup in DuckDB:</p> <pre><code>import duckdb\n\nCATALOG_URL = \"http://localhost:8181/catalog\"\nWAREHOUSE = \"my_warehouse\"\n\n# Required if OAuth2 authentication is enabled for Lakekeeper\nCLIENT_ID = \"your-client-id\"\nCLIENT_SECRET = \"your-client-secret\"\nKEYCLOAK_TOKEN_ENDPOINT = \"http://your-idp/realms/iceberg/protocol/openid-connect/token\"\n\n# Install and load Iceberg extension\nduckdb.sql(\"INSTALL ICEBERG;\")\nduckdb.sql(\"LOAD ICEBERG;\")\n\n# Create secret for authentication\nduckdb.sql(f\"\"\"\n    CREATE SECRET lakekeeper_secret (\n        TYPE ICEBERG,\n        CLIENT_ID '{CLIENT_ID}',\n        CLIENT_SECRET '{CLIENT_SECRET}',\n        OAUTH2_SCOPE 'lakekeeper',\n        OAUTH2_SERVER_URI '{KEYCLOAK_TOKEN_ENDPOINT}'\n    )\n\"\"\")\n\n# Attach catalog\nduckdb.sql(f\"\"\"\n    ATTACH '{WAREHOUSE}' AS my_datalake (\n        TYPE ICEBERG,\n        ENDPOINT '{CATALOG_URL}',\n        SECRET lakekeeper_secret\n    )\n\"\"\")\n\n# Query tables\nduckdb.sql(\"SELECT * FROM my_datalake.my_namespace.my_table\").show()\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#starburst", "title": "Starburst", "text": "<p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in Starburst:</p> S3-CompatibleAzureGCS <p>Starburst supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <p>Please find additional configuration Options in the Starburst docs.    </p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Starburst.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for GCS, so that GCS credentials must be specified in the connector.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/0.11.x/docs/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.11.x/docs/engines/#risingwave", "title": "RisingWave", "text": "<p>RisingWave is a distributed SQL streaming database that is wire-compatible with PostgreSQL, designed for real-time data ingestion, processing, and querying. Unlike many other query engines that use a <code>CATALOG</code> abstraction, RisingWave connects to Lakekeeper through a <code>CONNECTION</code> object, which allows it to use Iceberg tables for sources, sinks, and internal tables.</p> <p>For a hands-on example, a Docker Compose setup is available in the RisingWave repository. You can find detailed deployment instructions in the official RisingWave documentation.</p> <p>Once you have both services running, you can create a <code>CONNECTION</code> in RisingWave to connect to Lakekeeper. The following is an example configuration. As parameters may change over time, please refer to the official RisingWave documentation for the most up-to-date and complete configuration options.</p> <pre><code>CREATE CONNECTION lakekeeper_catalog_conn\nWITH (\n    type = 'iceberg',\n    catalog.type = 'rest',\n    catalog.uri = 'http://lakekeeper:8181/catalog/',\n    warehouse.path = 'risingwave-warehouse',\n    s3.access.key = 'hummockadmin',\n    s3.secret.key = 'hummockadmin',\n    s3.path.style.access = 'true',\n    s3.endpoint = 'http://minio-0:9301',\n    s3.region = 'us-east-1'\n);\n</code></pre> <p>After creating the connection, you must set it as the default for your session to create and query internal Iceberg tables. The <code>SET</code> command applies the change to the current session only, while <code>ALTER SYSTEM</code> makes it persistent across restarts.</p> <pre><code>-- Set for the current session\nSET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n\n-- Set persistent for the system\nALTER SYSTEM SET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n</code></pre>"}, {"location": "docs/0.11.x/docs/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/0.11.x/docs/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/0.11.x/docs/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/0.11.x/docs/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/0.11.x/docs/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/0.11.x/docs/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/0.11.x/docs/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/0.11.x/docs/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/0.11.x/docs/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/0.11.x/docs/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/0.11.x/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For medium or large deployments, ensure the <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> and <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> are set to a higher value to allow Lakekeeper to use more connections to the database.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>When using OpenFGA, make sure that Caching is enabled. Check the OpenFGA in Production section for more information.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/0.11.x/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p> <p>// ...existing code...</p>"}, {"location": "docs/0.11.x/docs/storage/#disabling-credential-vending-remote-signing", "title": "Disabling Credential Vending &amp; Remote Signing", "text": "<p>Lakekeeper provides multiple ways to control how credentials and remote signing information are provided to clients.</p> <p>You can disable credential vending and remote signing on a per-warehouse basis using storage profile settings. For S3 warehouses, set <code>remote-signing-enabled</code> to <code>false</code> to disable remote signing and <code>sts-enabled</code> to <code>false</code> to disable STS vended credentials. For Azure ADLS warehouses, set <code>sas-enabled</code> to <code>false</code> to disable SAS token generation. For GCS warehouses, set <code>sts-enabled</code> to <code>false</code> to disable STS token generation. When these options are disabled at the storage profile level, clients will not receive the corresponding credentials or signing information for that warehouse, regardless of the request headers. Lakekeeper downscopes vended credentials for all supported storages to the location of the table being accessed and ensures that there are no overlapping table locations within a warehouse.</p> <p>Clients can also control credential delegation per request using the <code>X-Iceberg-Access-Delegation</code> header. Lakekeeper supports the standard Iceberg REST spec values (<code>vended-credentials</code> and <code>remote-signing</code>), plus a special <code>client-managed</code> value. When set to <code>client-managed</code>, no credentials or signing information are returned, regardless of storage profile configuration. This allows clients to use their own credentials for direct storage access.</p>"}, {"location": "docs/0.11.x/docs/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/0.11.x/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with S3-compatible storages &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Rook Ceph Rados, NetApp StorageGRID 12.0 or newer, Minio and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>When a client requests table configuration, Lakekeeper selects between remote signing and vended credentials based on the <code>X-Iceberg-Access-Delegation</code> header and storage profile settings:</p> <ul> <li>If the header is set to <code>client-managed</code>, neither credentials nor signing information are returned</li> <li>If the header specifies <code>vended-credentials</code> or <code>remote-signing</code>, that method is used if enabled in the storage profile</li> <li>If both methods are requested or neither is specified, Lakekeeper attempts to provide vended credentials first (if STS is enabled), then falls back to remote signing (if enabled)</li> <li>If both methods are disabled at the storage profile level, no credentials are returned regardless of the header value</li> </ul> <p>For maximum client compatibility, we recommend enabling both STS and remote signing when your S3 storage supports it.</p> <p>For some older remote signing clients that cannot handle table-specific remote signing endpoint locations, Lakekeeper needs to identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/0.11.x/docs/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>remote-signing-enabled</code> Boolean No <code>true</code> Whether to enable remote signing for S3 requests. When disabled, clients cannot use remote signing for this storage profile even if STS is disabled. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key. <code>legacy-md5-behavior</code> Boolean No <code>false</code> A flag to enable the legacy behavior of using MD5 checksums for operations that require checksums."}, {"location": "docs/0.11.x/docs/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/0.11.x/docs/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/0.11.x/docs/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/0.11.x/docs/storage/#cors-configuration", "title": "CORS Configuration", "text": "<p>For browser-based access to S3 buckets (required for DuckDB WASM), you need to configure CORS (Cross-Origin Resource Sharing) on your S3 bucket.</p> <p>To configure CORS for your S3 bucket:</p> <ol> <li>In the AWS S3 Configuration Menu, klick on the name of your bucket</li> <li>Choose Permissions Tab</li> <li>In the Cross-origin resource sharing (CORS) section, choose Edit</li> <li>In the CORS configuration editor text box, type or copy and paste a new CORS configuration, or edit an existing configuration. The CORS configuration is a JSON file. The text that you type in the editor must be valid JSON. See below for an example.</li> <li>Choose Save changes</li> </ol> <p>Example CORS policy:</p> <pre><code>[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"POST\",\n            \"PUT\",\n            \"DELETE\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"https://lakekeeper.example.com\"\n        ],\n        \"ExposeHeaders\": []\n    }\n]\n</code></pre> <p>Replace <code>https://lakekeeper.example.com</code> with the origin where your Lakekeeper instance is hosted.</p>"}, {"location": "docs/0.11.x/docs/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/0.11.x/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.11.x/docs/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. weur&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code>. Use a Data Location Hint as region.</p>"}, {"location": "docs/0.11.x/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/0.11.x/docs/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>sas-enabled</code> Boolean No <code>true</code> Whether to enable SAS (Shared Access Signature) token generation for Azure Data Lake Storage. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.11.x/docs/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/0.11.x/docs/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/0.11.x/docs/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>sts-enabled</code> Boolean No <code>true</code> Whether to enable STS (Security Token Service) downscoped token generation for GCS. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/0.11.x/docs/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/0.11.x/docs/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/0.11.x/docs/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/0.11.x/docs/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/0.11.x/docs/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p> <p>Link to Expire Snapshots</p>"}, {"location": "docs/0.11.x/docs/table-maintenance/#expire-snapshots", "title": "Expire Snapshots", "text": "<p>Lakekeeper automatically expires old table snapshots based on configurable age and retention policies. This helps manage storage costs and performance by removing outdated snapshot metadata and associated data files.</p> <p>Expire snapshots can be configured per warehouse and optionally overridden at the table level using Iceberg table properties.</p>"}, {"location": "docs/0.11.x/docs/table-maintenance/#configuration", "title": "Configuration", "text": "<p>Configuration can be set via the Management UI or REST API endpoints:</p> <ul> <li>GET <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> <li>POST <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> </ul> Parameter Type Default Description <code>enable-expire-snapshots</code> boolean <code>false</code> Enable automatic snapshot expiration for all tables in the warehouse. Can be overridden per table with <code>lakekeeper.history.expire.enabled</code> <code>max-snapshot-age-ms</code> integer <code>432000000</code> (5 days) Maximum age of snapshots in milliseconds before expiration. Override per table with <code>history.expire.max-snapshot-age-ms</code> <code>min-snapshots-to-keep</code> integer <code>1</code> Minimum snapshots to retain on each table branch. Override per table with <code>history.expire.min-snapshots-to-keep</code> <code>min-snapshots-to-expire</code> integer <code>20</code> Minimum snapshots required before expiration job is scheduled (prevents expensive jobs for few snapshots). Override per table with <code>lakekeeper.history.expire.min-snapshots-to-expire</code> <code>max-ref-age-ms</code> integer <code>9223372036854775807</code> (no limit) Maximum age for snapshot references (except main branch). Main branch references never expire"}, {"location": "docs/0.11.x/docs/table-maintenance/#table-level-overrides", "title": "Table-Level Overrides", "text": "<p>Individual tables can override warehouse settings using these Iceberg table properties:</p> <ul> <li><code>lakekeeper.history.expire.enabled</code> - Enable/disable for specific table</li> <li><code>history.expire.max-snapshot-age-ms</code> - Custom max age for table snapshots  </li> <li><code>history.expire.min-snapshots-to-keep</code> - Custom minimum retention for table</li> <li><code>lakekeeper.history.expire.min-snapshots-to-expire</code> - Custom threshold for table</li> </ul> <p>Note</p> <p>Tables with <code>gc.enabled=false</code> are excluded from automatic expiration regardless of other settings.</p>"}, {"location": "docs/0.11.x/docs/table-maintenance/#production-deployment", "title": "Production Deployment", "text": "<p>For production workloads, we recommend running expire snapshots workers in dedicated pods to avoid impacting REST API performance. This can be achieved by:</p> <ol> <li>API pods: Set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS=0</code> to disable workers</li> <li>Worker pods: Use default worker configuration (2 workers) to handle expire snapshots tasks or set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> to desired number of workers</li> </ol>"}, {"location": "docs/0.11.x/docs/table-maintenance/#task-scheduling", "title": "Task Scheduling", "text": "<p>Expire snapshots tasks are intelligently scheduled immediately after table commits when needed, eliminating the overhead of cron-based polling. This ensures timely cleanup while maintaining optimal performance.</p>"}, {"location": "docs/0.11.x/docs/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/0.11.x/docs/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/0.11.x/docs/api/management-plus/", "title": "Management plus", "text": ""}, {"location": "docs/0.11.x/docs/api/management/", "title": "Management", "text": ""}, {"location": "docs/0.5.0/docs/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself, as this can introduce multiple security risks. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.5.0/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding Github Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>In the following section we describe common setups for popular IdPs. Please refer to the documentation of your IdP for further information.</p>"}, {"location": "docs/0.5.0/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: One for Lakekeeper itself, one for the Lakekeeper UI and one for a machine client (Spark) to access Lakekeeper. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and best security.</p>"}, {"location": "docs/0.5.0/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> </ol>"}, {"location": "docs/0.5.0/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations. We are using one Application to secure the Lakekeeper API and login with UI using public flows (Authorization Code Flow): <pre><code>LAKEKEEPER__BASE_URI=&lt;URI where lakekeeper is reachable, in my example http://localhost:8181&gt;\n// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__IDP_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__IDP_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre></p> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.5.0/docs/authentication/#app-3-machine-user-spark", "title": "App 3: Machine User / Spark", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs2`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.5.0/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> The Lakekeeper Helm Chart creates the required binding by default.</p>"}, {"location": "docs/0.5.0/docs/authorization/", "title": "Authorization", "text": "<p>Authorization can only be enabled if Authentication is set up. Please check the Authentication Docs for more information.</p> <p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA enables a powerful permission model with bi-directional inheritance, essential for managing modern lakehouses with hierarchical namespaces. Our model balances usability and control for administrators.</p> <p>Please check the Authorization Configuration for details on enabling Authorization with Lakekeeper.</p>"}, {"location": "docs/0.5.0/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.5.0/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.5.0/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.5.0/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.5.0/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.5.0/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"}, {"location": "docs/0.5.0/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.5.0/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.5.0/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.5.0/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.5.0/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.5.0/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.5.0/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.5.0/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.5.0/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.5.0/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.5.0/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.5.0/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.5.0/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.5.0/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.5.0/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.5.0/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> must not be changed after the initial bootstrapping or permissions might not work.</p>"}, {"location": "docs/0.5.0/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.5.0/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.5.0/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.5.0/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.5.0/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.5.0/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"}, {"location": "docs/0.5.0/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper removes all files of these tables in the storage.</p>"}, {"location": "docs/0.5.0/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse.</p> <p>The undropping Endpoint is going to be release in Lakekeeper 0.6.0.</p>"}, {"location": "docs/0.5.0/docs/concepts/#migration", "title": "Migration", "text": "<p>Migration is a crucial step that must be performed before starting the Lakekeeper. It initializes the persistent backend storage and, if enabled, the authorization system. </p> <p>For each Lakekeeper update, migration must be executed before the <code>serve</code> command can be called. This ensures that all necessary updates and configurations are applied to the system. It is possible to skip Lakekeeper versions during migration.</p>"}, {"location": "docs/0.5.0/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.5.0/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port the Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS."}, {"location": "docs/0.5.0/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"}, {"location": "docs/0.5.0/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.5.0/docs/configuration/#task-queues", "title": "Task queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"}, {"location": "docs/0.5.0/docs/configuration/#nats", "title": "Nats", "text": "<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"}, {"location": "docs/0.5.0/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."}, {"location": "docs/0.5.0/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> is chosen, you need to provide additional parameters. The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>] <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set"}, {"location": "docs/0.5.0/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. If supported by the IdP, we recommend setting at least <code>openid email</code> <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP."}, {"location": "docs/0.5.0/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.5.0/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.5.0/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"}, {"location": "docs/0.5.0/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently all committers need to sign the CLA in github. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently we prefer to spent our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.5.0/docs/developer-guide/#quickstart", "title": "Quickstart", "text": "<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"}, {"location": "docs/0.5.0/docs/developer-guide/#developing-with-docker-compose", "title": "Developing with docker compose", "text": "<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"}, {"location": "docs/0.5.0/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"}, {"location": "docs/0.5.0/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"}, {"location": "docs/0.5.0/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"}, {"location": "docs/0.5.0/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Please check the Integration Test Docs.</p>"}, {"location": "docs/0.5.0/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"}, {"location": "docs/0.5.0/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage</li> </ul> <p>When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</p>"}, {"location": "docs/0.5.0/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with Minio &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Minio, Rook Ceph and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p>"}, {"location": "docs/0.5.0/docs/storage/#aws", "title": "AWS", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.5.0/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it is ignored. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.5.0/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p> <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.5.0/docs/storage/#gcs", "title": "GCS", "text": "<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this.</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"}, {"location": "docs/0.5.2/docs/authentication/", "title": "Authentication", "text": "<p>Warning</p> <p>We recommend to use Authorization always when Authentication is enabled. If <code>Authentication</code> is enabled, the UI only works if Authorization is also enabled.</p> <p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself, as this can introduce multiple security risks. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.5.2/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding Github Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>In the following section we describe common setups for popular IdPs. Please refer to the documentation of your IdP for further information.</p>"}, {"location": "docs/0.5.2/docs/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.5.2/docs/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>Create a new \"Client scope\":<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__BASE_URI=http://localhost:8181 (URI where lakekeeper is reachable)\nLAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.5.2/docs/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.5.2/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.5.2/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> </ol>"}, {"location": "docs/0.5.2/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__BASE_URI=http://localhost:8181 (URI where lakekeeper is reachable)\n// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre></p> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.5.2/docs/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.5.2/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> The Lakekeeper Helm Chart creates the required binding by default.</p>"}, {"location": "docs/0.5.2/docs/authorization/", "title": "Authorization", "text": "<p>Authorization can only be enabled if Authentication is set up. Please check the Authentication Docs for more information.</p> <p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA enables a powerful permission model with bi-directional inheritance, essential for managing modern lakehouses with hierarchical namespaces. Our model balances usability and control for administrators.</p> <p>Please check the Authorization Configuration for details on enabling Authorization with Lakekeeper.</p>"}, {"location": "docs/0.5.2/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.5.2/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.5.2/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.5.2/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.5.2/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.5.2/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"}, {"location": "docs/0.5.2/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.5.2/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.5.2/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.5.2/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.5.2/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.5.2/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.5.2/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.5.2/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.5.2/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.5.2/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.5.2/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.5.2/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.5.2/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.5.2/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.5.2/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.5.2/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> must not be changed after the initial bootstrapping or permissions might not work.</p>"}, {"location": "docs/0.5.2/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.5.2/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.5.2/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.5.2/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.5.2/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.5.2/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"}, {"location": "docs/0.5.2/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper removes all files of these tables in the storage.</p>"}, {"location": "docs/0.5.2/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse. The expiration delay is determined at the time of dropping the table, that means changing the delay in the warehouse settings will only affect newly dropped tables. If you want \"soft-deleted\" tables to be gone faster, undrop the tables, change the expiration delay and re-drop them. </p>"}, {"location": "docs/0.5.2/docs/concepts/#migration", "title": "Migration", "text": "<p>Migration is a crucial step that must be performed before starting the Lakekeeper. It initializes the persistent backend storage and, if enabled, the authorization system. </p> <p>For each Lakekeeper update, migration must be executed before the <code>serve</code> command can be called. This ensures that all necessary updates and configurations are applied to the system. It is possible to skip Lakekeeper versions during migration.</p>"}, {"location": "docs/0.5.2/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.5.2/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port the Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS."}, {"location": "docs/0.5.2/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"}, {"location": "docs/0.5.2/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.5.2/docs/configuration/#task-queues", "title": "Task queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"}, {"location": "docs/0.5.2/docs/configuration/#nats", "title": "Nats", "text": "<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"}, {"location": "docs/0.5.2/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."}, {"location": "docs/0.5.2/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> is chosen, you need to provide additional parameters. The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>] <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set"}, {"location": "docs/0.5.2/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code> <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code>"}, {"location": "docs/0.5.2/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.5.2/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently all committers need to sign the CLA in github. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently we prefer to spent our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/#quickstart", "title": "Quickstart", "text": "<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/#developing-with-docker-compose", "title": "Developing with docker compose", "text": "<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"}, {"location": "docs/0.5.2/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"}, {"location": "docs/0.5.2/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"}, {"location": "docs/0.5.2/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Please check the Integration Test Docs.</p>"}, {"location": "docs/0.5.2/docs/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by copying the latest existing one, e.g. <code>authz/openfga/v1/</code> to <code>authz/openfga/v2/</code></li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2/schema.fga</code></li> <li>create a diff between the old and new schema via <code>diff -u authz/openfga/v1/schema.fga authz/openfga/v2/schema.fga &gt; authz/openfga/v2/changed.diff</code> to help your reviewers</li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2/schema.fga &gt; authz/openfga/v2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::models.rs</code>, extend <code>CollaborationModels</code> with a field for your version, e.g., <code>v2</code> and then add your new model version on top of the file, like: <pre><code>const V2_MODEL: &amp;str = include_str!(\"../../../../../../../authz/openfga/v2/schema.json\");\n\nstatic MODEL: LazyLock&lt;CollaborationModels&gt; = LazyLock::new(|| CollaborationModels {\n    v1: serde_json::from_str(V1_MODEL).expect(\"Failed to parse OpenFGA model V1 as JSON\"),\n    // this is your added model below\n    v2: serde_json::from_str(V2_MODEL).expect(\"Failed to parse OpenFGA model V2 as JSON\"),\n});\n</code></pre></li> <li>set your model as the active model like: <code>const ACTIVE_MODEL: ModelVersion = ModelVersion::V2;</code></li> <li>implement the migration in <code>crate::service::authz::implementations::openfga::migrations::migrate</code> like: <pre><code>match model_version {\n    ModelVersion::V1 =&gt; {\n    // no migration to be done, we start at v1\n    }\n    ModelVersion::V2 =&gt; v2::migrate(client, &amp;store).await,\n}\n</code></pre></li> </ol>"}, {"location": "docs/0.5.2/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> </ul>"}, {"location": "docs/0.5.2/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage</li> </ul> <p>When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</p>"}, {"location": "docs/0.5.2/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with Minio &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Minio, Rook Ceph and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p>"}, {"location": "docs/0.5.2/docs/storage/#aws", "title": "AWS", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.5.2/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it is ignored. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.5.2/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p> <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.5.2/docs/storage/#gcs", "title": "GCS", "text": "<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this.</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"}, {"location": "docs/0.6.0/docs/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/0.6.0/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding Github Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p>"}, {"location": "docs/0.6.0/docs/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/0.6.0/docs/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/0.6.0/docs/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/0.6.0/docs/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__BASE_URI=http://localhost:8181 (URI where lakekeeper is reachable)\nLAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/0.6.0/docs/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/0.6.0/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/0.6.0/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol>"}, {"location": "docs/0.6.0/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__BASE_URI=http://localhost:8181 (URI where lakekeeper is reachable)\n// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre></p> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/0.6.0/docs/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/0.6.0/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> The Lakekeeper Helm Chart creates the required binding by default.</p>"}, {"location": "docs/0.6.0/docs/authorization/", "title": "Authorization", "text": "<p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper's default permission model uses the CNCF project OpenFGA to store and evaluate permissions. OpenFGA enables a powerful permission model with bi-directional inheritance, essential for managing modern lakehouses with hierarchical namespaces. Our model balances usability and control for administrators. In addition to OpenFGA, Lakekeeper's OPA bridge provides an additional translation layer that allows query engines such as trino to access Lakekeeper's permissions via Open Policy Agent (OPA). Please find more information in the OPA Bridge Guide.</p> <p>Please check the Authorization Configuration for details on enabling Authorization with Lakekeeper.</p>"}, {"location": "docs/0.6.0/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on Github. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/0.6.0/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/0.6.0/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/0.6.0/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/0.6.0/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/0.6.0/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. However, they cannot grant privileges or manage ownership.</p>"}, {"location": "docs/0.6.0/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/0.6.0/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/0.6.0/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/0.6.0/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.6.0/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/0.6.0/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/0.6.0/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/0.6.0/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/0.6.0/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>To-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/0.6.0/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/0.6.0/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/0.6.0/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/0.6.0/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/0.6.0/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper uses the wonderful OpenFGA Project. OpenFGA is automatically deployed in our docker-compose and helm installations. Authorization can only be used if Lakekeeper is connected to an Identity Provider.</li> <li>Secret Store (optional): By default, Lakekeeper stores all secrets (i.e. S3 access credentials) encrypted in the Persistence Backend. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. Currently Nats is supported, we are working on support for Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/0.6.0/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/0.6.0/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). By default, this <code>Server ID</code> is set to <code>00000000-0000-0000-0000-000000000000</code>. It can be changed by setting the <code>LAKEKEEPER__SERVER_ID</code> environment variable. We recommend to not set the <code>Server ID</code> explicitly, unless multiple Lakekeeper instances share a single Authorization system. The <code>Server ID</code> must not be changed after the initial bootstrapping or permissions might not work.</p>"}, {"location": "docs/0.6.0/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/0.6.0/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.6.0/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/0.6.0/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/0.6.0/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/0.6.0/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding Github Issue if this would be of interest to you.</p>"}, {"location": "docs/0.6.0/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/0.6.0/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>In Lakekeeper, warehouses can enable soft deletion. If soft deletion is enabled for a warehouse, when a table or view is dropped, it is not immediately deleted from the catalog. Instead, it is marked as dropped and a job for its cleanup is scheduled. The table is then deleted after the warehouse specific expiration delay has passed. This will allow for a recovery of tables that have been dropped by accident. \"Undropping\" a table is only possible if soft-deletes are enabled for a Warehouse. The expiration delay is determined at the time of dropping the table, that means changing the delay in the warehouse settings will only affect newly dropped tables. If you want \"soft-deleted\" tables to be gone faster, undrop the tables, change the expiration delay and re-drop them. </p>"}, {"location": "docs/0.6.0/docs/concepts/#migration", "title": "Migration", "text": "<p>Migration is a crucial step that must be performed before starting the Lakekeeper. It initializes the persistent backend storage and, if enabled, the authorization system. </p> <p>For each Lakekeeper update, migration must be executed before the <code>serve</code> command can be called. This ensures that all necessary updates and configurations are applied to the system. It is possible to skip Lakekeeper versions during migration.</p>"}, {"location": "docs/0.6.0/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__BASE_URI</code>, <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/0.6.0/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Base URL where the catalog is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port the Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS."}, {"location": "docs/0.6.0/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_READ</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence:</p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds"}, {"location": "docs/0.6.0/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/0.6.0/docs/configuration/#task-queues", "title": "Task queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__QUEUE_CONFIG__MAX_RETRIES</code> 5 Number of retries before a task is considered failed  Default: 5 <code>LAKEKEEPER__QUEUE_CONFIG__MAX_AGE</code> 3600 Amount of seconds before a task is considered stale and could be picked up by another worker. Default: 3600 <code>LAKEKEEPER__QUEUE_CONFIG__POLL_INTERVAL</code> 10 Amount of seconds between polling for new tasks. Default: 10"}, {"location": "docs/0.6.0/docs/configuration/#nats", "title": "Nats", "text": "<p>Lakekeeper can publish change events to Nats (Kafka is coming soon). The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against nats, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing nats credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> Nats token to use for authentication"}, {"location": "docs/0.6.0/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>External OpenID and Kubernetes Authentication can also be enabled together. If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously."}, {"location": "docs/0.6.0/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> is chosen, you need to provide additional parameters. The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>] <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set"}, {"location": "docs/0.6.0/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code>"}, {"location": "docs/0.6.0/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/0.6.0/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main should go through a PR. CI checks should pass before merging the PR. Before merge commits are squashed. PR titles should follow Conventional Commits.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently all committers need to sign the CLA in github. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently we prefer to spent our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/#quickstart", "title": "Quickstart", "text": "<pre><code># start postgres\ndocker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# migrate db\ncd crates/iceberg-catalog\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# run tests\ncargo test --all-features --all-targets\n\n# run clippy\ncargo clippy --all-features --all-targets\n</code></pre> <p>This quickstart does not run tests against cloud-storage providers or KV2. For that, please refer to the sections below.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/#developing-with-docker-compose", "title": "Developing with docker compose", "text": "<p>The following shell snippet will start a full development environment including the catalog plus its dependencies and a jupyter server with spark. The iceberg-catalog and its migrations will be built from source. This can be useful for development and testing.</p> <pre><code>$ cd examples\n$ docker-compose -f docker-compose.yaml -f docker-compose-latest.yaml up -d --build\n</code></pre> <p>You may then head to <code>localhost:8888</code> and try out one of the notebooks.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. You can use Docker to launch one.:</p> <p><pre><code>docker run -d --name postgres-15 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\n</code></pre> The <code>crates/iceberg-catalog</code> folder contains a <code>.env.sample</code> File. Copy this file to <code>.env</code> and add your database credentials if they differ.</p> <p>Run:</p> <pre><code>sqlx database create\nsqlx migrate run\n</code></pre>"}, {"location": "docs/0.6.0/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# this will enable the KV2 tests\necho 'export TEST_KV2=1' &gt;&gt; .env\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\ncargo test --all-features --all-targets\n</code></pre>"}, {"location": "docs/0.6.0/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, in order to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code># TEST_AZURE=&lt;some-value&gt; controls a proc macro which either includes or excludes the azure tests\n# if you compiled without TEST_AZURE, you'll have to change a file or do a cargo clean before rerunning tests. The same applies for the TEST_AWS and TEST_MINIO env vars.\nexport TEST_AZURE=1\nexport AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\nexport AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\nexport AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\n\nexport TEST_AWS=1\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\n# replace with actual values\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport TEST_MINIO=1\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n</code></pre> <p>You may then run a test via:</p> <pre><code>source .example.env-from-above\ncargo test service::storage::s3::test::aws::test_can_validate\n</code></pre>"}, {"location": "docs/0.6.0/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Please check the Integration Test Docs.</p>"}, {"location": "docs/0.6.0/docs/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by copying the latest existing one, e.g. <code>authz/openfga/v1/</code> to <code>authz/openfga/v2/</code></li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2/schema.fga</code></li> <li>create a diff between the old and new schema via <code>diff -u authz/openfga/v1/schema.fga authz/openfga/v2/schema.fga &gt; authz/openfga/v2/changed.diff</code> to help your reviewers</li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2/schema.fga &gt; authz/openfga/v2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::models.rs</code>, extend <code>CollaborationModels</code> with a field for your version, e.g., <code>v2</code> and then add your new model version on top of the file, like: <pre><code>const V2_MODEL: &amp;str = include_str!(\"../../../../../../../authz/openfga/v2/schema.json\");\n\nstatic MODEL: LazyLock&lt;CollaborationModels&gt; = LazyLock::new(|| CollaborationModels {\n    v1: serde_json::from_str(V1_MODEL).expect(\"Failed to parse OpenFGA model V1 as JSON\"),\n    // this is your added model below\n    v2: serde_json::from_str(V2_MODEL).expect(\"Failed to parse OpenFGA model V2 as JSON\"),\n});\n</code></pre></li> <li>set your model as the active model like: <code>const ACTIVE_MODEL: ModelVersion = ModelVersion::V2;</code></li> <li>implement the migration in <code>crate::service::authz::implementations::openfga::migrations::migrate</code> like: <pre><code>match model_version {\n    ModelVersion::V1 =&gt; {\n    // no migration to be done, we start at v1\n    }\n    ModelVersion::V2 =&gt; v2::migrate(client, &amp;store).await,\n}\n</code></pre></li> </ol>"}, {"location": "docs/0.6.0/docs/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/0.6.0/docs/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/0.6.0/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> </ul>"}, {"location": "docs/0.6.0/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage</li> </ul> <p>When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</p>"}, {"location": "docs/0.6.0/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with Minio &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Minio, Rook Ceph and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p>"}, {"location": "docs/0.6.0/docs/storage/#aws", "title": "AWS", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.6.0/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it is ignored. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/0.6.0/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p> <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/0.6.0/docs/storage/#gcs", "title": "GCS", "text": "<p>For GCS, the used bucket needs to disable hierarchical namespaces and should have the storage admin role.</p> <p>A sample storage profile could look like this.</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre>"}, {"location": "docs/latest/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/latest/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding GitHub Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/latest/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/latest/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/latest/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/latest/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/latest/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/latest/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/latest/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/latest/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/latest/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/latest/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/latest/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/latest/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/latest/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> <p>The Lakekeeper Helm Chart creates the required binding by default.</p> <p>Applications running in Kubernetes pods can now authenticate using the service account token, which is typically mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Simply read this token and include it in the <code>Authorization</code> header.</p> <p>Example with CURL: <pre><code>curl -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     http://my-lakekeeper:8181/catalog/v1/config\n</code></pre></p> <p>Example with Spark: <pre><code>spark-submit \\\n  --conf spark.sql.catalog.lakekeeper.token=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n  --conf spark.sql.catalog.lakekeeper.uri=\"http://my-lakekeeper:8181/catalog\" \\\n  my-spark-job.py\n</code></pre></p> <p>User identities appear in Lakekeeper as <code>k8s~&lt;namespace&gt;~&lt;service-account-name&gt;</code>.</p>"}, {"location": "docs/latest/authorization/", "title": "Authorization", "text": ""}, {"location": "docs/latest/authorization/#overview", "title": "Overview", "text": "<p>Authentication verifies who you are, while authorization determines what you can do.</p> <p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper currently supports the following Authorizers:</p> <ul> <li>AllowAll: A simple authorizer that allows all requests. This is mainly intended for development and testing purposes.</li> <li>OpenFGA: A fine-grained authorization system based on the CNCF project OpenFGA. Please find more information in the Authorization with OpenFGA section. OpenFGA requires an additional OpenFGA service to be deployed (this is included in our self-contained examples and our helm charts).</li> <li>Cedar: An enterprise-grade policy-based authorization system based on Cedar. The cedar authorizer is built into Lakekeeper and requires no additional external services. Please find more information in the Authorization with Cedar section.</li> <li>Custom: Lakekeeper supports custom authorizers via the <code>Authorizer</code> trait.</li> </ul>"}, {"location": "docs/latest/authorization/#authorization-with-openfga", "title": "Authorization with OpenFGA", "text": "<p>Lakekeeper can use OpenFGA to store and evaluate permissions. OpenFGA provides bi-directional inheritance, which is key for managing hierarchical namespaces in modern lakehouses. For query engines like Trino, Lakekeeper's OPA bridge translates OpenFGA permissions into Open Policy Agent (OPA) format. See the OPA Bridge Guide for details.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/latest/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on GitHub. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/latest/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/latest/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/latest/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/latest/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/latest/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. They can delegate the <code>data_admin</code> role they already hold (for example to team members), but they do not have general grant or ownership administration capabilities.</p>"}, {"location": "docs/latest/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/latest/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/latest/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/latest/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/latest/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/latest/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/latest/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/latest/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/latest/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>Top-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/latest/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/latest/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/latest/authorization/#openfga-in-production", "title": "OpenFGA in Production", "text": "<p>When deploying OpenFGA in production environments, ensure you follow the OpenFGA Production Checklist.</p> <p>Lakekeeper includes Query Consistency specifications with each authorization request to OpenFGA. For most operations, <code>MINIMIZE_LATENCY</code> consistency provides optimal performance while maintaining sufficient data consistency guarantees.</p> <p>For medium to large-scale deployments, we strongly recommend enabling caching in OpenFGA and increasing the database connection pool limits. These optimizations significantly reduce database load and improve authorization latency. Configure the following environment variables in OpenFGA (written for version 1.10). You may increase the number of connections further if your database deployment can handle additional connections:</p> <pre><code>OPENFGA_DATASTORE_MAX_OPEN_CONNS=200\nOPENFGA_DATASTORE_MAX_IDLE_CONNS=100\nOPENFGA_CACHE_CONTROLLER_ENABLED=true\nOPENFGA_CHECK_QUERY_CACHE_ENABLED=true\nOPENFGA_CHECK_ITERATOR_CACHE_ENABLED=true\n</code></pre>"}, {"location": "docs/latest/authorization/#authorization-with-cedar", "title": "Authorization with Cedar", "text": "<p>Cedar is an enterprise-grade, policy-based authorization system built into Lakekeeper that requires no external services. Cedar uses a declarative policy language to define access controls, making it ideal for organizations that prefer infrastructure-as-code approaches to authorization management.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/latest/authorization/#schema-and-entity-model", "title": "Schema and Entity Model", "text": "<p>For each authorization request, Lakekeeper provides the complete entity hierarchy from the requested resource up to the server level. This ensures policies have full context for making authorization decisions.</p> <p>When a user queries table <code>ns1.ns2.table1</code> in warehouse <code>wh-1</code> within project <code>my-project</code>, Cedar receives the following entities:</p> <ul> <li><code>Server</code> (root)</li> <li><code>Project::\"my-project\"</code></li> <li><code>Warehouse::\"wh-1\"</code> (parent: <code>my-project</code>)</li> <li><code>Namespace::\"ns1\"</code> (parent: <code>wh-1</code>)</li> <li><code>Namespace::\"ns2\"</code> (parent: <code>ns1</code>)</li> <li><code>Table::\"table1\"</code> (parent: <code>ns2</code>)</li> </ul> <p>This hierarchical context allows policies to reference any level in the path. For example, you can write policies that grant access based on the warehouse name, namespace hierarchy, or specific table properties.</p> <p>The Lakekeeper Cedar schema defines all available entity types, attributes, and actions. All loaded entities and policies are validated against this schema on startup and refresh. You can download the schema here: lakekeeper.cedarschema or find it on GitHub.</p> <p>Important: Lakekeeper does not provide Roles as built-in entities. Roles must be defined as custom entities in your entity JSON files.</p>"}, {"location": "docs/latest/authorization/#policy-examples", "title": "Policy Examples", "text": "<p>Grant admin access to a specific user: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action,\n    resource\n);\n</code></pre></p> <p>Role-based warehouse access: <pre><code>// Grant full access to all entities in a warehouse with name \"wh-1\"\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"NamespaceActions\",\n               Lakekeeper::Action::\"TableActions\",\n               Lakekeeper::Action::\"ViewActions\"],\n    resource\n)\nwhen { resource.warehouse.name == \"wh-1\" };\n\n// Allow modification of the warehouse itself\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"WarehouseModifyActions\"],\n    resource\n)\nwhen { resource.name == \"wh-1\" };\n</code></pre></p> <p>Table read access for all tables in the <code>analytics</code> namespace of warehouse <code>wh-1</code>: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action in [Lakekeeper::Action::\"TableSelectActions\"],\n    resource\n)\nwhen {\n    resource.namespace.name == \"analytics\" &amp;&amp;\n    resource.warehouse.name == \"wh-1\"\n};\n</code></pre></p>"}, {"location": "docs/latest/authorization/#entity-definition-example", "title": "Entity Definition Example", "text": "<p>Define roles and assign users to them using JSON entity files:</p> <pre><code>[\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::User\",\n            \"id\": \"oidc~90471f73-e338-4032-9a6b-1e021cc3cb1e\"\n        },\n        \"attrs\": {\n            \"display_name\": \"machine-user-1\"\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"data-engineering\"\n            }\n        ]\n    },\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::Role\",\n            \"id\": \"data-engineering\"\n        },\n        \"attrs\": {\n            \"name\": \"DataEngineering\",\n            \"project\": {\n                \"__entity\": {\n                    \"type\": \"Lakekeeper::Project\",\n                    \"id\": \"00000000-0000-0000-0000-000000000000\"\n                }\n            }\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"warehouse-1-admins\"\n            }\n        ]\n    }\n]\n</code></pre>"}, {"location": "docs/latest/authorization/#policy-and-entity-management", "title": "Policy and Entity Management", "text": "<p>Startup Behavior:</p> <ul> <li>All policy and entity files are loaded and validated against the Cedar schema</li> <li>If any file is unreadable or invalid, Lakekeeper fails to start with an error</li> </ul> <p>This ensures that authorization policies are always valid before serving requests</p> <p>Refresh Behavior: Configure automatic policy refresh using <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> (default: 5 seconds):</p> <ol> <li>Change Detection: Lightweight checks monitor ConfigMap versions and file timestamps</li> <li>Reload on Change: Modified entity or policy files trigger a full reload of all files to guarantee consistency</li> <li>Atomic Updates: The in-memory store is only updated if all files reload successfully</li> <li>Error Handling: If any reload fails, the previous configuration is retained, an error is logged, and health checks report unhealthy status</li> </ol> <p>This approach ensures that authorization policies remain consistent and that partial updates never compromise security.</p>"}, {"location": "docs/latest/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/latest/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/latest/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper supports different Authorizers. Please refer to the Authorization Documentation for more information.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/latest/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/latest/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/latest/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/latest/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/latest/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/latest/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/latest/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/latest/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding GitHub Issue if this would be of interest to you.</p>"}, {"location": "docs/latest/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/latest/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/latest/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/latest/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/latest/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/latest/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/latest/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/latest/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/latest/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/latest/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/latest/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/latest/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/latest/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/latest/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/latest/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option. <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> 2 Number of workers spawned that work on expire Snapshots tasks. See Expire Snapshots Docs for more information."}, {"location": "docs/latest/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/latest/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/latest/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/latest/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/latest/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> <code>resource_access.lakekeeper.roles</code> Specify the claim to use in provided JWT tokens to extract roles. The field should contain an array of strings or a single string. Supports nested claims using dot notation, e.g., \"resource_access.account.roles\". Currently only has an effect when using the Cedar Authorizer. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/latest/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> or <code>cedar</code> is chosen, additional parameters are required (see below). The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>, <code>cedar</code>]"}, {"location": "docs/latest/configuration/#openfga", "title": "OpenFGA", "text": "Variable Example Description <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/latest/configuration/#cedar", "title": "Cedar", "text": "Variable Example Description <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__LOCAL_FILES</code> <code>[/path/to/policies1.cedar,/path/to/policies2.cedar]</code> List of local file paths containing Cedar policies in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__LOCAL_FILES</code> <code>[/path/to/entities1.json,/path/to/entities2.json]</code> List of local JSON file paths containing additional Cedar entities (typically roles). <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedar</code> is treated as a policy source in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedarentities.json</code> is treated as an entity source. <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> <code>5</code> Refresh interval in seconds for reloading policies and entities from Kubernetes ConfigMaps and local files. Default: <code>5</code> seconds. See Cedar Authorization for more information. <code>LAKEKEEPER__CEDAR__EXTERNALLY_MANAGED_USER_AND_ROLES</code> <code>false</code> When set to <code>true</code>, Lakekeeper expects all roles and users to be managed externally via entities.json and does not extract <code>Lakekeeper::Role</code> or <code>Lakekeeper::User</code> entities from the user's token. When set to <code>false</code> (default), Lakekeeper automatically provides <code>Lakekeeper::Role</code> and <code>Lakekeeper::User</code> entities to Cedar based on information extracted from the user's token. When set to <code>false</code>, ensure <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> is configured to specify which claim in the token contains role information. <code>LAKEKEEPER__CEDAR__SCHEMA_FILE</code> <code>/path/to/custom/schema.cedarschema</code> Optional path to a custom Cedar schema file. If provided, this schema will be used instead of the embedded default schema. Useful for extending or customizing the Cedar schema. Compatibility with the Lakekeeper schema must be ensured for all entities provided by Lakekeeper (Server, Project, Namespace, Table, View. User &amp; Role if externally managed roles is <code>false</code>). <p>Debug configurations for Cedar</p> Variable Example Description <code>LAKEKEEPER__CEDAR__DEBUG__LOG_ENTITIES</code> <code>false</code> If <code>true</code>, logs all internal entities (excluding externally managed entities) for each authorization request at debug level. This is useful for debugging authorization issues but can be verbose and impacts performance. Logging only occurs when both this flag is <code>true</code> AND debug logging is enabled (<code>RUST_LOG=debug</code>). Default: <code>false</code>."}, {"location": "docs/latest/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/latest/configuration/#caching", "title": "Caching", "text": "<p>Lakekeeper uses in-memory caches to speed up certain operations.</p> <p>Short-Term Credentials (STC) Cache</p> <p>When Lakekeeper vends short-term credentials for cloud storage access (S3 STS, Azure SAS tokens, or GCP access tokens), these credentials can be cached to reduce load on cloud identity services and improve response times.</p> Variable Example Description <code>LAKEKEEPER__CACHE__STC__ENABLED</code> <code>true</code> Enable or disable the short-term credentials cache. Default: <code>true</code> <code>LAKEKEEPER__CACHE__STC__CAPACITY</code> <code>10000</code> Maximum number of credential entries to cache. Default: <code>10000</code> <p>Expiry Mechanism: Cached credentials automatically expire based on the validity period of the underlying cloud credentials. Lakekeeper caches credentials for half their lifetime (e.g., if GCP STS returns credentials valid for 1 hour, they're cached for 30 minutes) with a maximum cache duration of 1 hour. This ensures credentials remain fresh while reducing unnecessary identity service calls.</p> <p>Metrics: The STC cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_stc_cache_size{cache_type=\"stc\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_stc_cache_hits_total{cache_type=\"stc\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_stc_cache_misses_total{cache_type=\"stc\"}</code>: Total number of cache misses</li> </ul> <p>Warehouse Cache</p> <p>Caches warehouse metadata to reduce database queries for warehouse lookups.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__WAREHOUSE__ENABLED</code> boolean <code>true</code> Enable/disable warehouse caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__CAPACITY</code> integer <code>1000</code> Maximum number of warehouses to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to Storage Profile may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. Warehouse metadata is guaranteed to be fresh for load table &amp; view operations also for multi-worker deployments.</p> <p>Metrics: The Warehouse cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_warehouse_cache_size{cache_type=\"warehouse\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_warehouse_cache_hits_total{cache_type=\"warehouse\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_warehouse_cache_misses_total{cache_type=\"warehouse\"}</code>: Total number of cache misses</li> </ul> <p>Namespace Cache</p> <p>Caches namespace metadata and hierarchies to reduce database queries for namespace lookups. Namespace lookups are also required for table &amp; view operations.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__NAMESPACE__ENABLED</code> boolean <code>true</code> Enable/disable namespace caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__NAMESPACE__CAPACITY</code> integer <code>1000</code> Maximum number of namespaces to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__NAMESPACE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to namespace properties may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. The namespace cache stores both individual namespaces and their parent hierarchies for efficient lookups.</p> <p>Metrics: The Namespace cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_namespace_cache_size{cache_type=\"namespace\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_namespace_cache_hits_total{cache_type=\"namespace\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_namespace_cache_misses_total{cache_type=\"namespace\"}</code>: Total number of cache misses</li> </ul> <p>Secrets Cache</p> <p>Caches storage secrets to reduce load on the secret store. Since Lakekeeper never updates secrets, long TTLs can significantly increase resilience against secret store outages, especially when the secret store is external to the main database backend.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__SECRETS__ENABLED</code> boolean <code>true</code> Enable/disable secrets caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__SECRETS__CAPACITY</code> integer <code>500</code> Maximum number of secrets to cache. Default: <code>500</code> <code>LAKEKEEPER__CACHE__SECRETS__TIME_TO_LIVE_SECS</code> integer <code>600</code> Time-to-live for cache entries in seconds. Default: <code>600</code> (10 minutes) <p>Metrics: The Secrets cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_secrets_cache_size{cache_type=\"secrets\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_secrets_cache_hits_total{cache_type=\"secrets\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_secrets_cache_misses_total{cache_type=\"secrets\"}</code>: Total number of cache misses</li> </ul>"}, {"location": "docs/latest/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/latest/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/latest/configuration/#debug", "title": "Debug", "text": "<p>Lakekeeper provides debugging options to help troubleshoot issues during development. These options should not be enabled in production environments as they can expose sensitive data and impact performance.</p> Variable Example Description <code>LAKEKEEPER__DEBUG__LOG_REQUEST_BODIES</code> <code>true</code> If set to <code>true</code>, Lakekeeper will log all incoming and outgoing request bodies at debug level. This is useful for debugging API interactions but should never be enabled in production as it can expose sensitive data (credentials, tokens, etc.) and significantly impact performance. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__MIGRATE_BEFORE_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper waits for the DB (30s) and runs migrations when <code>serve</code> is called. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__AUTO_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper will automatically start the server when no subcommand is provided (i.e., when running the binary without arguments). This is useful for development environments to quickly start the server without explicitly specifying the <code>serve</code> command. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__EXTENDED_LOGS</code> <code>false</code> Controls whether file names and line numbers are included in JSON log output. When set to <code>false</code>, these fields are omitted for cleaner logs. When set to <code>true</code>, each log entry includes <code>filename</code> and <code>line_number</code> fields for easier debugging. Default: <code>false</code> <p>Warning: Debug options can expose sensitive information in logs and should only be used in secure development environments.</p>"}, {"location": "docs/latest/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/latest/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/latest/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/latest/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/latest/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:17\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db (make sure you have sqlx installed `cargo install sqlx-cli`)\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code (make sure you have cargo-sort installed, `cargo install cargo-sort`)\n# You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/latest/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/latest/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p> <p>lakekeeper</p> <p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol> <p>lakekeeper-bin</p> <p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/latest/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/latest/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/latest/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/latest/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/latest/developer-guide/#schema-qualification-warning", "title": "\u26a0\ufe0f Schema Qualification Warning", "text": "<p>IMPORTANT: When adding new migrations, do NOT schema qualify references to any database objects. Schema qualification will break deployments that place the application in a schema different than the public one.</p> <p>\u274c Incorrect - Do NOT do this: <pre><code>-- This will break deployments in non-public schemas\nCREATE TABLE public.my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO public.my_new_table (name) VALUES ('example');\n\nALTER TABLE public.existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>\u2705 Correct - Do this instead: <pre><code>-- This will work in any schema\nCREATE TABLE my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO my_new_table (name) VALUES ('example');\n\nALTER TABLE existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>The migration system will automatically apply the migration in the correct schema context, so explicit schema qualification is unnecessary and will cause issues in deployments where Lakekeeper is deployed to a custom schema.</p>"}, {"location": "docs/latest/developer-guide/#inspecting-the-db", "title": "Inspecting the db", "text": "<p>The db schema is the result of all migrations applied in order. To inspect it you can:</p> <pre><code># Assumes you set up the db as described above\n\n# Get a shell in the db's container\ndocker exec -it postgres-16 /bin/bash\n\n# Then you can connect to the db\npsql \"postgresql://postgres:postgres@localhost:5432/postgres\"\n# And inspect it, for instance by describing views or tables\n\\d+ active_tabulars\n\n# Or you can dump the entire schema\npg_dump --schema-only \"postgresql://postgres:postgres@localhost:5432/postgres\" &gt; /home/lakekeeper_schema.sql\n# Copy it out of the container and then inspect it or pass it as context to LLMs\ndocker cp postgres-16:/home/lakekeeper_schema.sql .\n</code></pre>"}, {"location": "docs/latest/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/latest/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/latest/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/latest/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/latest/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/latest/developer-guide/#building-the-docs-locally", "title": "Building the docs locally", "text": "<pre><code>cd site\njust serve\n</code></pre>"}, {"location": "docs/latest/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on GitHub.</p>"}, {"location": "docs/latest/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/latest/engines/#duckdb-wasm", "title": "DuckDB WASM", "text": "<p>DuckDB WASM allows you to query Lakekeeper directly from your browser. If you are using the Lakekeeper UI, DuckDB WASM is pre-configured. To use DuckDB WASM from the Lakekeeper UI, there are two important requirements due to browser security restrictions:</p> <p>Requirements:</p> <ol> <li>Same-Origin Access: The S3 endpoint must be accessible from your browser at the same URL/origin that Lakekeeper uses to access it. For example, if Lakekeeper accesses S3 at <code>http://my-s3-endpoint:9000</code>, your browser must also be able to reach it at <code>http://my-s3-endpoint:9000</code>. This means the Docker Compose examples won't work with DuckDB WASM out of the box, as the S3 endpoint is typically only accessible within the Docker network, while your browser is not in this network.</li> <li>CORS Policy: Your S3 storage must be configured with a CORS policy that allows requests from the Lakekeeper origin. See the CORS Configuration guide for setup instructions.</li> </ol>"}, {"location": "docs/latest/engines/#duckdb", "title": "DuckDB", "text": "<p>Basic setup in DuckDB:</p> <pre><code>import duckdb\n\nCATALOG_URL = \"http://localhost:8181/catalog\"\nWAREHOUSE = \"my_warehouse\"\n\n# Required if OAuth2 authentication is enabled for Lakekeeper\nCLIENT_ID = \"your-client-id\"\nCLIENT_SECRET = \"your-client-secret\"\nKEYCLOAK_TOKEN_ENDPOINT = \"http://your-idp/realms/iceberg/protocol/openid-connect/token\"\n\n# Install and load Iceberg extension\nduckdb.sql(\"INSTALL ICEBERG;\")\nduckdb.sql(\"LOAD ICEBERG;\")\n\n# Create secret for authentication\nduckdb.sql(f\"\"\"\n    CREATE SECRET lakekeeper_secret (\n        TYPE ICEBERG,\n        CLIENT_ID '{CLIENT_ID}',\n        CLIENT_SECRET '{CLIENT_SECRET}',\n        OAUTH2_SCOPE 'lakekeeper',\n        OAUTH2_SERVER_URI '{KEYCLOAK_TOKEN_ENDPOINT}'\n    )\n\"\"\")\n\n# Attach catalog\nduckdb.sql(f\"\"\"\n    ATTACH '{WAREHOUSE}' AS my_datalake (\n        TYPE ICEBERG,\n        ENDPOINT '{CATALOG_URL}',\n        SECRET lakekeeper_secret\n    )\n\"\"\")\n\n# Query tables\nduckdb.sql(\"SELECT * FROM my_datalake.my_namespace.my_table\").show()\n</code></pre>"}, {"location": "docs/latest/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/latest/engines/#starburst", "title": "Starburst", "text": "<p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in Starburst:</p> S3-CompatibleAzureGCS <p>Starburst supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <p>Please find additional configuration Options in the Starburst docs.    </p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Starburst.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for GCS, so that GCS credentials must be specified in the connector.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/latest/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/latest/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/latest/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/latest/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/latest/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/latest/engines/#risingwave", "title": "RisingWave", "text": "<p>RisingWave is a distributed SQL streaming database that is wire-compatible with PostgreSQL, designed for real-time data ingestion, processing, and querying. Unlike many other query engines that use a <code>CATALOG</code> abstraction, RisingWave connects to Lakekeeper through a <code>CONNECTION</code> object, which allows it to use Iceberg tables for sources, sinks, and internal tables.</p> <p>For a hands-on example, a Docker Compose setup is available in the RisingWave repository. You can find detailed deployment instructions in the official RisingWave documentation.</p> <p>Once you have both services running, you can create a <code>CONNECTION</code> in RisingWave to connect to Lakekeeper. The following is an example configuration. As parameters may change over time, please refer to the official RisingWave documentation for the most up-to-date and complete configuration options.</p> <pre><code>CREATE CONNECTION lakekeeper_catalog_conn\nWITH (\n    type = 'iceberg',\n    catalog.type = 'rest',\n    catalog.uri = 'http://lakekeeper:8181/catalog/',\n    warehouse.path = 'risingwave-warehouse',\n    s3.access.key = 'hummockadmin',\n    s3.secret.key = 'hummockadmin',\n    s3.path.style.access = 'true',\n    s3.endpoint = 'http://minio-0:9301',\n    s3.region = 'us-east-1'\n);\n</code></pre> <p>After creating the connection, you must set it as the default for your session to create and query internal Iceberg tables. The <code>SET</code> command applies the change to the current session only, while <code>ALTER SYSTEM</code> makes it persistent across restarts.</p> <pre><code>-- Set for the current session\nSET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n\n-- Set persistent for the system\nALTER SYSTEM SET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n</code></pre>"}, {"location": "docs/latest/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/latest/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/latest/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/latest/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/latest/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/latest/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/latest/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/latest/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/latest/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/latest/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/latest/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For medium or large deployments, ensure the <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> and <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> are set to a higher value to allow Lakekeeper to use more connections to the database.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>When using OpenFGA, make sure that Caching is enabled. Check the OpenFGA in Production section for more information.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/latest/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p> <p>// ...existing code...</p>"}, {"location": "docs/latest/storage/#disabling-credential-vending-remote-signing", "title": "Disabling Credential Vending &amp; Remote Signing", "text": "<p>Lakekeeper provides multiple ways to control how credentials and remote signing information are provided to clients.</p> <p>You can disable credential vending and remote signing on a per-warehouse basis using storage profile settings. For S3 warehouses, set <code>remote-signing-enabled</code> to <code>false</code> to disable remote signing and <code>sts-enabled</code> to <code>false</code> to disable STS vended credentials. For Azure ADLS warehouses, set <code>sas-enabled</code> to <code>false</code> to disable SAS token generation. For GCS warehouses, set <code>sts-enabled</code> to <code>false</code> to disable STS token generation. When these options are disabled at the storage profile level, clients will not receive the corresponding credentials or signing information for that warehouse, regardless of the request headers. Lakekeeper downscopes vended credentials for all supported storages to the location of the table being accessed and ensures that there are no overlapping table locations within a warehouse.</p> <p>Clients can also control credential delegation per request using the <code>X-Iceberg-Access-Delegation</code> header. Lakekeeper supports the standard Iceberg REST spec values (<code>vended-credentials</code> and <code>remote-signing</code>), plus a special <code>client-managed</code> value. When set to <code>client-managed</code>, no credentials or signing information are returned, regardless of storage profile configuration. This allows clients to use their own credentials for direct storage access.</p>"}, {"location": "docs/latest/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/latest/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with S3-compatible storages &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Rook Ceph Rados, NetApp StorageGRID 12.0 or newer, Minio and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>When a client requests table configuration, Lakekeeper selects between remote signing and vended credentials based on the <code>X-Iceberg-Access-Delegation</code> header and storage profile settings:</p> <ul> <li>If the header is set to <code>client-managed</code>, neither credentials nor signing information are returned</li> <li>If the header specifies <code>vended-credentials</code> or <code>remote-signing</code>, that method is used if enabled in the storage profile</li> <li>If both methods are requested or neither is specified, Lakekeeper attempts to provide vended credentials first (if STS is enabled), then falls back to remote signing (if enabled)</li> <li>If both methods are disabled at the storage profile level, no credentials are returned regardless of the header value</li> </ul> <p>For maximum client compatibility, we recommend enabling both STS and remote signing when your S3 storage supports it.</p> <p>For some older remote signing clients that cannot handle table-specific remote signing endpoint locations, Lakekeeper needs to identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/latest/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>remote-signing-enabled</code> Boolean No <code>true</code> Whether to enable remote signing for S3 requests. When disabled, clients cannot use remote signing for this storage profile even if STS is disabled. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key. <code>legacy-md5-behavior</code> Boolean No <code>false</code> A flag to enable the legacy behavior of using MD5 checksums for operations that require checksums."}, {"location": "docs/latest/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/latest/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/latest/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/latest/storage/#cors-configuration", "title": "CORS Configuration", "text": "<p>For browser-based access to S3 buckets (required for DuckDB WASM), you need to configure CORS (Cross-Origin Resource Sharing) on your S3 bucket.</p> <p>To configure CORS for your S3 bucket:</p> <ol> <li>In the AWS S3 Configuration Menu, klick on the name of your bucket</li> <li>Choose Permissions Tab</li> <li>In the Cross-origin resource sharing (CORS) section, choose Edit</li> <li>In the CORS configuration editor text box, type or copy and paste a new CORS configuration, or edit an existing configuration. The CORS configuration is a JSON file. The text that you type in the editor must be valid JSON. See below for an example.</li> <li>Choose Save changes</li> </ol> <p>Example CORS policy:</p> <pre><code>[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"POST\",\n            \"PUT\",\n            \"DELETE\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"https://lakekeeper.example.com\"\n        ],\n        \"ExposeHeaders\": []\n    }\n]\n</code></pre> <p>Replace <code>https://lakekeeper.example.com</code> with the origin where your Lakekeeper instance is hosted.</p>"}, {"location": "docs/latest/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/latest/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/latest/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. weur&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code>. Use a Data Location Hint as region.</p>"}, {"location": "docs/latest/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/latest/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>sas-enabled</code> Boolean No <code>true</code> Whether to enable SAS (Shared Access Signature) token generation for Azure Data Lake Storage. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/latest/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/latest/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/latest/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>sts-enabled</code> Boolean No <code>true</code> Whether to enable STS (Security Token Service) downscoped token generation for GCS. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/latest/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/latest/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/latest/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/latest/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/latest/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p> <p>Link to Expire Snapshots</p>"}, {"location": "docs/latest/table-maintenance/#expire-snapshots", "title": "Expire Snapshots", "text": "<p>Lakekeeper automatically expires old table snapshots based on configurable age and retention policies. This helps manage storage costs and performance by removing outdated snapshot metadata and associated data files.</p> <p>Expire snapshots can be configured per warehouse and optionally overridden at the table level using Iceberg table properties.</p>"}, {"location": "docs/latest/table-maintenance/#configuration", "title": "Configuration", "text": "<p>Configuration can be set via the Management UI or REST API endpoints:</p> <ul> <li>GET <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> <li>POST <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> </ul> Parameter Type Default Description <code>enable-expire-snapshots</code> boolean <code>false</code> Enable automatic snapshot expiration for all tables in the warehouse. Can be overridden per table with <code>lakekeeper.history.expire.enabled</code> <code>max-snapshot-age-ms</code> integer <code>432000000</code> (5 days) Maximum age of snapshots in milliseconds before expiration. Override per table with <code>history.expire.max-snapshot-age-ms</code> <code>min-snapshots-to-keep</code> integer <code>1</code> Minimum snapshots to retain on each table branch. Override per table with <code>history.expire.min-snapshots-to-keep</code> <code>min-snapshots-to-expire</code> integer <code>20</code> Minimum snapshots required before expiration job is scheduled (prevents expensive jobs for few snapshots). Override per table with <code>lakekeeper.history.expire.min-snapshots-to-expire</code> <code>max-ref-age-ms</code> integer <code>9223372036854775807</code> (no limit) Maximum age for snapshot references (except main branch). Main branch references never expire"}, {"location": "docs/latest/table-maintenance/#table-level-overrides", "title": "Table-Level Overrides", "text": "<p>Individual tables can override warehouse settings using these Iceberg table properties:</p> <ul> <li><code>lakekeeper.history.expire.enabled</code> - Enable/disable for specific table</li> <li><code>history.expire.max-snapshot-age-ms</code> - Custom max age for table snapshots  </li> <li><code>history.expire.min-snapshots-to-keep</code> - Custom minimum retention for table</li> <li><code>lakekeeper.history.expire.min-snapshots-to-expire</code> - Custom threshold for table</li> </ul> <p>Note</p> <p>Tables with <code>gc.enabled=false</code> are excluded from automatic expiration regardless of other settings.</p>"}, {"location": "docs/latest/table-maintenance/#production-deployment", "title": "Production Deployment", "text": "<p>For production workloads, we recommend running expire snapshots workers in dedicated pods to avoid impacting REST API performance. This can be achieved by:</p> <ol> <li>API pods: Set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS=0</code> to disable workers</li> <li>Worker pods: Use default worker configuration (2 workers) to handle expire snapshots tasks or set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> to desired number of workers</li> </ol>"}, {"location": "docs/latest/table-maintenance/#task-scheduling", "title": "Task Scheduling", "text": "<p>Expire snapshots tasks are intelligently scheduled immediately after table commits when needed, eliminating the overhead of cron-based polling. This ensures timely cleanup while maintaining optimal performance.</p>"}, {"location": "docs/latest/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/latest/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/latest/api/management-plus/", "title": "Management <span class=\"lkp\"></span>", "text": ""}, {"location": "docs/latest/api/management/", "title": "Management (Core)", "text": ""}, {"location": "docs/latest/docs/authentication/", "title": "Authentication", "text": "<p>Authentication is crucial for securing access to Lakekeeper. By enabling authentication, you ensure that only authorized users can access and interact with your data. Lakekeeper supports authentication via any OpenID (or OAuth 2) capable identity provider as well as authentication for Kubernetes service accounts, allowing you to integrate with your existing identity providers.</p> <p>Authentication and Authorization are distinct processes in Lakekeeper. Authentication verifies the identity of users, ensuring that only authorized individuals can access the system. This is performed via an Identity Provider (IdP) such as OpenID or Kubernetes. Authorization, on the other hand, determines what authenticated users are allowed to do within the system. Lakekeeper is extendable and can connect to different authorization systems. By default, Lakekeeper uses OpenFGA to manage and evaluate permissions, providing a robust and flexible authorization model. For more details, see the Authorization guide.</p> <p>Lakekeeper does not issue API-Keys or Client-Credentials itself. Instead, it relies on external IdPs for authentication, ensuring a secure and centralized management of user identities. This approach minimizes the risk of credential leakage and simplifies the integration with existing security infrastructures.</p>"}, {"location": "docs/latest/docs/authentication/#openid-provider", "title": "OpenID Provider", "text": "<p>Lakekeeper can be configured to integrate with all common identity providers. For best performance, tokens are validated locally against the server keys (<code>jwks_uri</code>). This requires all incoming tokens to be JWT tokens. If you require support for opaque tokens, please upvote the corresponding GitHub Issue.</p> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. Optionally, if <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, Lakekeeper validates the <code>aud</code> field of the provided token to match the specified value. We recommend to specify the audience in all deployments, so that tokens leaked for other applications in the same IdP cannot be used to access data in Lakekeeper.</p> <p>Users are automatically added to Lakekeeper after successful Authentication (user provides a valid token with the correct issuer and audience). If a User does not yet exist in Lakekeeper's Database, the provided JWT token is parsed. The following fields are parsed:</p> <ul> <li><code>name</code>: <code>name</code> or <code>given_name</code>/ <code>first_name</code> and <code>family_name</code>/ <code>last_name</code> or <code>app_displayname</code> or <code>preferred_username</code></li> <li><code>subject</code>: <code>sub</code> unless <code>subject_claim</code> is set, then it will be the value of the claim.</li> <li><code>claims</code>: all claims</li> <li><code>email</code>: <code>email</code> or <code>upn</code> if it contains an <code>@</code> or <code>preferred_username</code> if it contains an <code>@</code></li> </ul> <p>If the <code>name</code> cannot be determined because none of the claims are available, the principal is registered under the name <code>Nameless App with ID &lt;user-id&gt;</code>. Lakekeeper determines the ID of users in the following order:</p> <ol> <li>If <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> is set, this field is used and must be present.</li> <li>If <code>oid</code> is present, it is used. The main motivation to prefer the <code>oid</code> over the <code>sub</code> is that the <code>sub</code> field is not unique across applications, while the <code>oid</code> is. (See for example Entra-ID). Lakekeeper needs to the same IDs as query engines in order to share Permissions.</li> <li>If the <code>sub</code> field is present, use it, otherwise fail.</li> </ol> <p>IDs from the OIDC provider in Lakekeeper have the form <code>oidc~&lt;ID from the provider&gt;</code>.</p>"}, {"location": "docs/latest/docs/authentication/#authenticating-machine-users", "title": "Authenticating Machine Users", "text": "<p>All common iceberg clients and IdPs support the OAuth2 <code>Client-Credential</code> flow. The <code>Client-Credential</code> flow requires a <code>Client-ID</code> and <code>Client-Secret</code> that is provided in a secure way to the client. In the following sections we demonstrate for selected IdPs how applications can be setup for machine users to connect.</p>"}, {"location": "docs/latest/docs/authentication/#authenticating-humans", "title": "Authenticating Humans", "text": "<p>Human Authentication flows are interactive by nature and are typically performed directly by the IdP. This enables the use of all security options that the IdP supports, including 2FA, hardware keys, single-sign-on and more. The recommended flows for authentication are Authorization Code Flow RFC6749#section-4.1 with PKCE and Device Code Flow RFC8628.</p> <p>At the time of writing all common iceberg clients (spark, trino, starrocks, pyiceberg, ...) do not support any authorization flow that is suitable for human users natively. The iceberg community is working on introducing those flows and we started an initiative to standardize and document them as part of the iceberg docs.</p> <p>Until iceberg clients are natively ready for human flows, authentication flows have to be performed outside of iceberg clients. To make this process as easy as possible, the Lakekeeper UI offers the option to get a new token for a human user:</p> <p></p> <p>The lifetime of this token is specified in the corresponding application in your IdP. We recommend to set the lifetime to no longer than one day.</p>"}, {"location": "docs/latest/docs/authentication/#keycloak", "title": "Keycloak", "text": "<p>We are creating two Client: The first client with a \"public\" profile for the Lakekeeper API &amp; UI and the second client for a machine client (e.g. Spark). Repeat step 2 for each machine client that is needed.</p>"}, {"location": "docs/latest/docs/authentication/#client-1-lakekeeper", "title": "Client 1: Lakekeeper", "text": "<ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>lakekeeper</code></li> <li>Name: choose any, for this example we choose  <code>Lakekeeper Catalog</code></li> <li>Client authentication: Leave \"Off\". We need a public client.</li> <li>Authentication Flows: Enable \"Standard flow\", OAuth 2.0 Device Authorization Grant\".</li> <li>Valid redirect URIs: For testing a wildcard \"*\" can be set. Otherwise the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>.</li> </ul> </li> <li>When the client is created, click on the \"Advanced\" tab of this client, scroll down to \"Advanced settings\" and set \"Access Token Lifespan\" to \"Expires in\" - 12 Hours.</li> <li>Create a new \"Client scope\" in the left side menu:<ul> <li>Name: choose any, for this example we choose  <code>lakekeeper</code> </li> <li>Description: <code>Client of Lakekeeper</code></li> <li>Type: Optional</li> </ul> </li> <li>When the scope is created, we need to add a new mapper. This is recommended because Lakekeeper can validate the <code>audience</code> (target service) of the token for increased security. In order to add the <code>lakekeeper</code> audience to the token every time the <code>lakekeeper</code> scope is requested, we create a new mapper. Select the \"Mappers\" tab of the previously created <code>lakekeeper</code> scope. Select \"Configure a new mapper\" -&gt; \"Audience\". <ul> <li>Name: choose any, for this example we choose  <code>Add lakekeeper Audience</code> </li> <li>Included Client Audience: Select the id of the previously created App 1. In our example this is <code>lakekeeper</code>.</li> <li>Make sure <code>Add to access token</code> and <code>Add to token introspection</code> is enabled.</li> </ul> </li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>lakekeeper</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Default\".</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations: <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=http://localhost:30080/realms/iceberg (URI of the keycloak realm)\nLAKEKEEPER__OPENID_AUDIENCE=lakekeeper (ID of Client 1)\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"lakekeeper\" (ID of Client 1)\n# LAKEKEEPER__UI__OPENID_SCOPE=\"lakekeeper\" (Name of the created scope, not required if scope was added as default)\n</code></pre></p>"}, {"location": "docs/latest/docs/authentication/#client-2-machine-user", "title": "Client 2: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"Client\":<ul> <li>Client Type: choose \"OpenID Connect\"</li> <li>Client ID: choose any, for this example we choose  <code>spark</code>.</li> <li>Name: choose any, for this example we choose  <code>Spark Client accessing Lakekeeper</code></li> <li>Client authentication: Turn \"On\". Leave \"Authorization\" turned \"Off\".</li> <li>Authentication Flows: Enable \"Service accounts roles\" and \"Standard Token Exchange\".</li> </ul> </li> <li>When the client is created, click on \"Credentials\", choose \"Client Authenticator\" as \"Client Id and Secret\". Copy the <code>Client Secret</code> for later use.</li> <li>Finally, we need to grant the <code>spark</code> client permission to use the <code>lakekeeper</code> scope which adds the correct audience to the issued token. Select the \"Client scopes\" tab of the <code>spark</code> client and select \"Add client scope\". Select the previously created scope, in our example this is <code>lakekeeper</code>. We recommend adding the scope as \"Optional\". By adding an optional scope the client can be re-used for other services, i.e. if Spark needs to access another catalog in the future.</li> </ol> <p>That's it! We can now use the second App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    scope=\"lakekeeper\", # Name of the created scope\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID of Client 2&gt;:&lt;Client-Secret of Client 2&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.lakekeeper.scope\": \"lakekeeper\", # Name of the created scope\n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `lakekeeper`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints.</p>"}, {"location": "docs/latest/docs/authentication/#entra-id-azure", "title": "Entra-ID (Azure)", "text": "<p>We are creating three App-Registrations: The first for Lakekeeper itself, the second for the Lakekeeper UI the third for a machine client (e.g. Spark) to access Lakekeeper. Repeat step 3 for each machine client that is needed. While App-Registrations can also be shared, the recommended setup we propose here offers more flexibility and better security.</p>"}, {"location": "docs/latest/docs/authentication/#app-1-lakekeeper-ui-application", "title": "App 1: Lakekeeper UI Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper-UI</code></li> <li>Redirect URI: Add the URL where the Lakekeeper UI is reachable for the user suffixed by <code>/callback</code>. E.g.: <code>http://localhost:8181/ui/callback</code>. If asked, select type \"Single Page Application (SPA)\".</li> </ul> </li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>. Also note the <code>Directory (tenant) ID</code>.</li> <li>Finally we recommend to set a policy for tokens to expire in 12 hours instead of the default ~1 hour. Please follow the Microsoft Tutorial to assign a corresponding policy to the Application. (If you find a good way to do this via the UI, please let us know so that we can update this documentation page!)</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"lakekeeper_ui\" {\n  display_name = \"Lakekeeper UI\"\n}\n\nresource \"azuread_application_redirect_uris\" \"lakekeeper_ui\" {\n  application_id = azuread_application_registration.lakekeeper_ui.id\n  type           = \"SPA\"\n\n  redirect_uris = [\n    &lt;insert-redirect-uris&gt;\n  ]\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_ui\" {\n  client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n</code></pre>"}, {"location": "docs/latest/docs/authentication/#app-2-lakekeeper-application", "title": "App 2: Lakekeeper Application", "text": "<ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper</code></li> <li>Redirect URI: Leave empty.</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Expose an API\" and on the top select \"Add\" beside <code>Application ID URI</code>.  Note down the <code>Application ID URI</code> (should be <code>api://&lt;Client ID&gt;</code>).</li> <li>Still in the \"Expose an API\" menus, select \"Add a Scope\". Fill the fields as follows:<ul> <li>Scope name: lakekeeper</li> <li>Who can consent? Admins and users</li> <li>Admin consent display name: Lakekeeper API</li> <li>Admin consent description: Access Lakekeeper API</li> <li>State: Enabled</li> </ul> </li> <li>After the <code>lakekeeper</code> scope is created, click \"Add a client application\" under the \"Authorized client applications\" headline. Select the previously created scope and paste as <code>Client ID</code> the previously noted ID from App 1.</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code>.</li> </ol> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"random_uuid\" \"lakekeeper_scope\" {}\n\nresource \"azuread_application\" \"lakekeeper\" {\n  display_name = \"Lakekeeper\"\n  owners       = [data.azuread_client_config.current.object_id]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application_registration.lakekeeper_ui.client_id\n    ]\n\n    oauth2_permission_scope {\n      id      = random_uuid.lakekeeper_scope.id\n      value   = \"lakekeeper\"\n      enabled = true\n      type    = \"User\"\n\n      admin_consent_description  = \"Lakekeeper API\"\n      admin_consent_display_name = \"Access Lakekeeper API\"\n      user_consent_description   = \"Lakekeeper API\"\n      user_consent_display_name  = \"Access Lakekeeper API\"\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      identifier_uris,\n    ]\n  }\n}\n\nresource \"azuread_application_identifier_uri\" \"lakekeeper\" {\n  application_id = azuread_application.lakekeeper.id\n  identifier_uri = \"api://${azuread_application.lakekeeper.client_id}\"\n}\n\nresource \"azuread_service_principal\" \"lakekeeper_client\" {\n  client_id = azuread_application.lakekeeper.client_id\n\n  feature_tags {\n    enterprise = true\n  }\n}\n\nresource \"azuread_application_pre_authorized\" \"lakekeeper\" {\n  application_id       = azuread_application.lakekeeper.id\n  authorized_client_id = azuread_application_registration.lakekeeper_ui.client_id\n\n  permission_ids = [\n    random_uuid.lakekeeper_scope.id\n  ]\n}\n</code></pre> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bashTerraform <pre><code>// Note the v2.0 at the End of the provider URI!\nLAKEKEEPER__OPENID_PROVIDER_URI=https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0\nLAKEKEEPER__OPENID_AUDIENCE=\"api://&lt;Client ID from App 2 (lakekeeper)&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from App 1 (lakekeeper-ui)&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile api://&lt;Client ID from App 2&gt;/lakekeeper\"\nLAKEKEEPER__OPENID_ADDITIONAL_ISSUERS=\"https://sts.windows.net/&lt;Tenant ID&gt;/\"\n// The additional issuer URL is required as https://login.microsoftonline.com/&lt;Tenant ID&gt;/v2.0/.well-known/openid-configuration\n// shows https://login.microsoftonline.com as the issuer but actually\n// issues tokens for https://sts.windows.net/. This is a well-known\n// problem in Entra ID.\n</code></pre> <pre><code>output \"LAKEKEEPER__OPENID_PROVIDER_URI\" {\n  value = \"https://login.microsoftonline.com/${azuread_service_principal.lakekeeper.application_tenant_id}/v2.0\"\n}\n\noutput \"LAKEKEEPER__OPENID_AUDIENCE\" {\n  value = azuread_application.lakekeeper.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_CLIENT_ID\" {\n  value = azuread_application_registration.lakekeeper_ui.client_id\n}\n\noutput \"LAKEKEEPER__UI__OPENID_SCOPE\" {\n  value = \"openid profile api://${azuread_application.lakekeeper.client_id}/lakekeeper\"\n}\n\noutput \"LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS\" {\n  value = \"https://sts.windows.net/${azuread_service_principal.lakekeeper.application_tenant_id}\"\n}\n</code></pre> <p>Before continuing with App 2, we recommend to create a Warehouse using any of the supported storages. Please check the Storage Documentation for more information. Without a Warehouse, we won't be able to test App 3.</p>"}, {"location": "docs/latest/docs/authentication/#app-3-machine-user", "title": "App 3: Machine User", "text": "<p>Repeat this process for each query engine / machine user that is required:</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Spark</code></li> <li>Redirect URI: Leave empty - we are going to use the Client Credential Flow</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>There might be an additional step needed before you can utilize the machine user. First, get the token for it using the credentials you created on previous steps: <pre><code>curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \\\nhttps://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token \\\n-d 'client_id={client_id}' \\\n-d 'grant_type=client_credentials' \\\n-d 'scope=email openid {APP2_client_id}%2F.default' \\\n-d 'client_secret={client_secret}'\n</code></pre> Note that <code>scope</code> parameter might not accept <code>api://</code> prefix for the APP2 scope for some Entra tenants. In that case, simply use <code>app2_client_id/.default</code> as shown above. Copy the <code>access_token</code> from the response and decode it using jwt.io or any other JWT decode tool. In order for automatic registration to work, token must contain the following claims:<ul> <li><code>app_displayname</code>: name of the APP3 assigned in step 1</li> <li><code>appid</code>: application identifier (client identifier) of the App 3</li> <li><code>idtyp</code>: \"app\" (indicates this is an Entra service principal)</li> </ul> </li> </ol> <p>For some Entra installations you might not get any of those claims in the JWT. <code>idtyp</code> can be added via optional claims in the App Registration of the previously created \"App 2\". Add them to <code>access_token</code> of App 2 and set <code>name</code> to <code>idtyp</code> and <code>essential</code> to <code>true</code>.</p> <p>Alternatively, the following snippets will setup the resources mentioned above:</p> Terraform <pre><code>resource \"azuread_application_registration\" \"my_lakekeeper_machine_user\" {\n  display_name = \"My Lakekeeper Machine User\"\n}\n\nresource \"azuread_service_principal\" \"my_lakekeeper_machine_user\" {\n  client_id = azuread_application_registration.my_lakekeeper_machine_user.client_id\n}\n\n\nresource \"azuread_application_password\" \"my_lakekeeper_machine_user\" {\n  application_id = azuread_application_registration.my_lakekeeper_machine_user.id\n}\n</code></pre> <p>That's it! We can now use the third App Registration to sign into Lakekeeper using Spark or other query engines. A Spark configuration would look like:</p> PyIcebergPySpark <pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"http://localhost:8181/catalog\",\n    warehouse=\"&lt;warehouse name&gt;\",\n    credential=\"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    scope=\"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    **{\n        \"oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\"\n    },\n)\n\nprint(catalog.list_namespaces())\n</code></pre> <pre><code>import pyspark\n\nconf = {\n    \"spark.jars.packages\": \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.apache.iceberg:iceberg-azure-bundle:1.7.0\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.azure-docs\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.azure-docs.type\": \"rest\",\n    \"spark.sql.catalog.azure-docs.uri\": \"http://localhost:8181/catalog\",\n    \"spark.sql.catalog.azure-docs.credential\": \"&lt;Client-ID of App 3 (spark)&gt;:&lt;Client-Secret of App 3 (spark)&gt;\",\n    \"spark.sql.catalog.azure-docs.warehouse\": \"&lt;warehouse name&gt;\",\n    \"spark.sql.catalog.azure-docs.scope\": \"email openid api://&lt;Client-ID of App 2 (lakekeeper)&gt;/.default\",\n    \"spark.sql.catalog.azure-docs.oauth2-server-uri\": \"https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/v2.0/token\",\n}\nconfig = pyspark.SparkConf().setMaster(\"local\")\n\nfor k, v in conf.items():\n    config = config.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=config).getOrCreate()\n\ntry:\n    spark.sql(\"USE `azure-docs`\")\nexcept Exception as e:\n    print(e.stackTrace)\n    raise e\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS `test`\")\nspark.sql(\"CREATE OR REPLACE TABLE `test`.`test_tbl` AS SELECT 1 a\")\n</code></pre> <p>If Authorization is enabled, the client will throw an error as no permissions have been granted yet. During this initial connect to the <code>/config</code> endpoint of Lakekeeper, the user is automatically provisioned so that it should show up when searching for users in the \"Grant\" dialog and user search endpoints. While we try to extract the name of the application from its token, this might not be possible in all setups. As a fallback we use the <code>Client ID</code> as the name of the user. Once permissions have been granted, the user is able to perform actions.</p>"}, {"location": "docs/latest/docs/authentication/#google-identity-platform", "title": "Google Identity Platform", "text": "<p>Warning</p> <p>At the time of writing (June 2025), Google Identity Platform lacks support for the standard OAuth2 Client Credentials Flow, which was established by the IETF in 2012 (!) specifically for machine-to-machine authentication. While the guide below explains how to secure Lakekeeper using Google Identity Platform, this solution only works for human users due to this limitation. For machine authentication, you would need to obtain access tokens through alternative methods outside of the Iceberg client ecosystem and provide them directly to your clients. However, such approaches fall outside the scope of this documentation. To see if google cloud supports client credentials in the meantime, check Google's <code>.well-known/openid-configuration</code>, and search for <code>client_credentials</code> in the <code>grant_types_supported</code> section. When using Lakekeeper with multiple IdPs (i.e. Google &amp; Kubernetes), the second IdP can still be used to authenticate Machines.</p> <p>Fist, read the warning box above (!). Additionally as of June 2025, the Google Identity Platform also does not support standard OAuth2 login flows for \"public clients\" such as Lakekeeper's Web-UI as part of the desired \"Web Application\" client type. Instead, Google still promotes the OAuth Implicit Flow instead of the much more secure Authorization Code Flow with PKCE for public clients. Using the implicit flow is discouraged by the IETF.</p> <p>As we don't want to lower our security or switch to legacy flows, we are using a workaround to register the Lakekeeper UI as a Native Application (Universal Windows Platform in this example), which allows the use of the proper flows, even though it is intended for a different purpose.</p> <p>If you're using Google Cloud Platform, please advocate for proper OAuth standard support by:</p> <ol> <li>Reporting this concern to your Google sales representative</li> <li>Upvoting these issues: 912693, 33416</li> <li>Sharing these discussions: StackOverflow and GitHub issue</li> </ol> <p>Due to these OAuth2 limitations in Google Identity Platform, we cannot recommend it for production deployments. Nevertheless, if you wish to proceed, here's how:</p>"}, {"location": "docs/latest/docs/authentication/#google-auth-platform-project-lakekeeper-application", "title": "Google Auth Platform Project: Lakekeeper Application", "text": "<p>Create a new GCP Project - each Project serves a single application as part of the \"Google Auth Platform\". When the new project is created, create the new internal Lakekeeper Application:</p> <ol> <li>Search for \"Google Auth Platform\", then select \"Branding\" on the left.</li> <li>Select \"Get started\" or modify the pre-filled form:<ul> <li>App Name: Select a good Name, for example <code>Lakekeeper</code></li> <li>User support email: This is shown to users later - select a team e-mail address.</li> <li>Audience: Internal (Otherwise people outside of your organization can login too)</li> <li>Contact Information / Email address: Email Addresses of Lakekeeper Admins or Team Email Address</li> </ul> </li> <li>After the Branding is created, select \"Data access\" in the left menu, and add the following non-sensitive scopes: <code>.../auth/userinfo.email</code>, <code>.../auth/userinfo.profile</code>, <code>openid</code></li> </ol>"}, {"location": "docs/latest/docs/authentication/#client-1-lakekeeper-ui", "title": "Client 1: Lakekeeper UI", "text": "<ol> <li>After the app is created, click in the left menu on \"Clients\" in the \"Google Auth Platform\" service</li> <li>Click on \"+Create credentials\"</li> <li>Select \"Universal Windows Platform (UWP)\" due to the lack of support for public clients in the more appropriate \"Web Application\" type described above. Enter any randomly generated number in the \"Store ID\" field and give the Application a good name, such as <code>Lakekeeper UI</code>. Then click \"Create\". Note the <code>Client ID</code>.</li> </ol> <p>We are now ready to deploy Lakekeeper and login via the UI. Set the following environment variables / configurations:</p> bash <pre><code>LAKEKEEPER__OPENID_PROVIDER_URI=https://accounts.google.com\nLAKEKEEPER__OPENID_AUDIENCE=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_CLIENT_ID=\"&lt;Client ID from Client 1&gt;\"\nLAKEKEEPER__UI__OPENID_SCOPE=\"openid profile\"\n</code></pre> <p>We are now able to login and bootstrap Lakekeeper.</p>"}, {"location": "docs/latest/docs/authentication/#kubernetes", "title": "Kubernetes", "text": "<p>If <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true, Lakekeeper validates incoming tokens against the default kubernetes context of the system. Lakekeeper uses the <code>TokenReview</code> to determine the validity of a token. By default the <code>TokenReview</code> resource is protected. When deploying Lakekeeper on Kubernetes, make sure to grant the <code>system:auth-delegator</code> Cluster Role to the service account used by Lakekeeper:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: allow-token-review\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: &lt;lakekeeper-serviceaccount&gt;\n  namespace: &lt;lakekeeper-namespace&gt;\n</code></pre> <p>The Lakekeeper Helm Chart creates the required binding by default.</p> <p>Applications running in Kubernetes pods can now authenticate using the service account token, which is typically mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Simply read this token and include it in the <code>Authorization</code> header.</p> <p>Example with CURL: <pre><code>curl -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     http://my-lakekeeper:8181/catalog/v1/config\n</code></pre></p> <p>Example with Spark: <pre><code>spark-submit \\\n  --conf spark.sql.catalog.lakekeeper.token=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n  --conf spark.sql.catalog.lakekeeper.uri=\"http://my-lakekeeper:8181/catalog\" \\\n  my-spark-job.py\n</code></pre></p> <p>User identities appear in Lakekeeper as <code>k8s~&lt;namespace&gt;~&lt;service-account-name&gt;</code>.</p>"}, {"location": "docs/latest/docs/authorization/", "title": "Authorization", "text": ""}, {"location": "docs/latest/docs/authorization/#overview", "title": "Overview", "text": "<p>Authentication verifies who you are, while authorization determines what you can do.</p> <p>Authorization can only be enabled if Authentication is enabled. Please check the Authentication Docs for more information.</p> <p>Lakekeeper currently supports the following Authorizers:</p> <ul> <li>AllowAll: A simple authorizer that allows all requests. This is mainly intended for development and testing purposes.</li> <li>OpenFGA: A fine-grained authorization system based on the CNCF project OpenFGA. Please find more information in the Authorization with OpenFGA section. OpenFGA requires an additional OpenFGA service to be deployed (this is included in our self-contained examples and our helm charts).</li> <li>Cedar: An enterprise-grade policy-based authorization system based on Cedar. The cedar authorizer is built into Lakekeeper and requires no additional external services. Please find more information in the Authorization with Cedar section.</li> <li>Custom: Lakekeeper supports custom authorizers via the <code>Authorizer</code> trait.</li> </ul>"}, {"location": "docs/latest/docs/authorization/#authorization-with-openfga", "title": "Authorization with OpenFGA", "text": "<p>Lakekeeper can use OpenFGA to store and evaluate permissions. OpenFGA provides bi-directional inheritance, which is key for managing hierarchical namespaces in modern lakehouses. For query engines like Trino, Lakekeeper's OPA bridge translates OpenFGA permissions into Open Policy Agent (OPA) format. See the OPA Bridge Guide for details.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/latest/docs/authorization/#grants", "title": "Grants", "text": "<p>The default permission model is focused on collaborating on data. Permissions are additive. The underlying OpenFGA model is defined in <code>schema.fga</code> on GitHub. The following grants are available:</p> Entity Grant server admin, operator project project_admin, security_admin, data_admin, role_creator, describe, select, create, modify warehouse ownership, pass_grants, manage_grants, describe, select, create, modify namespace ownership, pass_grants, manage_grants, describe, select, create, modify table ownership, pass_grants, manage_grants, describe, select, modify view ownership, pass_grants, manage_grants, describe, modify role assignee, ownership"}, {"location": "docs/latest/docs/authorization/#ownership", "title": "Ownership", "text": "<p>Owners of objects have all rights on the specific object. When principals create new objects, they automatically become owners of these objects. This enables powerful self-service szenarios where users can act autonomously in a (sub-)namespace. By default, Owners of objects are also able to access grants on objects, which enables them to expand the access to their owned objects to new users. Enabling Managed Access for a Warehouse or Namespace removes the <code>grant</code> privilege from owners.</p>"}, {"location": "docs/latest/docs/authorization/#server-admin", "title": "Server: Admin", "text": "<p>A <code>server</code>'s <code>admin</code> role is the most powerful role (apart from <code>operator</code>) on the server. In order to guarantee auditability, this role can list and administrate all Projects, but does not have access to data in projects. While the <code>admin</code> can assign himself the <code>project_admin</code> role for a project, this assignment is tracked by <code>OpenFGA</code> for audits. <code>admin</code>s can also manage all projects (but no entities within it), server settings and users.</p>"}, {"location": "docs/latest/docs/authorization/#server-operator", "title": "Server: Operator", "text": "<p>The <code>operator</code> has unrestricted access to all objects in Lakekeeper. It is designed to be used by technical users (e.g., a Kubernetes Operator) managing the Lakekeeper deployment.</p>"}, {"location": "docs/latest/docs/authorization/#project-security-admin", "title": "Project: Security Admin", "text": "<p>A <code>security_admin</code> in a project can manage all security-related aspects, including grants and ownership for the project and all objects within it. However, they cannot modify or access the content of any object, except for listing and browsing purposes.</p>"}, {"location": "docs/latest/docs/authorization/#project-data-admin", "title": "Project: Data Admin", "text": "<p>A <code>data_admin</code> in a project can manage all data-related aspects, including creating, modifying, and deleting objects within the project. They can delegate the <code>data_admin</code> role they already hold (for example to team members), but they do not have general grant or ownership administration capabilities.</p>"}, {"location": "docs/latest/docs/authorization/#project-admin", "title": "Project: Admin", "text": "<p>A <code>project_admin</code> in a project has the combined responsibilities of both <code>security_admin</code> and <code>data_admin</code>. They can manage all security-related aspects, including grants and ownership, as well as all data-related aspects, including creating, modifying, and deleting objects within the project.</p>"}, {"location": "docs/latest/docs/authorization/#project-role-creator", "title": "Project: Role Creator", "text": "<p>A <code>role_creator</code> in a project can create new roles within it. This role is essential for delegating the creation of roles without granting broader administrative privileges.</p>"}, {"location": "docs/latest/docs/authorization/#describe", "title": "Describe", "text": "<p>The <code>describe</code> grant allows a user to view metadata and details about an object without modifying it. This includes listing objects and viewing their properties. The <code>describe</code> grant is inherited down the object hierarchy, meaning if a user has the <code>describe</code> grant on a higher-level entity, they can also describe all child entities within it. The <code>describe</code> grant is implicitly included with the <code>select</code>, <code>create</code>, and <code>modify</code> grants.</p>"}, {"location": "docs/latest/docs/authorization/#select", "title": "Select", "text": "<p>The <code>select</code> grant allows a user to read data from an object, such as tables or views. This includes querying and retrieving data. The <code>select</code> grant is inherited down the object hierarchy, meaning if a user has the <code>select</code> grant on a higher-level entity, they can select all views and tables within it. The <code>select</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/latest/docs/authorization/#create", "title": "Create", "text": "<p>The <code>create</code> grant allows a user to create new objects within an entity, such as tables, views, or namespaces. The <code>create</code> grant is inherited down the object hierarchy, meaning if a user has the <code>create</code> grant on a higher-level entity, they can also create objects within all child entities. The <code>create</code> grant implicitly includes the <code>describe</code> grant.</p>"}, {"location": "docs/latest/docs/authorization/#modify", "title": "Modify", "text": "<p>The <code>modify</code> grant allows a user to change the content or properties of an object, such as updating data in tables or altering views. The <code>modify</code> grant is inherited down the object hierarchy, meaning if a user has the <code>modify</code> grant on a higher-level entity, they can also modify all child entities within it. The <code>modify</code> grant implicitly includes the <code>select</code> and <code>describe</code> grants.</p>"}, {"location": "docs/latest/docs/authorization/#pass-grants", "title": "Pass Grants", "text": "<p>The <code>pass_grants</code> grant allows a user to pass their own privileges to other users. This means that if a user has certain permissions on an object, they can grant those same permissions to others. However, the <code>pass_grants</code> grant does not include the ability to pass the <code>pass_grants</code> privilege itself.</p>"}, {"location": "docs/latest/docs/authorization/#manage-grants", "title": "Manage Grants", "text": "<p>The <code>manage_grants</code> grant allows a user to manage all grants on an object, including creating, modifying, and revoking grants. This also includes <code>manage_grants</code> and <code>pass_grants</code>.</p>"}, {"location": "docs/latest/docs/authorization/#inheritance", "title": "Inheritance", "text": "<ul> <li>Top-Down-Inheritance: Permissions in higher up entities are inherited to their children. For example if the <code>modify</code> privilege is granted on a <code>warehouse</code> for a principal, this principal is also able to <code>modify</code> any namespaces, including nesting ones, tables and views within it.</li> <li>Bottom-Up-Inheritance: Permissions on lower entities, for example tables, inherit basic navigational privileges to all higher layer principals. For example, if a user is granted the <code>select</code> privilege on table <code>ns1.ns2.table_1</code>, that user is implicitly granted limited list privileges on <code>ns1</code> and <code>ns2</code>. Only items in the direct path are presented to users. If <code>ns1.ns3</code> would exist as well, a list on <code>ns1</code> would only show <code>ns1.ns2</code>.</li> </ul>"}, {"location": "docs/latest/docs/authorization/#managed-access", "title": "Managed Access", "text": "<p>Managed access is a feature designed to provide stricter control over access privileges within Lakekeeper. It is particularly useful for organizations that require a more restrictive access control model to ensure data security and compliance.</p> <p>In some cases, the default ownership model, which grants all privileges to the creator of an object, can be too permissive. This can lead to situations where non-admin users unintentionally share data with unauthorized users by granting privileges outside the scope defined by administrators. Managed access addresses this concern by removing the <code>grant</code> privilege from owners and centralizing the management of access privileges.</p> <p>With managed access, admin-like users can define access privileges on high-level container objects, such as warehouses or namespaces, and ensure that all child objects inherit these privileges. This approach prevents non-admin users from granting privileges that are not authorized by administrators, thereby reducing the risk of unintentional data sharing and enhancing overall security.</p> <p>Managed access combines elements of Role-Based Access Control (RBAC) and Discretionary Access Control (DAC). While RBAC allows privileges to be assigned to roles and users, DAC assigns ownership to the creator of an object. By integrating managed access, Lakekeeper provides a balanced access control model that supports both self-service analytics and data democratization while maintaining strict security controls.</p> <p>Managed access can be enabled or disabled for warehouses and namespaces using the UI or the <code>../managed-access</code> Endpoints. Managed access settings are inherited down the object hierarchy, meaning if managed access is enabled on a higher-level entity, it applies to all child entities within it.</p>"}, {"location": "docs/latest/docs/authorization/#best-practices", "title": "Best Practices", "text": "<p>We recommend separating access to data from the ability to grant privileges. To achieve this, the <code>security_admin</code> and <code>data_admin</code> roles divide the responsibilities of the initial <code>project_admin</code>, who has the authority to perform tasks in both areas.</p>"}, {"location": "docs/latest/docs/authorization/#openfga-in-production", "title": "OpenFGA in Production", "text": "<p>When deploying OpenFGA in production environments, ensure you follow the OpenFGA Production Checklist.</p> <p>Lakekeeper includes Query Consistency specifications with each authorization request to OpenFGA. For most operations, <code>MINIMIZE_LATENCY</code> consistency provides optimal performance while maintaining sufficient data consistency guarantees.</p> <p>For medium to large-scale deployments, we strongly recommend enabling caching in OpenFGA and increasing the database connection pool limits. These optimizations significantly reduce database load and improve authorization latency. Configure the following environment variables in OpenFGA (written for version 1.10). You may increase the number of connections further if your database deployment can handle additional connections:</p> <pre><code>OPENFGA_DATASTORE_MAX_OPEN_CONNS=200\nOPENFGA_DATASTORE_MAX_IDLE_CONNS=100\nOPENFGA_CACHE_CONTROLLER_ENABLED=true\nOPENFGA_CHECK_QUERY_CACHE_ENABLED=true\nOPENFGA_CHECK_ITERATOR_CACHE_ENABLED=true\n</code></pre>"}, {"location": "docs/latest/docs/authorization/#authorization-with-cedar", "title": "Authorization with Cedar", "text": "<p>Cedar is an enterprise-grade, policy-based authorization system built into Lakekeeper that requires no external services. Cedar uses a declarative policy language to define access controls, making it ideal for organizations that prefer infrastructure-as-code approaches to authorization management.</p> <p>Check the Authorization Configuration for setup details.</p>"}, {"location": "docs/latest/docs/authorization/#schema-and-entity-model", "title": "Schema and Entity Model", "text": "<p>For each authorization request, Lakekeeper provides the complete entity hierarchy from the requested resource up to the server level. This ensures policies have full context for making authorization decisions.</p> <p>When a user queries table <code>ns1.ns2.table1</code> in warehouse <code>wh-1</code> within project <code>my-project</code>, Cedar receives the following entities:</p> <ul> <li><code>Server</code> (root)</li> <li><code>Project::\"my-project\"</code></li> <li><code>Warehouse::\"wh-1\"</code> (parent: <code>my-project</code>)</li> <li><code>Namespace::\"ns1\"</code> (parent: <code>wh-1</code>)</li> <li><code>Namespace::\"ns2\"</code> (parent: <code>ns1</code>)</li> <li><code>Table::\"table1\"</code> (parent: <code>ns2</code>)</li> </ul> <p>This hierarchical context allows policies to reference any level in the path. For example, you can write policies that grant access based on the warehouse name, namespace hierarchy, or specific table properties.</p> <p>The Lakekeeper Cedar schema defines all available entity types, attributes, and actions. All loaded entities and policies are validated against this schema on startup and refresh. You can download the schema here: lakekeeper.cedarschema or find it on GitHub.</p> <p>Important: Lakekeeper does not provide Roles as built-in entities. Roles must be defined as custom entities in your entity JSON files.</p>"}, {"location": "docs/latest/docs/authorization/#policy-examples", "title": "Policy Examples", "text": "<p>Grant admin access to a specific user: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action,\n    resource\n);\n</code></pre></p> <p>Role-based warehouse access: <pre><code>// Grant full access to all entities in a warehouse with name \"wh-1\"\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"NamespaceActions\",\n               Lakekeeper::Action::\"TableActions\",\n               Lakekeeper::Action::\"ViewActions\"],\n    resource\n)\nwhen { resource.warehouse.name == \"wh-1\" };\n\n// Allow modification of the warehouse itself\npermit (\n    principal in Lakekeeper::Role::\"warehouse-1-admins\",\n    action in [Lakekeeper::Action::\"WarehouseModifyActions\"],\n    resource\n)\nwhen { resource.name == \"wh-1\" };\n</code></pre></p> <p>Table read access for all tables in the <code>analytics</code> namespace of warehouse <code>wh-1</code>: <pre><code>permit (\n    principal == Lakekeeper::User::\"oidc~&lt;sub-field-from-user-token&gt;\",\n    action in [Lakekeeper::Action::\"TableSelectActions\"],\n    resource\n)\nwhen {\n    resource.namespace.name == \"analytics\" &amp;&amp;\n    resource.warehouse.name == \"wh-1\"\n};\n</code></pre></p>"}, {"location": "docs/latest/docs/authorization/#entity-definition-example", "title": "Entity Definition Example", "text": "<p>Define roles and assign users to them using JSON entity files:</p> <pre><code>[\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::User\",\n            \"id\": \"oidc~90471f73-e338-4032-9a6b-1e021cc3cb1e\"\n        },\n        \"attrs\": {\n            \"display_name\": \"machine-user-1\"\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"data-engineering\"\n            }\n        ]\n    },\n    {\n        \"uid\": {\n            \"type\": \"Lakekeeper::Role\",\n            \"id\": \"data-engineering\"\n        },\n        \"attrs\": {\n            \"name\": \"DataEngineering\",\n            \"project\": {\n                \"__entity\": {\n                    \"type\": \"Lakekeeper::Project\",\n                    \"id\": \"00000000-0000-0000-0000-000000000000\"\n                }\n            }\n        },\n        \"parents\": [\n            {\n                \"type\": \"Lakekeeper::Role\",\n                \"id\": \"warehouse-1-admins\"\n            }\n        ]\n    }\n]\n</code></pre>"}, {"location": "docs/latest/docs/authorization/#policy-and-entity-management", "title": "Policy and Entity Management", "text": "<p>Startup Behavior:</p> <ul> <li>All policy and entity files are loaded and validated against the Cedar schema</li> <li>If any file is unreadable or invalid, Lakekeeper fails to start with an error</li> </ul> <p>This ensures that authorization policies are always valid before serving requests</p> <p>Refresh Behavior: Configure automatic policy refresh using <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> (default: 5 seconds):</p> <ol> <li>Change Detection: Lightweight checks monitor ConfigMap versions and file timestamps</li> <li>Reload on Change: Modified entity or policy files trigger a full reload of all files to guarantee consistency</li> <li>Atomic Updates: The in-memory store is only updated if all files reload successfully</li> <li>Error Handling: If any reload fails, the previous configuration is retained, an error is logged, and health checks report unhealthy status</li> </ol> <p>This approach ensures that authorization policies remain consistent and that partial updates never compromise security.</p>"}, {"location": "docs/latest/docs/bootstrap/", "title": "Bootstrap / Initialize", "text": "<p>After the initial deployment, Lakekeeper needs to be bootstrapped. This can be done via the UI or the <code>/management/v1/bootstrap</code> endpoint. A typical POST request to bootstrap Lakekeeper looks like this:</p> <pre><code>curl --location 'https://&lt;lakekeeper-url&gt;/management/v1/bootstrap' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;my-bearer-token&gt;' \\\n--data '{\n    \"accept-terms-of-use\": true\n}'\n</code></pre> <p><code>&lt;my-bearer-token&gt;</code> is obtained by logging into the IdP before bootstrapping Lakekeeper. If authentication is disabled, no token is required. Lakekeeper can only be bootstrapped once.</p> <p>During bootstrapping, Lakekeeper performs the following actions:</p> <ul> <li>Grants the server's <code>admin</code> role to the user performing the POST request. The user is identified by their token. If authentication is disabled, the <code>Authorization</code> header is not required, and no <code>admin</code> is set, as permissions are disabled in this case.</li> <li>Stores the current Server ID to prevent unwanted future changes of the Server ID that would break permissions.</li> <li>Accepts terms of use as defined by our License.</li> <li>If <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is enabled (default), a default project with the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is created.</li> </ul> <p>If the initial user is a technical user (e.g., a Kubernetes Operator) managing the Lakekeeper deployment, the <code>admin</code> role might not be sufficient as it limits access to projects until the <code>admin</code> grants themselves permission. For technical users, the <code>operator</code> role grants full access to all APIs and can be obtained by adding <code>\"is-operator\": true</code> to the JSON body of the bootstrap request.</p>"}, {"location": "docs/latest/docs/concepts/", "title": "Concepts", "text": ""}, {"location": "docs/latest/docs/concepts/#architecture", "title": "Architecture", "text": "<p>Lakekeeper is an implementation of the Apache Iceberg REST Catalog API.  Lakekeeper depends on the following, partially optional, external dependencies:</p> Connected systems. Green boxes are recommended for production. <ul> <li>Persistence Backend / Catalog (required): We currently support only Postgres, but plan to expand our support to more Databases in the future.</li> <li>Warehouse Storage (required): When a new Warehouse is created, storage credentials are required.</li> <li>Identity Provider (optional): Lakekeeper can authenticate incoming requests using any OIDC capable Identity Provider (IdP). Lakekeeper can also natively authenticate kubernetes service accounts.</li> <li>Authorization System (optional): For permission management, Lakekeeper supports different Authorizers. Please refer to the Authorization Documentation for more information.</li> <li>Secret Store (required): Lakekeeper requires a Secret Store to stores secrets such as Warehouse credentials. By default, Lakekeeper uses the default Postgres connection to store encrypted secrets. To increase security, Lakekeeper can also use external systems to store secrets. Currently all Hashicorp-Vault like stores are supported.</li> <li>Event Store (optional): Lakekeeper can send Change Events to an Event Store. We support NATS and Apache Kafka</li> <li>Data Contract System (optional): Lakekeeper can interface with external data contract systems to prohibit breaking changes to your tables.</li> </ul> <p>To get started quickly with the latest version of Lakekeeper check our Getting Started Guide.</p>"}, {"location": "docs/latest/docs/concepts/#entity-hierarchy", "title": "Entity Hierarchy", "text": "<p>In addition to entities defined in the Apache Iceberg specification or the REST specification (Namespaces, Tables, etc.), Lakekeeper introduces new entities for permission management and multi-tenant setups. The following entities are available in Lakekeeper:</p> <p></p> Lakekeeper Entity Hierarchy <p></p> <p>Project, Server, User and Roles are entities unknown to the Iceberg Rest Specification. Lakekeeper serves two APIs:</p> <ol> <li>The Iceberg REST API is served at endpoints prefixed with <code>/catalog</code>. External query engines connect to this API to interact with the Lakekeeper. Lakekeeper also implements the S3 remote signing API which is hosted at <code>/&lt;warehouse-id&gt;/v1/aws/s3/sign</code>.</li> <li>The Lakekeeper Management API is served at endpoints prefixed with <code>/management</code>. It is used to configure Lakekeeper and manage entities that are not part of the Iceberg REST Catalog specification, such as permissions.</li> </ol>"}, {"location": "docs/latest/docs/concepts/#server", "title": "Server", "text": "<p>The Server is the highest entity in Lakekeeper, representing a single instance or a cluster of Lakekeeper pods sharing a common state. Each server has a unique identifier (UUID). The Server ID is generated randomly on first startup and stored in the Database Backend.</p>"}, {"location": "docs/latest/docs/concepts/#project", "title": "Project", "text": "<p>For single-company setups, we recommend using a single Project setup, which is the default. Unless <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> is explicitly set to <code>false</code>, a default project is created during bootstrapping with the nil UUID.</p>"}, {"location": "docs/latest/docs/concepts/#warehouse", "title": "Warehouse", "text": "<p>Each Project can contain multiple Warehouses. Query engines connect to Lakekeeper by specifying a Warehouse name in the connection configuration.</p> <p>Each Warehouse is associated with a unique location on object stores. Never share locations between Warehouses to ensure no data is leaked via vended credentials. Each Warehouse stores information on how to connect to its location via a <code>storage-profile</code> and an optional <code>storage-credential</code>.</p> <p>Warehouses can be configured to use Soft-Deletes. When enabled, tables are not eagerly deleted but kept in a deleted state for a configurable amount of time. During this time, they can be restored. Please note that Warehouses and Namespaces cannot be deleted via the <code>/catalog</code> API if child objects are present. This includes soft-deleted Tables. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/latest/docs/concepts/#namespaces", "title": "Namespaces", "text": "<p>Each Warehouses can contain multiple Namespaces. Namespaces can be nested and serve as containers for Namespaces, Tables and Views. Using the <code>/catalog</code> API, a Namespace cannot be dropped unless it is empty. A cascade-drop API is added in one of the next releases as part of the <code>/management</code> API.</p>"}, {"location": "docs/latest/docs/concepts/#tables-views", "title": "Tables &amp; Views", "text": "<p>Each Namespace can contain multiple Tables and Views. When creating new Tables and Views, we recommend to not specify the <code>location</code> explicitly. If locations are specified explicitly, the location must be a valid sub location of the <code>storage-profile</code> of the Warehouse - this is validated by Lakekeeper upon creation. Lakekeeper also ensures that there are no Tables or Views that use a parent- or sub-folder as their <code>location</code> and that the location is empty on creation. These checks are required to ensure that no data is leaked via vended-credentials.</p>"}, {"location": "docs/latest/docs/concepts/#users", "title": "Users", "text": "<p>Lakekeeper is no Identity Provider. The identities of users are exclusively managed via an external Identity Provider to ensure compliance with basic security standards. Lakekeeper does not store any Password / Certificates / API Keys or any other secret that grants access to data for users. Instead, we only store Name, Email and type of users with the sole purpose of providing a convenient search while assigning privileges.</p> <p>Users can be provisioned to Lakekeeper by either of the following endpoints:</p> <ul> <li>Explicit user creation via the POST <code>/management/user</code> endpoint. This endpoint is called automatically by the UI upon login. Thus, users are \"searchable\" after their first login to the UI.</li> <li>Implicit on-the-fly creation when calling GET <code>/catalog/v1/config</code>. This can be used to register technical users simply by connecting to the Lakekeeper with your favorite tool (i.e. Spark). The initial connection will probably fail because privileges are missing to use this endpoint, but the user is provisioned anyway so that privileges can be assigned before re-connecting.</li> </ul>"}, {"location": "docs/latest/docs/concepts/#roles", "title": "Roles", "text": "<p>Projects can contain multiple Roles, allowing Roles to be reused in all Warehouses within the Project. Roles can be nested arbitrarily, meaning that a role can contain other roles within it. Roles can be provisioned automatically using the <code>/management/v1/role</code> endpoint or manually created via the UI. We are looking into SCIM support to simplify role provisioning. Please consider upvoting the corresponding GitHub Issue if this would be of interest to you.</p>"}, {"location": "docs/latest/docs/concepts/#dropping-tables", "title": "Dropping Tables", "text": "<p>Currently all tables stored in Lakekeeper are assumed to be managed by Lakekeeper. The concept of \"external\" tables will follow in a later release. When managed tables are dropped, Lakekeeper defaults to setting <code>purgeRequested</code> parameter of the <code>dropTable</code> endpoint to true unless explicitly set to false. Currently most query engines do not set this flag, which defaults to enabling purge. If purge is enabled for a drop, all files of the table are removed.</p>"}, {"location": "docs/latest/docs/concepts/#soft-deletion", "title": "Soft Deletion", "text": "<p>Lakekeeper allows warehouses to enable soft deletion as a data protection mechanism. When enabled:</p> <ul> <li>Tables and views aren't immediately removed from the catalog when dropped</li> <li>Instead, they're marked as deleted and scheduled for cleanup</li> <li>The data remains recoverable until the configured expiration period elapses</li> <li>Recovery is only possible for warehouses with soft deletion enabled</li> <li>The expiration delay is fixed at the time of dropping - changing warehouse settings only affects newly dropped tables</li> </ul> <p>Soft deletion works correctly only when clients follow these behaviors:</p> <ol> <li> <p><code>DROP TABLE xyz</code> (standard): Clients should not remove any files themselves, and should call the <code>dropTable</code> endpoint without the <code>purgeRequested</code> flag. Lakekeeper handles file removal for managed tables. This works well with all query engines.</p> </li> <li> <p><code>DROP TABLE xyz PURGE</code>: Clients should not delete files themselves, and should call the <code>dropTable</code> endpoint with the <code>purgeRequested</code> flag set to true. Lakekeeper will remove files for managed tables (and for unmanaged tables in a future release). Unfortunately not all query engines adhere to this behavior, as described below.</p> </li> </ol> <p>Unfortunately, some Java-based query engines like Spark don't follow the expected behavior for <code>PURGE</code> operations. Instead, they immediately delete files, which undermines soft deletion functionality. The Apache Iceberg community has agreed to fix this in Iceberg 2.0. For Iceberg 1.x versions, we're working on a new <code>io.client-side.purge-enabled</code> flag for better control.</p> <p>Warning</p> <p>Never use <code>DROP TABLE xyz PURGE</code> with clients like Spark that immediately remove files when soft deletion is enabled!</p> <p>For S3-based storage, Lakekeeper provides a protective configuration option in storage profiles: <code>push-s3-delete-disabled</code>. When set to <code>true</code>, this:</p> <ul> <li>Prevents clients from deleting files by pushing the <code>s3.delete-enabled: false</code> setting to clients</li> <li>Preserves soft deletion functionality even when <code>PURGE</code> is specified</li> <li>Affects all file deletion operations, including maintenance procedures like <code>expire_snapshots</code></li> </ul> <p>When running table maintenance procedures that need to remove files with <code>push-s3-delete-disabled: true</code>, you must explicitly override with <code>s3.delete-enabled: true</code> in your client configuration:</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # ... Additional configuration options\n    # THE FOLLOWING IS THE NEW OPTION:\n    # Enabling s3 deletion explicitly - this overrides any Lakekeeper setting\n    f\"spark.sql.catalog.{catalog_name}.s3.delete-enabled\": \"true\",\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/latest/docs/concepts/#protection-and-deletion-mechanisms-in-lakekeeper", "title": "Protection and Deletion Mechanisms in Lakekeeper", "text": "<p>Lakekeeper provides several complementary mechanisms for protecting data assets and managing their deletion while balancing flexibility and data governance.</p>"}, {"location": "docs/latest/docs/concepts/#protection", "title": "Protection", "text": "<p>Protection prevents accidental deletion of important entities in Lakekeeper. When an entity is protected, attempts to delete it through standard API calls will be rejected.</p> <p>Protection can be applied to Warehouses, Namespaces, Tables, and Views via the Management API.</p>"}, {"location": "docs/latest/docs/concepts/#recursive-deletion-on-namespaces", "title": "Recursive Deletion on Namespaces", "text": "<p>By default, Lakekeeper enforces that namespaces must be empty before deletion. Recursive deletion provides a way to delete a namespace and all its contained entities in a single operation.</p> <p>When deleting a namespace, add the recursive=true query parameter to the request.</p> <p>Protected entities within the hierarchy will prevent recursive deletion unless force is also used.</p>"}, {"location": "docs/latest/docs/concepts/#force-deletion", "title": "Force Deletion", "text": "<p>Force deletion is an administrative override that allows deletion of protected entities and bypasses certain safety checks:</p> <ul> <li>Bypasses protection settings</li> <li>Overrides soft-deletion mechanisms for immediate hard deletion</li> </ul> <p>Add the <code>force=true</code> query parameter to deletion requests: <pre><code>DELETE /catalog/v1/{prefix}/namespaces/{namespace}?force=true\n</code></pre></p> <p>Force can be combined with recursive deletion (<code>recursive=true&amp;force=true</code>) to delete an entire protected hierarchy. The <code>purgeRequested</code> flag for tables is still respected and determines if the physical data of the table should be removed. Purge defaults to true for tables managed by Lakekeeper.</p>"}, {"location": "docs/latest/docs/concepts/#upgrades-migration", "title": "Upgrades &amp; Migration", "text": "<p>Lakekeeper relies on a persistent backend (Postgres) and an optional authorization system (OpenFGA). As Lakekeeper evolves, these systems may need schema or configuration updates to support new features and improvements. The <code>lakekeeper migrate</code> command initializes and updates both Postgres schemas (creating necessary tables and structures) and authorization models to ensure compatibility with your current Lakekeeper version.</p> <p>Migration is required before each Lakekeeper upgrade. You must run the migration before starting the <code>lakekeeper serve</code> command to ensure all system components are properly updated and configured. Without running the migration first, the <code>lakekeeper serve</code> command will fail to start with the error: \"Database is not up to date with binary, make sure to run the migrate command before starting the server.\" Migrations are designed to be resilient - you can safely skip intermediate versions and migrate directly to your target version. If the system is already up to date, the migration command will exit immediately without making any changes.</p> <p>All migrations run within a transaction, ensuring that either the entire migration completes successfully or the database remains unchanged. This prevents partial migrations that could leave your system in an inconsistent state.</p> <p>Always create a backup of your Postgres database before running migrations. While migrations are designed to be safe, having a backup ensures you can restore your system to a known good state if needed.</p> <p>When using the Lakekeeper Helm Chart, migrations are handled automatically through a dedicated job during deployment.</p>"}, {"location": "docs/latest/docs/configuration/", "title": "Configuration", "text": "<p>Lakekeeper is configured via environment variables. Settings listed in this page are shared between all projects and warehouses. Previous to Lakekeeper Version <code>0.5.0</code> please prefix all environment variables with <code>ICEBERG_REST__</code> instead of <code>LAKEKEEPER__</code>.</p> <p>For most deployments, we recommend to set at least the following variables: <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>, <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code>.</p>"}, {"location": "docs/latest/docs/configuration/#routing-and-base-url", "title": "Routing and Base-URL", "text": "<p>Some Lakekeeper endpoints return links pointing at Lakekeeper itself. By default, these links are generated using the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers, if these are not present, the <code>host</code> header is used. If this is not working for you, you may set the <code>LAKEKEEPER_BASE_URI</code> environment variable to the base-URL where Lakekeeper is externally reachable. This may be necessary if Lakekeeper runs behind a reverse proxy or load balancer, and you cannot set the headers accordingly. In general, we recommend relying on the headers. To respect the <code>host</code> header but not the <code>x-forwarded-</code> headers, set <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> to <code>false</code>.</p>"}, {"location": "docs/latest/docs/configuration/#general", "title": "General", "text": "Variable Example Description <code>LAKEKEEPER__BASE_URI</code> <code>https://example.com:8181</code> Optional base-URL where the catalog is externally reachable. Default: <code>None</code>. See Routing and Base-URL. <code>LAKEKEEPER__ENABLE_DEFAULT_PROJECT</code> <code>true</code> If <code>true</code>, the NIL Project ID (\"00000000-0000-0000-0000-000000000000\") is used as a default if the user does not specify a project when connecting. This option is enabled by default, which we recommend for all single-project (single-tenant) setups. Default: <code>true</code>. <code>LAKEKEEPER__RESERVED_NAMESPACES</code> <code>system,examples,information_schema</code> Reserved Namespaces that cannot be created via the REST interface <code>LAKEKEEPER__METRICS_PORT</code> <code>9000</code> Port where the Prometheus metrics endpoint is reachable. Default: <code>9000</code> <code>LAKEKEEPER__LISTEN_PORT</code> <code>8181</code> Port Lakekeeper listens on. Default: <code>8181</code> <code>LAKEKEEPER__BIND_IP</code> <code>0.0.0.0</code>, <code>::1</code>, <code>::</code> IP Address Lakekeeper binds to. Default: <code>0.0.0.0</code> (listen to all incoming IPv4 packages) <code>LAKEKEEPER__SECRET_BACKEND</code> <code>postgres</code> The secret backend to use. If <code>kv2</code> (Hashicorp KV Version 2) is chosen, you need to provide additional parameters Default: <code>postgres</code>, one-of: [<code>postgres</code>, <code>kv2</code>] <code>LAKEKEEPER__SERVE_SWAGGER_UI</code> <code>true</code> If <code>true</code>, Lakekeeper serves a swagger UI for management &amp; catalog openAPI specs under <code>/swagger-ui</code> <code>LAKEKEEPER__ALLOW_ORIGIN</code> <code>*</code> A comma separated list of allowed origins for CORS. <code>LAKEKEEPER__USE_X_FORWARDED_HEADERS</code> <code>false</code> If true, Lakekeeper respects the <code>x-forwarded-host</code>, <code>x-forwarded-proto</code>, <code>x-forwarded-port</code> and <code>x-forwarded-prefix</code> headers in incoming requests. This is mostly relevant for the <code>/config</code> endpoint. Default: <code>true</code> (Headers are respected.)"}, {"location": "docs/latest/docs/configuration/#pagination", "title": "Pagination", "text": "<p>Lakekeeper has default values for <code>default</code> and <code>max</code> page sizes of paginated queries. These are safeguards against malicious requests and the problems related to large page sizes described below.</p> <p>The REST catalog spec requires servers to return all results if <code>pageToken</code> is not set in the request. To obtain that behavior, set <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> to 4294967295, which corresponds to <code>u32::MAX</code>. Larger page sizes would lead to practical problems. Things to keep in mind:</p> <ul> <li>Retrieving huge numbers of rows is expensive, which might be exploited by malicious requests.</li> <li>Requests may time out or responses may exceed size limits for huge numbers of results. </li> </ul> Variable Example Description <code>LAKEKEEPER__PAGINATION_SIZE_DEFAULT</code> <code>1024</code> The default page size used for paginated queries. This value is used if the request's <code>pageToken</code> is set but empty. Default: <code>100</code> <code>LAKEKEEPER__PAGINATION_SIZE_MAX</code> <code>2048</code> The max page size used for paginated queries. This value is used if the request's <code>pageToken</code> is not set. Default: <code>1000</code>"}, {"location": "docs/latest/docs/configuration/#storage", "title": "Storage", "text": "Variable Example Description <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using AWS system identities (i.e. through <code>AWS_*</code> environment variables or EC2 instance profiles) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable AWS system identities, set <code>LAKEKEEPER__ENABLE_AWS_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (AWS system credentials disabled) <code>LAKEKEEPER__S3_ENABLE_DIRECT_SYSTEM_CREDENTIALS</code> <code>true</code> By default, when using AWS system credentials, users must specify an <code>assume-role-arn</code> for Lakekeeper to assume when accessing S3. Setting this option to <code>true</code> allows Lakekeeper to use system credentials directly without role assumption, meaning the system identity must have direct access to warehouse locations. Default: <code>false</code> (direct system credential access disabled) <code>LAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS</code> <code>true</code> Controls whether an <code>external-id</code> is required when assuming a role with AWS system credentials. External IDs provide additional security when cross-account role assumption is used. Default: true (external ID required) <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using Azure system identities (i.e. through <code>AZURE_*</code> environment variables or VM managed identities) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable Azure system identities, set <code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (Azure system credentials disabled) <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> <code>true</code> Lakekeeper supports using GCP system identities (i.e. through <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variables or the Compute Engine Metadata Server) as storage credentials for warehouses. This feature is disabled by default to prevent accidental access to restricted storage locations. To enable GCP system identities, set <code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS</code> to <code>true</code>. Default: <code>false</code> (GCP system credentials disabled)"}, {"location": "docs/latest/docs/configuration/#persistence-store", "title": "Persistence Store", "text": "<p>Currently Lakekeeper supports only Postgres as a persistence store. You may either provide connection strings using <code>PG_DATABASE_URL_*</code> or use the <code>PG_*</code> environment variables. Connection strings take precedence. Postgres needs to be Version 15 or higher.</p> <p>Lakekeeper supports configuring separate database URLs for read and write operations, allowing you to utilize read replicas for better scalability. By directing read queries to dedicated replicas via <code>LAKEKEEPER__PG_DATABASE_URL_READ</code>, you can significantly reduce load on your database primary (specified by <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>), improving overall system performance as your deployment scales. This separation is particularly beneficial for read-heavy workloads. When using read replicas, be aware that replication lag may occur between the primary and replica databases depending on your Database setup. This means that immediately after a write operation, the changes might not be instantly visible when querying a read-only Lakekeeper endpoint (which uses the read replica). Consider this potential lag when designing applications that require immediate read-after-write consistency. For deployments where read-after-write consistency is critical, you can simply omit the <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> setting, which will cause all operations to use the primary database connection. </p> Variable Example Description <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for reading. Defaults to <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code>. <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> <code>postgres://postgres:password@localhost:5432/iceberg</code> Postgres Database connection string used for writing. If <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> is not specified, this connection is also used for reading. <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> <code>This is unsafe, please set a proper key</code> If <code>LAKEKEEPER__SECRET_BACKEND=postgres</code>, this key is used to encrypt secrets. It is required to change this for production deployments. <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> <code>10</code> Number of connections in the read pool <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> <code>5</code> Number of connections in the write pool <code>LAKEKEEPER__PG_HOST_R</code> <code>localhost</code> Hostname for read operations. Defaults to <code>LAKEKEEPER__PG_HOST_W</code>. <code>LAKEKEEPER__PG_HOST_W</code> <code>localhost</code> Hostname for write operations <code>LAKEKEEPER__PG_PORT</code> <code>5432</code> Port number <code>LAKEKEEPER__PG_USER</code> <code>postgres</code> Username for authentication <code>LAKEKEEPER__PG_PASSWORD</code> <code>password</code> Password for authentication <code>LAKEKEEPER__PG_DATABASE</code> <code>iceberg</code> Database name <code>LAKEKEEPER__PG_SSL_MODE</code> <code>require</code> SSL mode (disable, allow, prefer, require) <code>LAKEKEEPER__PG_SSL_ROOT_CERT</code> <code>/path/to/root/cert</code> Path to SSL root certificate <code>LAKEKEEPER__PG_ENABLE_STATEMENT_LOGGING</code> <code>true</code> Enable SQL statement logging <code>LAKEKEEPER__PG_TEST_BEFORE_ACQUIRE</code> <code>true</code> Test connections before acquiring from the pool <code>LAKEKEEPER__PG_CONNECTION_MAX_LIFETIME</code> <code>1800</code> Maximum lifetime of connections in seconds <code>LAKEKEEPER__PG_ACQUIRE_TIMEOUT</code> <code>10</code> Timeout to acquire a new postgres connection in seconds. Default: <code>5</code>"}, {"location": "docs/latest/docs/configuration/#vault-kv-version-2", "title": "Vault KV Version 2", "text": "<p>Configuration parameters if a Vault KV version 2 (i.e. Hashicorp Vault) compatible storage is used as a backend. Currently, we only support the <code>userpass</code> authentication method. Configuration may be passed as single values like <code>LAKEKEEPER__KV2__URL=http://vault.local</code> or as a compound value: <code>LAKEKEEPER__KV2='{url=\"http://localhost:1234\", user=\"test\", password=\"test\", secret_mount=\"secret\"}'</code></p> Variable Example Description <code>LAKEKEEPER__KV2__URL</code> <code>https://vault.local</code> URL of the KV2 backend <code>LAKEKEEPER__KV2__USER</code> <code>admin</code> Username to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__PASSWORD</code> <code>password</code> Password to authenticate against the KV2 backend <code>LAKEKEEPER__KV2__SECRET_MOUNT</code> <code>kv/data/iceberg</code> Path to the secret mount in the KV2 backend"}, {"location": "docs/latest/docs/configuration/#task-queues", "title": "Task Queues", "text": "<p>Lakekeeper uses task queues internally to remove soft-deleted tabulars and purge tabular files. The following global configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__TASK_POLL_INTERVAL</code> 3600ms/30s Interval between polling for new tasks. Default: 10s. Supported units: ms (milliseconds) and s (seconds), leaving the unit out is deprecated, it'll default to seconds but is due to be removed in a future release. <code>LAKEKEEPER__TASK_TABULAR_EXPIRATION_WORKERS</code> 2 Number of workers spawned to expire soft-deleted tables and views. <code>LAKEKEEPER__TASK_TABULAR_PURGE_WORKERS</code> 2 Number of workers spawned to purge table files after dropping a table with the purge option. <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> 2 Number of workers spawned that work on expire Snapshots tasks. See Expire Snapshots Docs for more information."}, {"location": "docs/latest/docs/configuration/#nats", "title": "NATS", "text": "<p>Lakekeeper can publish change events to NATS. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__NATS_ADDRESS</code> <code>nats://localhost:4222</code> The URL of the NATS server to connect to <code>LAKEKEEPER__NATS_TOPIC</code> <code>iceberg</code> The subject to publish events to <code>LAKEKEEPER__NATS_USER</code> <code>test-user</code> User to authenticate against NATS, needs <code>LAKEKEEPER__NATS_PASSWORD</code> <code>LAKEKEEPER__NATS_PASSWORD</code> <code>test-password</code> Password to authenticate against nats, needs <code>LAKEKEEPER__NATS_USER</code> <code>LAKEKEEPER__NATS_CREDS_FILE</code> <code>/path/to/file.creds</code> Path to a file containing NATS credentials <code>LAKEKEEPER__NATS_TOKEN</code> <code>xyz</code> NATS token to use for authentication"}, {"location": "docs/latest/docs/configuration/#kafka", "title": "Kafka", "text": "<p>Lakekeeper uses rust-rdkafka to enable publishing events to Kafka.</p> <p>The following features of rust-rdkafka are enabled:</p> <ul> <li>tokio</li> <li>ztstd</li> <li>gssapi-vendored</li> <li>curl-static</li> <li>ssl-vendored</li> <li>libz-static</li> </ul> <p>This means that all features of librdkafka are usable. All necessary dependencies are statically linked and cannot be disabled. If you want to use dynamic linking or disable a feature, you'll have to fork Lakekeeper and change the features accordingly. Please refer to the documentation of rust-rdkafka for details on how to enable dynamic linking or disable certain features.</p> <p>To publish events to Kafka, set the following environment variables:</p> Variable Example Description <code>LAKEKEEPER__KAFKA_TOPIC</code> <code>lakekeeper</code> The topic to which events are published <code>LAKEKEEPER__KAFKA_CONFIG</code> <code>{\"bootstrap.servers\"=\"host1:port,host2:port\",\"security.protocol\"=\"SSL\"}</code> librdkafka Configuration as \"Dictionary\". Note that you cannot use \"JSON-Style-Syntax\". Also see notes below <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> <code>/path/to/config_file</code> librdkafka Configuration to be loaded from a file. Also see notes below"}, {"location": "docs/latest/docs/configuration/#notes", "title": "Notes", "text": "<p><code>LAKEKEEPER__KAFKA_CONFIG</code> and <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> are mutually exclusive and the values are not merged, if both variables are set. In case that both are set, <code>LAKEKEEPER__KAFKA_CONFIG</code> is used.</p> <p>A <code>LAKEKEEPER__KAFKA_CONFIG_FILE</code> could look like this:</p> <pre><code>{\n  \"bootstrap.servers\"=\"host1:port,host2:port\",\n  \"security.protocol\"=\"SASL_SSL\",\n  \"sasl.mechanisms\"=\"PLAIN\",\n}\n</code></pre> <p>Checking configuration parameters is deferred to <code>rdkafka</code></p>"}, {"location": "docs/latest/docs/configuration/#logging-cloudevents", "title": "Logging Cloudevents", "text": "<p>Cloudevents can also be logged, if you do not have Nats up and running. This feature can be enabled by setting Cloudevents can also be logged, if you do not have Nats or Kafka up and running. This feature can be enabled by setting</p> <p><code>LAKEKEEPER__LOG_CLOUDEVENTS=true</code></p>"}, {"location": "docs/latest/docs/configuration/#authentication", "title": "Authentication", "text": "<p>To prohibit unwanted access to data, we recommend to enable Authentication.</p> <p>Authentication is enabled if:</p> <ul> <li><code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set OR</li> <li><code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is set to true</li> </ul> <p>In Lakekeeper multiple Authentication mechanisms can be enabled together, for example OpenID + Kubernetes. Lakekeeper builds an internal Authenticator chain of up to three identity providers. Incoming tokens need to be JWT tokens - Opaque tokens are not yet supported. Incoming tokens are introspected, and each Authentication provider checks if the given token can be handled by this provider. If it can be handled, the token is authenticated against this provider, otherwise the next Authenticator in the chain is checked.</p> <p>The following Authenticators are available. Enabled Authenticators are checked in order:</p> <ol> <li>OpenID / OAuth2 Enabled if: <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set Validates Token with: Locally with JWKS Keys fetched from the well-known configuration. Accepts JWT if (both must be true):<ul> <li>Issuer matches the issuer provided in the <code>.well-known/openid-configuration</code> of the <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> OR issuer matches any of the <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code>.</li> <li>If <code>LAKEKEEPER__OPENID_AUDIENCE</code> is specified, any of the configured audiences must be present in the token</li> </ul> </li> <li>Kubernetes Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API    Accepts JWT if:<ul> <li>Token audience matches any of the audiences provided in <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code></li> <li>If <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> is not set, all tokens proceed to validation! We highly recommend to configure audiences, for most deployments <code>https://kubernetes.default.svc</code> works.</li> </ul> </li> <li>Kubernetes Legacy Tokens Enabled if: <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true and <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> is true Validates Token with: Kubernetes <code>TokenReview</code> API Accepts JWT if:<ul> <li>Tokens issuer is <code>kubernetes/serviceaccount</code> or <code>https://kubernetes.default.svc.cluster.local</code></li> </ul> </li> </ol> <p>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is specified, Lakekeeper will  verify access tokens against this provider. The provider must provide the <code>.well-known/openid-configuration</code> endpoint and the openid-configuration needs to have <code>jwks_uri</code> and <code>issuer</code> defined. </p> <p>Typical values for <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> are:</p> <ul> <li>Keycloak: <code>https://keycloak.local/realms/{your-realm}</code></li> <li>Entra-ID: <code>https://login.microsoftonline.com/{your-tenant-id-here}/v2.0/</code></li> </ul> <p>Please check the Authentication Guide for more details.</p> Variable Example Description <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID Provider URL. Lakekeeper expects to find <code>&lt;LAKEKEEPER__OPENID_PROVIDER_URI&gt;/.well-known/openid-configuration</code> and load JWKS tokens from there. Do not include the <code>/.well-known/openid-configuration</code> in the provided URL. <code>LAKEKEEPER__OPENID_AUDIENCE</code> <code>the-client-id-of-my-app</code> If set, the <code>aud</code> of the provided token must match the value provided. Multiple allowed audiences can be provided as a comma separated list. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> <code>https://sts.windows.net/&lt;Tenant&gt;/</code> A comma separated list of additional issuers to trust. The issuer defined in the <code>issuer</code> field of the <code>.well-known/openid-configuration</code> is always trusted. <code>LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS</code> has no effect if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is not set. <code>LAKEKEEPER__OPENID_SCOPE</code> <code>lakekeeper</code> Specify a scope that must be present in provided tokens received from the openid provider. <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> <code>sub</code> or <code>oid</code> Specify the field in the user's claims that is used to identify a User. By default Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim. <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> <code>resource_access.lakekeeper.roles</code> Specify the claim to use in provided JWT tokens to extract roles. The field should contain an array of strings or a single string. Supports nested claims using dot notation, e.g., \"resource_access.account.roles\". Currently only has an effect when using the Cedar Authorizer. <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> true If true, kubernetes service accounts can authenticate to Lakekeeper. This option is compatible with <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> - multiple IdPs (OIDC and Kubernetes) can be enabled simultaneously. <code>LAKEKEEPER__KUBERNETES_AUTHENTICATION_AUDIENCE</code> <code>https://kubernetes.default.svc</code> Audiences that are expected in Kubernetes tokens. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true. <code>LAKEKEEPER_TEST__KUBERNETES_AUTHENTICATION_ACCEPT_LEGACY_SERVICEACCOUNT</code> <code>false</code> Add an authenticator that handles tokens with no audiences and the issuer set to <code>kubernetes/serviceaccount</code>. Only has an effect if <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code> is true."}, {"location": "docs/latest/docs/configuration/#authorization", "title": "Authorization", "text": "<p>Authorization is only effective if Authentication is enabled. Authorization must not be enabled after Lakekeeper has been bootstrapped! Please create a new Lakekeeper instance, bootstrap it with authorization enabled, and migrate your tables.</p> Variable Example Description <code>LAKEKEEPER__AUTHZ_BACKEND</code> <code>allowall</code> The authorization backend to use. If <code>openfga</code> or <code>cedar</code> is chosen, additional parameters are required (see below). The <code>allowall</code> backend disables authorization - authenticated users can access all endpoints. Default: <code>allowall</code>, one-of: [<code>openfga</code>, <code>allowall</code>, <code>cedar</code>]"}, {"location": "docs/latest/docs/configuration/#openfga", "title": "OpenFGA", "text": "Variable Example Description <code>LAKEKEEPER__OPENFGA__ENDPOINT</code> <code>http://localhost:35081</code> OpenFGA Endpoint (gRPC). <code>LAKEKEEPER__OPENFGA__STORE_NAME</code> <code>lakekeeper</code> The OpenFGA Store to use. Default: <code>lakekeeper</code> <code>LAKEKEEPER__OPENFGA__API_KEY</code> <code>my-api-key</code> The API Key used for Pre-shared key authentication to OpenFGA. If <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> is set, the API Key is ignored. If neither API Key nor Client ID is specified, no authentication is used. <code>LAKEKEEPER__OPENFGA__CLIENT_ID</code> <code>12345</code> The Client ID to use for Authenticating if OpenFGA is secured via OIDC. <code>LAKEKEEPER__OPENFGA__CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER__OPENFGA__TOKEN_ENDPOINT</code> <code>https://keycloak.example.com/realms/master/protocol/openid-connect/token</code> Token Endpoint to use when exchanging client credentials for an access token for OpenFGA. Required if Client ID is set <code>LAKEKEEPER__OPENFGA__SCOPE</code> <code>openfga</code> Additional scopes to request in the Client Credential flow. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code> <code>collaboration</code> Explicitly set the Authorization model prefix. Defaults to <code>collaboration</code> if not set. We recommend to use this setting only in combination with <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_PREFIX</code>. <code>LAKEKEEPER__OPENFGA__AUTHORIZATION_MODEL_VERSION</code> <code>3.1</code> Version of the model to use. If specified, the specified model version must already exist. This can be used to roll-back to previously applied model versions or to connect to externally managed models. Migration is disabled if the model version is set. Version should have the format .. <code>LAKEKEEPER__OPENFGA__MAX_BATCH_CHECK_SIZE</code> <code>50</code> p The maximum number of checks than can be handled by a batch check request. This is a configuration option of the <code>OpenFGA</code> server with default value 50."}, {"location": "docs/latest/docs/configuration/#cedar", "title": "Cedar", "text": "Variable Example Description <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__LOCAL_FILES</code> <code>[/path/to/policies1.cedar,/path/to/policies2.cedar]</code> List of local file paths containing Cedar policies in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__LOCAL_FILES</code> <code>[/path/to/entities1.json,/path/to/entities2.json]</code> List of local JSON file paths containing additional Cedar entities (typically roles). <code>LAKEKEEPER__CEDAR__POLICY_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedar</code> is treated as a policy source in Cedar format (not JSON). <code>LAKEKEEPER__CEDAR__ENTITY_JSON_SOURCES__K8S_CM</code> <code>[my-cm-1, my-cm-2]</code> List of Kubernetes ConfigMap names in the same namespace as Lakekeeper. Every key ending with <code>.cedarentities.json</code> is treated as an entity source. <code>LAKEKEEPER__CEDAR__REFRESH_INTERVAL_SECS</code> <code>5</code> Refresh interval in seconds for reloading policies and entities from Kubernetes ConfigMaps and local files. Default: <code>5</code> seconds. See Cedar Authorization for more information. <code>LAKEKEEPER__CEDAR__EXTERNALLY_MANAGED_USER_AND_ROLES</code> <code>false</code> When set to <code>true</code>, Lakekeeper expects all roles and users to be managed externally via entities.json and does not extract <code>Lakekeeper::Role</code> or <code>Lakekeeper::User</code> entities from the user's token. When set to <code>false</code> (default), Lakekeeper automatically provides <code>Lakekeeper::Role</code> and <code>Lakekeeper::User</code> entities to Cedar based on information extracted from the user's token. When set to <code>false</code>, ensure <code>LAKEKEEPER__OPENID_ROLES_CLAIM</code> is configured to specify which claim in the token contains role information. <code>LAKEKEEPER__CEDAR__SCHEMA_FILE</code> <code>/path/to/custom/schema.cedarschema</code> Optional path to a custom Cedar schema file. If provided, this schema will be used instead of the embedded default schema. Useful for extending or customizing the Cedar schema. Compatibility with the Lakekeeper schema must be ensured for all entities provided by Lakekeeper (Server, Project, Namespace, Table, View. User &amp; Role if externally managed roles is <code>false</code>). <p>Debug configurations for Cedar</p> Variable Example Description <code>LAKEKEEPER__CEDAR__DEBUG__LOG_ENTITIES</code> <code>false</code> If <code>true</code>, logs all internal entities (excluding externally managed entities) for each authorization request at debug level. This is useful for debugging authorization issues but can be verbose and impacts performance. Logging only occurs when both this flag is <code>true</code> AND debug logging is enabled (<code>RUST_LOG=debug</code>). Default: <code>false</code>."}, {"location": "docs/latest/docs/configuration/#ui", "title": "UI", "text": "<p>When using the built-in UI which is hosted as part of the Lakekeeper binary, most values are pre-set with the corresponding values of Lakekeeper itself. Customization is typically required if Authentication is enabled. Please check the Authentication guide for more information.</p> Variable Example Description <code>LAKEKEEPER__UI__OPENID_PROVIDER_URI</code> <code>https://keycloak.local/realms/{your-realm}</code> OpenID provider URI used for login in the UI. Defaults to <code>LAKEKEEPER__OPENID_PROVIDER_URI</code>. Set this only if the IdP is reachable under a different URI from the users browser and lakekeeper. <code>LAKEKEEPER__UI__OPENID_CLIENT_ID</code> <code>lakekeeper-ui</code> Client ID to use for the Authorization Code Flow of the UI. Required if Authentication is enabled. Defaults to <code>lakekeeper</code> <code>LAKEKEEPER__UI__OPENID_REDIRECT_PATH</code> <code>/callback</code> Path where the UI receives the callback including the tokens from the users browser. Defaults to: <code>/callback</code> <code>LAKEKEEPER__UI__OPENID_SCOPE</code> <code>openid email</code> Scopes to request from the IdP. Defaults to <code>openid profile email</code>. <code>LAKEKEEPER__UI__OPENID_RESOURCE</code> <code>lakekeeper-api</code> Resources to request from the IdP. If not specified, the <code>resource</code> field is omitted (default). <code>LAKEKEEPER__UI__OPENID_POST_LOGOUT_REDIRECT_PATH</code> <code>/logout</code> Path the UI calls when users are logged out from the IdP. Defaults to <code>/logout</code> <code>LAKEKEEPER__UI__LAKEKEEPER_URL</code> <code>https://example.com/lakekeeper</code> URI where the users browser can reach Lakekeeper. Defaults to the value of <code>LAKEKEEPER__BASE_URI</code>. <code>LAKEKEEPER__UI__OPENID_TOKEN_TYPE</code> <code>access_token</code> The token type to use for authenticating to Lakekeeper. The default value <code>access_token</code> works for most IdPs. Some IdPs, such as the Google Identity Platform, recommend the use of the OIDC ID Token instead. To use the ID token instead of the access token for Authentication, specify a value of <code>id_token</code>. Possible values are <code>access_token</code> and <code>id_token</code>."}, {"location": "docs/latest/docs/configuration/#caching", "title": "Caching", "text": "<p>Lakekeeper uses in-memory caches to speed up certain operations.</p> <p>Short-Term Credentials (STC) Cache</p> <p>When Lakekeeper vends short-term credentials for cloud storage access (S3 STS, Azure SAS tokens, or GCP access tokens), these credentials can be cached to reduce load on cloud identity services and improve response times.</p> Variable Example Description <code>LAKEKEEPER__CACHE__STC__ENABLED</code> <code>true</code> Enable or disable the short-term credentials cache. Default: <code>true</code> <code>LAKEKEEPER__CACHE__STC__CAPACITY</code> <code>10000</code> Maximum number of credential entries to cache. Default: <code>10000</code> <p>Expiry Mechanism: Cached credentials automatically expire based on the validity period of the underlying cloud credentials. Lakekeeper caches credentials for half their lifetime (e.g., if GCP STS returns credentials valid for 1 hour, they're cached for 30 minutes) with a maximum cache duration of 1 hour. This ensures credentials remain fresh while reducing unnecessary identity service calls.</p> <p>Metrics: The STC cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_stc_cache_size{cache_type=\"stc\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_stc_cache_hits_total{cache_type=\"stc\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_stc_cache_misses_total{cache_type=\"stc\"}</code>: Total number of cache misses</li> </ul> <p>Warehouse Cache</p> <p>Caches warehouse metadata to reduce database queries for warehouse lookups.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__WAREHOUSE__ENABLED</code> boolean <code>true</code> Enable/disable warehouse caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__CAPACITY</code> integer <code>1000</code> Maximum number of warehouses to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__WAREHOUSE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to Storage Profile may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. Warehouse metadata is guaranteed to be fresh for load table &amp; view operations also for multi-worker deployments.</p> <p>Metrics: The Warehouse cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_warehouse_cache_size{cache_type=\"warehouse\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_warehouse_cache_hits_total{cache_type=\"warehouse\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_warehouse_cache_misses_total{cache_type=\"warehouse\"}</code>: Total number of cache misses</li> </ul> <p>Namespace Cache</p> <p>Caches namespace metadata and hierarchies to reduce database queries for namespace lookups. Namespace lookups are also required for table &amp; view operations.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__NAMESPACE__ENABLED</code> boolean <code>true</code> Enable/disable namespace caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__NAMESPACE__CAPACITY</code> integer <code>1000</code> Maximum number of namespaces to cache. Default: <code>1000</code> <code>LAKEKEEPER__CACHE__NAMESPACE__TIME_TO_LIVE_SECS</code> integer <code>60</code> Time-to-live for cache entries in seconds. Default: <code>60</code> <p>If the cache is enabled, changes to namespace properties may take up to the configured TTL (default: 60 seconds) to be reflected in all Lakekeeper workers. If a single worker is used, the Cache is always up to date. The namespace cache stores both individual namespaces and their parent hierarchies for efficient lookups.</p> <p>Metrics: The Namespace cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_namespace_cache_size{cache_type=\"namespace\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_namespace_cache_hits_total{cache_type=\"namespace\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_namespace_cache_misses_total{cache_type=\"namespace\"}</code>: Total number of cache misses</li> </ul> <p>Secrets Cache</p> <p>Caches storage secrets to reduce load on the secret store. Since Lakekeeper never updates secrets, long TTLs can significantly increase resilience against secret store outages, especially when the secret store is external to the main database backend.</p> Configuration Key Type Default Description <code>LAKEKEEPER__CACHE__SECRETS__ENABLED</code> boolean <code>true</code> Enable/disable secrets caching. Default: <code>true</code> <code>LAKEKEEPER__CACHE__SECRETS__CAPACITY</code> integer <code>500</code> Maximum number of secrets to cache. Default: <code>500</code> <code>LAKEKEEPER__CACHE__SECRETS__TIME_TO_LIVE_SECS</code> integer <code>600</code> Time-to-live for cache entries in seconds. Default: <code>600</code> (10 minutes) <p>Metrics: The Secrets cache exposes Prometheus metrics for monitoring:</p> <ul> <li><code>lakekeeper_secrets_cache_size{cache_type=\"secrets\"}</code>: Current number of entries in the cache</li> <li><code>lakekeeper_secrets_cache_hits_total{cache_type=\"secrets\"}</code>: Total number of cache hits</li> <li><code>lakekeeper_secrets_cache_misses_total{cache_type=\"secrets\"}</code>: Total number of cache misses</li> </ul>"}, {"location": "docs/latest/docs/configuration/#endpoint-statistics", "title": "Endpoint Statistics", "text": "<p>Lakekeeper collects statistics about the usage of its endpoints. Every Lakekeeper instance accumulates endpoint calls for a certain duration in memory before writing them into the database. The following configuration options are available:</p> Variable Example Description <code>LAKEKEEPER__ENDPOINT_STAT_FLUSH_INTERVAL</code> 30s Interval in seconds to write endpoint statistics into the database. Default: 30s, valid units are (s|ms)"}, {"location": "docs/latest/docs/configuration/#ssl-dependencies", "title": "SSL Dependencies", "text": "<p>You may be running Lakekeeper in your own environment which uses self-signed certificates for e.g. Minio. Lakekeeper is built with reqwest's <code>rustls-tls-native-roots</code> feature activated, this means <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables are respected. If both are not set, the system's default CA store is used. If you want to use a custom CA store, set <code>SSL_CERT_FILE</code> to the path of the CA file or <code>SSL_CERT_DIR</code> to the path of the CA directory. The certificate used by the server cannot be a CA. It needs to be an end entity certificate, else you may run into <code>CaUsedAsEndEntity</code> errors.</p>"}, {"location": "docs/latest/docs/configuration/#debug", "title": "Debug", "text": "<p>Lakekeeper provides debugging options to help troubleshoot issues during development. These options should not be enabled in production environments as they can expose sensitive data and impact performance.</p> Variable Example Description <code>LAKEKEEPER__DEBUG__LOG_REQUEST_BODIES</code> <code>true</code> If set to <code>true</code>, Lakekeeper will log all incoming and outgoing request bodies at debug level. This is useful for debugging API interactions but should never be enabled in production as it can expose sensitive data (credentials, tokens, etc.) and significantly impact performance. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__MIGRATE_BEFORE_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper waits for the DB (30s) and runs migrations when <code>serve</code> is called. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__AUTO_SERVE</code> <code>true</code> If set to <code>true</code>, Lakekeeper will automatically start the server when no subcommand is provided (i.e., when running the binary without arguments). This is useful for development environments to quickly start the server without explicitly specifying the <code>serve</code> command. Default: <code>false</code> <code>LAKEKEEPER__DEBUG__EXTENDED_LOGS</code> <code>false</code> Controls whether file names and line numbers are included in JSON log output. When set to <code>false</code>, these fields are omitted for cleaner logs. When set to <code>true</code>, each log entry includes <code>filename</code> and <code>line_number</code> fields for easier debugging. Default: <code>false</code> <p>Warning: Debug options can expose sensitive information in logs and should only be used in secure development environments.</p>"}, {"location": "docs/latest/docs/configuration/#test-configurations", "title": "Test Configurations", "text": "Variable Example Description <code>LAKEKEEPER__SKIP_STORAGE_VALIDATION</code> true If set to true, Lakekeeper does not validate the provided storage configuration &amp; credentials when creating or updating Warehouses. This is not suitable for production. Default: false"}, {"location": "docs/latest/docs/customize/", "title": "Customize", "text": "<p>As Customizability is one of the core features we are missing in other IRC implementations, we try to do things differently. The core implementation of this crate is based on four modules that back the <code>axum</code> service router:</p> <ul> <li><code>Catalog</code> is the interface to the DB backend where Warehouses, Namespaces, Tables and other entities are managed.</li> <li><code>SecretStore</code> is the interface to a secure storage for secrets.</li> <li><code>Authorizer</code> is the interface to the permission system used by Lakekeeper. It may expose its own APIs.</li> <li><code>EventPublisher</code> is the interface to message queues to send change events to.</li> <li><code>ContractValidator</code> allows an external system to prohibit changes to tables if, for example, data contracts are violated</li> <li><code>TaskQueue</code> is the interface to the task store, used to schedule tasks like soft-deletes</li> </ul> <p>All components come pre-implemented, however we encourage you to write custom implementations, for example to seamlessly grant access to tables via your companies Data Governance solution, or publish events to your very important messaging service.</p>"}, {"location": "docs/latest/docs/developer-guide/", "title": "Developer Guide", "text": "<p>All commits to main go through a PR. CI checks have to pass before merging the PR. Keep in mind that CI checks include lints. Before merge, commits are squashed, but GitHub is taking care of this, so don't worry. PR titles should follow Conventional Commits. We encourage small and orthogonal PRs. If you want to work on a bigger feature, please open an issue and discuss it with us first. </p> <p>If you want to work on something but don't know what, take a look at our issues tagged with <code>help wanted</code>. If you're still unsure, please reach out to us via the Lakekeeper Discord. If you have questions while working on something, please use the GitHub issue or our Discord. We are happy to guide you!</p>"}, {"location": "docs/latest/docs/developer-guide/#foundation-cla", "title": "Foundation &amp; CLA", "text": "<p>We hate red tape. Currently, all committers need to sign the CLA in GitHub. To ensure the future of Lakekeeper, we want to donate the project to a foundation. We are not sure yet if this is going to be Apache, Linux, a Lakekeeper foundation or something else. Currently, we prefer to spend our time on adding cool new features to Lakekeeper, but we will revisit this topic during 2026.</p>"}, {"location": "docs/latest/docs/developer-guide/#initial-setup", "title": "Initial Setup", "text": "<p>To work on small and self-contained features, it is usually enough to have a Postgres database running while setting a few envs. The code block below should get you started up to running most unit tests as well as clippy.</p> <p><pre><code># start postgres\ndocker run -d --name postgres-16 -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:17\n# set envs\necho 'export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres' &gt; .env\necho 'export ICEBERG_REST__PG_ENCRYPTION_KEY=\"abc\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_READ=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\necho 'export ICEBERG_REST__PG_DATABASE_URL_WRITE=\"postgresql://postgres:postgres@localhost/postgres\"' &gt;&gt; .env\nsource .env\n\n# Migrate db (make sure you have sqlx installed `cargo install sqlx-cli`)\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# Run tests (make sure you have cargo nextest installed, `cargo install cargo-nextest`)\ncargo nextest run --all-features\n\n# run clippy\njust check-clippy\n# formatting the code (make sure you have cargo-sort installed, `cargo install cargo-sort`)\n# You may have to install nightly rust toolchain\njust fix-format\n</code></pre> Keep in mind that some tests are excluded by the <code>default-filter</code> in <code>.config/nextest.toml</code>. You can find a list of them in the Testing section below or by searching for modules whose name contains <code>_integration_tests</code> within files ending with <code>.rs</code>. There are a few cargo commands we run on CI. You may install just to run them conveniently. If you made any changes to SQL queries, please follow Working with SQLx before submitting your PR.</p>"}, {"location": "docs/latest/docs/developer-guide/#code-structure", "title": "Code structure", "text": ""}, {"location": "docs/latest/docs/developer-guide/#what-is-where", "title": "What is where?", "text": "<p>We have three crates, <code>lakekeeper</code>, <code>lakekeeper-bin</code> and <code>iceberg-ext</code>. The bulk of the code is in <code>lakekeeper</code>. The <code>lakekeeper-bin</code> crate contains the main entry point for the catalog. The <code>iceberg-ext</code> crate contains extensions to <code>iceberg-rust</code>. </p> <p>lakekeeper</p> <p>The <code>lakekeeper</code> crate contains the core of the catalog. It is structured into several modules:</p> <ol> <li><code>api</code> - contains the implementation of the REST API handlers as well as the <code>axum</code> router instantiation.</li> <li><code>catalog</code> - contains the core business logic of the REST catalog</li> <li><code>service</code> - contains various function blocks that make up the whole service, e.g., authn, authz and implementations of specific cloud storage backends.</li> <li><code>tests</code> - contains integration tests and some common test helpers, see below for more information.</li> <li><code>implementations</code> - contains the concrete implementation of the catalog backend, currently there's only a Postgres implementation and an alternative for Postgres as secret-store, <code>kv2</code>.</li> </ol> <p>lakekeeper-bin</p> <p>The main function branches out into multiple commands, amongst others, there's a health-check, migrations, but also serve which is likely the most relevant to you. In case you are forking us to implement your own AuthZ backend, you'll want to change the <code>serve</code> command to use your own implementation, just follow the call-chain.</p>"}, {"location": "docs/latest/docs/developer-guide/#where-to-put-tests", "title": "Where to put tests?", "text": "<p>We try to keep unit-tests close to the code they are testing. E.g., all tests for the database module of tables are located in <code>crates/lakekeeper/src/implementations/postgres/tabular/table/mod.rs</code>. While working on more complex features we noticed a lot of repetition within tests and started to put commonly used functions into <code>crates/lakekeeper/src/tests/mod.rs</code>. Within the <code>tests</code> module, there are also some higher-level tests that cannot be easily mapped to a single module or require a non-trivial setup. Depending on what you are working on, you may want to put your tests there.</p>"}, {"location": "docs/latest/docs/developer-guide/#i-need-to-add-an-endpoint", "title": "I need to add an endpoint", "text": "<p>You'll start at <code>api</code> and add the endpoint function to either <code>management</code> or <code>iceberg</code> depending on whether the endpoint belongs to official iceberg REST specification. The likely next step is to extend the respective <code>Service</code> trait so that there's a function to be called from the REST handler. Within the trait function, depending on your feature, you may need to store or fetch something from the storage backend. Depending on if the functionality already exists, you can do so via the respective function on the <code>C</code> generic and either the <code>state: ApiContext&lt;State&lt;...&gt;&gt;</code> struct or by first getting a transaction via <code>C::Transaction::begin_&lt;write|read&gt;(state.v1_state.catalog.clone()).await?;</code>. If you need to add a new function to the storage backend, extend the <code>Catalog</code> trait and implement it in the respective modules within <code>implementations</code>. Remember to do appropriate AuthZ checks within the function of the respective <code>Service</code> trait.</p>"}, {"location": "docs/latest/docs/developer-guide/#debugging-complex-issues-and-prototyping-using-our-examples", "title": "Debugging complex issues and prototyping using our examples", "text": "<p>To debug more complex issues, work on prototypes or simply an initial manual test, you can use one of the <code>examples</code>. Unless you are working on AuthN or AuthZ, you'll most likely want to use the minimal example. All examples come with a <code>docker-compose-build.yaml</code> which will build the catalog image from source. The invocation looks like this: <code>docker compose -f docker-compose.yaml -f docker-compose-build.yaml up -d --build</code>. Aside from building the catalog, the <code>docker-compose-build.yaml</code> overlay also exposes the docker services to your host, so you can also use it as a development environment by e.g. pointing your env vars to the docker container to test against its minio instance. If you made changes to SQL queries, you'll have to run <code>just sqlx-prepare</code> before rebuilding the catalog image. This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database.</p> <p>After spinning the example up, you may head to <code>localhost:8888</code> and use one of the notebooks.</p>"}, {"location": "docs/latest/docs/developer-guide/#working-with-sqlx", "title": "Working with SQLx", "text": "<p>This crate uses sqlx. For development and compilation a Postgres Database is required. This is part of the Initial setup. If your database credentials used differ, please modify the <code>.env</code> accordingly and run <code>source .env</code> again.</p> <p>Run: <pre><code># Migrate db. Make sure you have sqlx-cli install with `cargo install sqlx-cli`\n# Run this locally if you change the db schema via `crates/lakekeeper/migrations`,\n# e.g. after adding a table or dropping a column.\ncd crates/lakekeeper\nsqlx database create &amp;&amp; sqlx migrate run\ncd ../..\n\n# If you changed any of the SQL statements embedded in Rust code, run this before pushing to GitHub.\njust sqlx-prepare\n</code></pre> This will update the sqlx queries in <code>.sqlx</code> to enable static checking of the queries without a migrated database. Remember to <code>git add .sqlx</code> before committing. If you forget, your PR will fail to build on GitHub. Be careful, if the command failed, <code>.sqlx</code> will be empty. But do not worry, it wouldn't build on GitHub so there's no way of really breaking things.</p>"}, {"location": "docs/latest/docs/developer-guide/#schema-qualification-warning", "title": "\u26a0\ufe0f Schema Qualification Warning", "text": "<p>IMPORTANT: When adding new migrations, do NOT schema qualify references to any database objects. Schema qualification will break deployments that place the application in a schema different than the public one.</p> <p>\u274c Incorrect - Do NOT do this: <pre><code>-- This will break deployments in non-public schemas\nCREATE TABLE public.my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO public.my_new_table (name) VALUES ('example');\n\nALTER TABLE public.existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>\u2705 Correct - Do this instead: <pre><code>-- This will work in any schema\nCREATE TABLE my_new_table (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255)\n);\n\nINSERT INTO my_new_table (name) VALUES ('example');\n\nALTER TABLE existing_table ADD COLUMN new_column INTEGER;\n</code></pre></p> <p>The migration system will automatically apply the migration in the correct schema context, so explicit schema qualification is unnecessary and will cause issues in deployments where Lakekeeper is deployed to a custom schema.</p>"}, {"location": "docs/latest/docs/developer-guide/#inspecting-the-db", "title": "Inspecting the db", "text": "<p>The db schema is the result of all migrations applied in order. To inspect it you can:</p> <pre><code># Assumes you set up the db as described above\n\n# Get a shell in the db's container\ndocker exec -it postgres-16 /bin/bash\n\n# Then you can connect to the db\npsql \"postgresql://postgres:postgres@localhost:5432/postgres\"\n# And inspect it, for instance by describing views or tables\n\\d+ active_tabulars\n\n# Or you can dump the entire schema\npg_dump --schema-only \"postgresql://postgres:postgres@localhost:5432/postgres\" &gt; /home/lakekeeper_schema.sql\n# Copy it out of the container and then inspect it or pass it as context to LLMs\ndocker cp postgres-16:/home/lakekeeper_schema.sql .\n</code></pre>"}, {"location": "docs/latest/docs/developer-guide/#kv2-vault", "title": "KV2 / Vault", "text": "<p>This catalog supports KV2 as a backend for secrets. Tests for KV2 are disabled by default. To enable them, you need to run the following commands:</p> <pre><code>docker run -d -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_ROOT_TOKEN_ID=myroot' -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' hashicorp/vault\n\n# append some more env vars to the .env file, it should already have PG related entries defined above.\n\n# the values below configure KV2\necho 'export ICEBERG_REST__KV2__URL=\"http://localhost:8200\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__USER=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__PASSWORD=\"test\"' &gt;&gt; .env\necho 'export ICEBERG_REST__KV2__SECRET_MOUNT=\"secret\"' &gt;&gt; .env\n\nsource .env\n# setup vault\n./tests/vault-setup.sh http://localhost:8200\n\n# Select kv2 tests\ncargo nextest run --all-features --all-targets \\\n    --ignore-default-filter -E \"test(::kv2_integration_tests::)\"\n</code></pre>"}, {"location": "docs/latest/docs/developer-guide/#test-cloud-storage-profiles", "title": "Test cloud storage profiles", "text": "<p>Currently, we're not aware of a good way of testing cloud storage integration against local deployments. That means, to test against AWS S3, GCS and ADLS Gen2, you need to set the following environment variables. For more information, take a look at the Storage Guide. A sample <code>.env</code> could look like this:</p> <pre><code>export LAKEKEEPER_TEST__AZURE_TENANT_ID=&lt;your tenant id&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_FILESYSTEM=&lt;your azure adls filesystem name&gt;\nexport LAKEKEEPER_TEST__AZURE_STORAGE_ACCOUNT_NAME=&lt;your azure storage account name&gt;\n# Auth Method 1: Client Credentials\nexport LAKEKEEPER_TEST__AZURE_CLIENT_ID=&lt;your entra id app registration client id&gt;\nexport LAKEKEEPER_TEST__AZURE_CLIENT_SECRET=&lt;your entra id app registration client secret&gt;\n# Auth Method 2: Shared Key\nexport LAKEKEEPER_TEST__AZURE_STORAGE_SHARED_KEY=&lt;shared key&gt;\n\nexport AWS_S3_BUCKET=&lt;your aws s3 bucket&gt;\nexport AWS_S3_REGION=&lt;your aws s3 region&gt;\nexport AWS_S3_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_S3_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_S3_STS_ROLE_ARN=arn:aws:iam::123456789012:role/role-name\n\n# the values below should work with the default minio in our docker-compose\nexport LAKEKEEPER_TEST__S3_BUCKET=tests\nexport LAKEKEEPER_TEST__S3_REGION=local\nexport LAKEKEEPER_TEST__S3_ACCESS_KEY=minio-root-user\nexport LAKEKEEPER_TEST__S3_SECRET_KEY=minio-root-password\nexport LAKEKEEPER_TEST__S3_ENDPOINT=http://localhost:9000\n\nexport LAKEKEEPER_TEST__GCS_CREDENTIAL='{\"type\": \"service_account\",\"project_id\": \"..\", ...}'\nexport LAKEKEEPER_TEST__GCS_BUCKET=name-of-gcs-bucket-without-hns\nexport LAKEKEEPER_TEST__GCS_HNS_BUCKET=name-of-gcs-bucket-with-hns\n</code></pre> <p>You may then run tests by ignoring the nextest's default filter and selecting the desired tests:</p> <pre><code>source .example.env-from-above\ncargo nextest run --all-features --ignore-default-filter -E \"test(::aws_integration_tests::)\"\n# see .config/nextest.toml for all filters\n</code></pre>"}, {"location": "docs/latest/docs/developer-guide/#running-integration-test", "title": "Running integration test", "text": "<p>Our integration tests are written in Python and use pytest. They are located in the <code>tests</code> folder. The integration tests spin up Lakekeeper and all the dependencies via <code>docker compose</code>. Please check the Integration Test Docs for more information.</p>"}, {"location": "docs/latest/docs/developer-guide/#running-authorization-unit-tests", "title": "Running Authorization unit tests", "text": "<p>Some authorization unit tests need to be run against an OpenFGA server. They are excluded by our nextest <code>default-filter</code>. The workflow for executing them is:</p> <pre><code># Start an OpenFGA server in a docker container\ndocker rm --force openfga-client &amp;&amp; docker run -d --name openfga-client -p 36080:8080 -p 36081:8081 -p 36300:3000 openfga/openfga:v1.8 run\n\n# Set Lakekeeper's OpenFGA endpoint\nexport LAKEKEEPER_TEST__OPENFGA__ENDPOINT=\"http://localhost:36081\"\n\n# Use a filterset to select the tests\ncargo nextest run --all-features --ignore-default-filter -E \"test(::openfga_integration_tests::)\"\n</code></pre>"}, {"location": "docs/latest/docs/developer-guide/#extending-authz", "title": "Extending Authz", "text": "<p>When adding a new endpoint, you may need to extend the authorization model. Please check the Authorization Docs for more information. For openfga, you'll have to perform the following steps:</p> <ol> <li>extend the respective enum in <code>crate::service::authz</code> by adding the new action, e.g. <code>crate::service::authz::CatalogViewAction::CanUndrop</code></li> <li>add the relation to <code>crate::service::authz::implementations::openfga::relations</code>, e.g. add <code>ViewRelation::CanUndrop</code></li> <li>add the mapping from the <code>implementations</code> type to the <code>service</code> type in <code>openfga::relations</code>, e.g. <code>CatalogViewAction::CanUndrop =&gt; ViewRelation::CanUndrop</code></li> <li>create a new authz schema version by renaming the version for backward compatible changes, e.g. <code>authz/openfga/v2.1/</code> to <code>authz/openfga/v2.2/</code>. For non-backward compatible changes create a new major version folder.</li> <li>apply your changes, e.g. add <code>define can_undrop: modify</code> to the <code>view</code> type in <code>authz/openfga/v2.2/schema.fga</code></li> <li>regenerate <code>schema.json</code> via <code>./fga model transform --file authz/openfga/v2.2/schema.fga &gt; authz/openfga/v2.2/schema.json</code> (download the <code>fga</code> binary from the OpenFGA repo)</li> <li>Head to <code>crate::service::authz::implementations::openfga::migration.rs</code>, modify <code>ACTIVE_MODEL_VERSION</code> to the newer version. For backwards compatible changes, change the <code>add_model</code> section. For changes that require migrations, add an additional <code>add_model</code> section that includes the migration fn.</li> </ol> <pre><code>pub(super) static ACTIVE_MODEL_VERSION: LazyLock&lt;AuthorizationModelVersion&gt; =\n    LazyLock::new(|| AuthorizationModelVersion::new(3, 0)); // &lt;- Change this for every change in the model\n\n\nfn get_model_manager(\n    client: &amp;BasicOpenFgaServiceClient,\n    store_name: Option&lt;String&gt;,\n) -&gt; openfga_client::migration::TupleModelManager&lt;BasicAuthLayer&gt; {\n    openfga_client::migration::TupleModelManager::new(\n        client.clone(),\n        &amp;store_name.unwrap_or(AUTH_CONFIG.store_name.clone()),\n        &amp;AUTH_CONFIG.authorization_model_prefix,\n    )\n    .add_model(\n        serde_json::from_str(include_str!(\n            // Change this for backward compatible changes.\n            // For non-backward compatible changes that require tuple migrations, add another `add_model` call.\n            \"../../../../../../../authz/openfga/v3.0/schema.json\"\n        ))\n        // Change also the model version in this string:\n        .expect(\"Model v3.0 is a valid AuthorizationModel in JSON format.\"),\n        AuthorizationModelVersion::new(3, 0),\n        // For major version upgrades, this is where tuple migrations go.\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n        None::&lt;MigrationFn&lt;_&gt;&gt;,\n    )\n}\n</code></pre>"}, {"location": "docs/latest/docs/developer-guide/#building-the-docs-locally", "title": "Building the docs locally", "text": "<pre><code>cd site\njust serve\n</code></pre>"}, {"location": "docs/latest/docs/engines/", "title": "Query Engines", "text": "<p>In this page we document how query engines can be configured to connect to Lakekeeper. Please also check the documentation of your query engine to obtain additional information. All Query engines that support the Apache Iceberg REST Catalog (IRC) also support Lakekeeper.</p> <p>If Lakekeeper Authorization is enabled, Lakekeeper enforces permissions based on the <code>sub</code> field in the received tokens. For query engines used by a single user, the user should use its own credentials to log-in to Lakekeeper.</p> <p>For query engines shared by multiple users, Lakekeeper supports two architectures that allow a shared query engine to enforce permissions for individual users:</p> <ol> <li>OAuth2 enabled query engines should use standard OAuth2 Token-Exchange to exchange the user's token of the query engine for a Lakekeeper token (RFC8693). The Catalog then receives a token that has the <code>sub</code> field set to the user using the query engine, instead of the technical user that is used to configure the catalog in the query engine itself.</li> <li>Query engines flexible enough to connect to external permission management systems such as Open Policy Agent (OPA), can directly enforce the same permissions on Data that Lakekeeper uses. Please find more information and a complete docker compose example with trino in the Open Policy Agent Guide.</li> </ol> <p>Shared query engines must use the same Identity Provider as Lakekeeper in both scenarios unless user-ids are mapped, for example in OPA.</p> <p>We are tracking open issues and missing features in query engines in a Tracking Issue on GitHub.</p>"}, {"location": "docs/latest/docs/engines/#generic-iceberg-rest-clients", "title": "Generic Iceberg REST Clients", "text": "<p>All Apache Iceberg REST clients are compatible with Lakekeeper, as Lakekeeper fully implements the standard Iceberg REST Catalog API specification. This page only contains some exemplary tools and configurations to help you get started. For tools not listed here, please refer to their documentation for specific configuration details and best practices when connecting to an Iceberg REST Catalog. Always check with your tool provider for the most up-to-date information regarding supported features and configuration options.</p> <p>When using Lakekeeper with authentication enabled, remember that you can follow the approaches described at the beginning of this page: either use credentials specific to individual users or leverage OAuth2 token exchange for shared query engines. The authentication parameters typically include credential pairs, OAuth2 server URIs, and scopes as shown in the examples above.</p>"}, {"location": "docs/latest/docs/engines/#duckdb-wasm", "title": "DuckDB WASM", "text": "<p>DuckDB WASM allows you to query Lakekeeper directly from your browser. If you are using the Lakekeeper UI, DuckDB WASM is pre-configured. To use DuckDB WASM from the Lakekeeper UI, there are two important requirements due to browser security restrictions:</p> <p>Requirements:</p> <ol> <li>Same-Origin Access: The S3 endpoint must be accessible from your browser at the same URL/origin that Lakekeeper uses to access it. For example, if Lakekeeper accesses S3 at <code>http://my-s3-endpoint:9000</code>, your browser must also be able to reach it at <code>http://my-s3-endpoint:9000</code>. This means the Docker Compose examples won't work with DuckDB WASM out of the box, as the S3 endpoint is typically only accessible within the Docker network, while your browser is not in this network.</li> <li>CORS Policy: Your S3 storage must be configured with a CORS policy that allows requests from the Lakekeeper origin. See the CORS Configuration guide for setup instructions.</li> </ol>"}, {"location": "docs/latest/docs/engines/#duckdb", "title": "DuckDB", "text": "<p>Basic setup in DuckDB:</p> <pre><code>import duckdb\n\nCATALOG_URL = \"http://localhost:8181/catalog\"\nWAREHOUSE = \"my_warehouse\"\n\n# Required if OAuth2 authentication is enabled for Lakekeeper\nCLIENT_ID = \"your-client-id\"\nCLIENT_SECRET = \"your-client-secret\"\nKEYCLOAK_TOKEN_ENDPOINT = \"http://your-idp/realms/iceberg/protocol/openid-connect/token\"\n\n# Install and load Iceberg extension\nduckdb.sql(\"INSTALL ICEBERG;\")\nduckdb.sql(\"LOAD ICEBERG;\")\n\n# Create secret for authentication\nduckdb.sql(f\"\"\"\n    CREATE SECRET lakekeeper_secret (\n        TYPE ICEBERG,\n        CLIENT_ID '{CLIENT_ID}',\n        CLIENT_SECRET '{CLIENT_SECRET}',\n        OAUTH2_SCOPE 'lakekeeper',\n        OAUTH2_SERVER_URI '{KEYCLOAK_TOKEN_ENDPOINT}'\n    )\n\"\"\")\n\n# Attach catalog\nduckdb.sql(f\"\"\"\n    ATTACH '{WAREHOUSE}' AS my_datalake (\n        TYPE ICEBERG,\n        ENDPOINT '{CATALOG_URL}',\n        SECRET lakekeeper_secret\n    )\n\"\"\")\n\n# Query tables\nduckdb.sql(\"SELECT * FROM my_datalake.my_namespace.my_table\").show()\n</code></pre>"}, {"location": "docs/latest/docs/engines/#trino", "title": "Trino", "text": "<p>The following docker compose examples are available for trino:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for trino</li> <li><code>Access-Control-Advanced</code>: Single trino instance secured by OAuth2 shared by multiple users. Lakekeeper Permissions for each individual user enforced by trino via the Open Policy Agent bridge.</li> </ul> <p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in trino:</p> S3-CompatibleAzureGCS <p>Trino supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Trino. If you are interested in vended-credentials for Azure, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Trino does not support vended-credentials for GCS, so that GCS credentials must be specified in Trino. If you are interested in vended-credentials for GCS, please up-vote the Trino Issue.</p> <p>Please find additional configuration Options in the Trino docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/latest/docs/engines/#starburst", "title": "Starburst", "text": "<p>If Soft-Deletion is enabled in Lakekeeper, make sure to set <code>\"iceberg.unique-table-location\" = 'true'</code>, to ensure that tables can be recreated in new locations while their dropped counterparts are waiting for expiration.</p> <p>As Lakekeeper supports nesting of namespaces, we recommend to set <code>\"iceberg.rest-catalog.nested-namespace-enabled\" = 'true'</code>.</p> <p>Basic setup in Starburst:</p> S3-CompatibleAzureGCS <p>Starburst supports vended-credentials from Iceberg REST Catalogs for S3, so that no S3 credentials are required when creating the Catalog.</p> <p>Please find additional configuration Options in the Starburst docs.    </p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"s3.region\" = '&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;',\n    \"fs.native-s3.enabled\" = 'true'\n    -- Required for some S3-compatible storages:\n    \"s3.path-style-access\" = 'true',\n    \"s3.endpoint\" = '&lt;Custom S3 endpoint&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;',\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for Azure, so that Storage Account credentials must be specified in Starburst.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-azure.enabled\" = 'true',\n    \"azure.auth-type\" = 'OAUTH',\n    \"azure.oauth.client-id\" = '&lt;Client-ID for an Application with Storage Account access&gt;',\n    \"azure.oauth.secret\" = '&lt;Client-Secret&gt;',\n    \"azure.oauth.tenant-id\" = '&lt;Tenant-ID&gt;',\n    \"azure.oauth.endpoint\" = 'https://login.microsoftonline.com/&lt;Tenant-ID&gt;/v2.0',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre> <p>Starburst does not support vended-credentials for GCS, so that GCS credentials must be specified in the connector.</p> <p>Please find additional configuration Options in the Starburst docs.</p> <pre><code>CREATE CATALOG lakekeeper USING iceberg\nWITH (\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = '&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;',\n    \"iceberg.rest-catalog.warehouse\" = '&lt;Name of the Warehouse in Lakekeeper&gt;',\n    \"iceberg.rest-catalog.nested-namespace-enabled\" = 'true',\n    \"iceberg.unique-table-location\" = 'true',\n    \"fs.native-gcs.enabled\" = 'true',\n    \"gcs.project-id\" = '&lt;Identifier for the project on Google Cloud Storage&gt;',\n    \"gcs.json-key\" = '&lt;Your Google Cloud service account key in JSON format&gt;',\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.security\" = 'OAUTH2',\n    \"iceberg.rest-catalog.oauth2.credential\" = '&lt;Client-ID&gt;:&lt;Client-Secret&gt;', -- Client-ID used to access Lakekeeper. Typically different to `azure.oauth.client-id`.\n    \"iceberg.rest-catalog.oauth2.server-uri\" = '&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;',\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.rest-catalog.oauth2.scope\" = '&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;'\n)\n</code></pre>"}, {"location": "docs/latest/docs/engines/#spark", "title": "Spark", "text": "<p>The following docker compose examples are available for spark:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control-Simple</code>: Lakekeeper secured with OAuth2, single technical User for spark</li> </ul> <p>Basic setup in spark:</p> S3-Compatible / Azure / GCS <p>Spark supports credential vending for all storage types, so that no credentials need to be specified in spark when creating the catalog.</p> <pre><code>import pyspark\nimport pyspark.sql\n\npyspark_version = pyspark.__version__\npyspark_version = \".\".join(pyspark_version.split(\".\")[:2]) # Strip patch version\niceberg_version = \"1.8.1\"\n\n# Disable the jars which are not needed\nspark_jars_packages = (\n    f\"org.apache.iceberg:iceberg-spark-runtime-{pyspark_version}_2.12:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-aws-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-azure-bundle:{iceberg_version},\"\n    f\"org.apache.iceberg:iceberg-gcp-bundle:{iceberg_version}\"\n)\n\ncatalog_name = \"lakekeeper\"\nconfiguration = {\n    \"spark.jars.packages\": spark_jars_packages,\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.defaultCatalog\": catalog_name,\n    f\"spark.sql.catalog.{catalog_name}\": \"org.apache.iceberg.spark.SparkCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.catalog-impl\": \"org.apache.iceberg.rest.RESTCatalog\",\n    f\"spark.sql.catalog.{catalog_name}.uri\": \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    # Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", # Client-ID used to access Lakekeeper\n    f\"spark.sql.catalog.{catalog_name}.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    f\"spark.sql.catalog.{catalog_name}.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    f\"spark.sql.catalog.{catalog_name}.scope\": \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    # Optional Parameter to configure which kind of vended-credential to use for S3:\n    f\"spark.sql.catalog.{catalog_name}.header.X-Iceberg-Access-Delegation\": \"vended-credentials\" # Alternatively \"remote-signing\"\n}\n\nspark_conf = pyspark.SparkConf().setMaster(\"local[*]\")\n\nfor k, v in configuration.items():\n    spark_conf = spark_conf.set(k, v)\n\nspark = pyspark.sql.SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sql(f\"USE {catalog_name}\")\n</code></pre>"}, {"location": "docs/latest/docs/engines/#pyiceberg", "title": "PyIceberg", "text": "<pre><code>import pyiceberg.catalog\nimport pyiceberg.catalog.rest\nimport pyiceberg.typedef\n\ncatalog = pyiceberg.catalog.rest.RestCatalog(\n    name=\"my_catalog_name\",\n    uri=\"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    warehouse=\"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    #  Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    credential=\"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    **{\n        \"oauth2-server-uri\": \"http://localhost:30080/realms/&lt;keycloak realm name&gt;/protocol/openid-connect/token\"\n    },\n    # Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    scope=\"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n)\n\nprint(catalog.list_namespaces())\n</code></pre>"}, {"location": "docs/latest/docs/engines/#aws-athena-spark", "title": "AWS Athena (Spark)", "text": "<p>Amazon Athena is a serverless query service that allows you to use SQL or PySpark to query data in Lakekeeper without provisioning infrastructure. The following steps demonstrate how to connect Athena PySpark with Lakekeeper.</p> <p>1. Create an Apache Spark workgroup in the AWS Athena console:</p> <ul> <li>Go to the Athena console &gt; Administration &gt; Workgroups</li> <li>Create a workgroup with Apache Spark as the analytics engine</li> </ul> <p>2. Create a new PySpark notebook:</p> <ul> <li>Give your notebook a name</li> <li>Select your Spark workgroup</li> <li> <p>Configure JSON properties with Lakekeeper catalog settings</p> <pre><code>{\n    \"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n    \"spark.sql.catalog.lakekeeper.type\": \"rest\",\n    \"spark.sql.catalog.lakekeeper.uri\": \"&lt;Lakekeeper Catalog URI&gt;\",\n    \"spark.sql.catalog.lakekeeper.warehouse\": \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"spark.sql.catalog.lakekeeper.credential\": \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\", \n    \"spark.sql.catalog.lakekeeper.oauth2-server-uri\": \"&lt;Token Endpoint of your IdP&gt;\"\n}\n</code></pre> </li> </ul> <p>3. Verify the connection in your notebook:</p> <pre><code># Verify connectivity to your Lakekeeper catalog\nspark.sql(\"select count(*) from lakekeeper.&lt;namespace&gt;.&lt;table&gt;\").show()\n</code></pre> <p>Amazon Athena has Iceberg pre-installed, so no additional package installations are required.</p>"}, {"location": "docs/latest/docs/engines/#starrocks", "title": "Starrocks", "text": "<p>Starrocks is improving the Iceberg REST support quickly. This guide is written for Starrocks 3.3, which does not support vended-credentials for AWS S3 with custom endpoints.</p> <p>The following docker compose examples are available for starrocks:</p> <ul> <li><code>Minimal</code>: No authentication</li> <li><code>Access-Control</code>: Lakekeeper secured with OAuth2, single technical user for starrocks</li> </ul> <p>Note: If you are using an IdP like Keycloak, in order for Starrocks to be able to authenticate with Lakekeeper you must ensure the client you are connecting to has \"Standard Token Exchange\" (or equivalent) enabled. Otherwise Starrocks will be unable to refresh access tokens and you will get authentication errors when the initial access token created by the <code>CREATE EXTERNAL CATALOG</code> command expires.</p> S3-Compatible <pre><code>CREATE EXTERNAL CATALOG rest_catalog\nPROPERTIES\n(\n    \"type\" = \"iceberg\",\n    \"iceberg.catalog.type\" = \"rest\",\n    \"iceberg.catalog.uri\" = \"&lt;Lakekeeper Catalog URI, i.e. http://localhost:8181/catalog&gt;\",\n    \"iceberg.catalog.warehouse\" = \"&lt;Name of the Warehouse in Lakekeeper&gt;\",\n    -- Required Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.security\" = \"OAUTH2\",\n    \"iceberg.catalog.oauth2-server-uri\" = \"&lt;Token Endpoint of your IdP, i.e. http://keycloak:8080/realms/iceberg/protocol/openid-connect/token&gt;\",\n    \"iceberg.catalog.credential\" = \"&lt;Client-ID&gt;:&lt;Client-Secret&gt;\",\n    -- Optional Parameters if OAuth2 authentication is enabled for Lakekeeper:\n    \"iceberg.catalog.scope\" = \"&lt;Scopes to request from the IdP, i.e. lakekeeper&gt;\",\n    -- S3 specific configuration, probably not required anymore in version 3.4.1 and newer.\n    \"aws.s3.region\" = \"&lt;AWS Region to use. For S3-compatible storage use a non-existent AWS region, such as local&gt;\",\n    \"aws.s3.access_key\" = \"&lt;S3 Access Key&gt;\",\n    \"aws.s3.secret_key\" = \"&lt;S3 Secret Access Key&gt;\",\n    -- Required for some S3-compatible storages:\n    \"aws.s3.endpoint\" = \"&lt;Custom S3 endpoint&gt;\",\n    \"aws.s3.enable_path_style_access\" = \"true\"\n)\n\n-- You must set your catalog in the current session before you can query Iceberg data\nSET CATALOG rest_catalog;\n\n-- Starrocks uses MySQL compatible terminology. This is equivalent to Namespaces\nSHOW DATABASES;\n\n-- Starrocks will let you create resources in Lakekeeper\nCREATE DATABASE testing;\n\n-- You must use your namespace like a SQL database\nUSE `testing`;\n\n-- In this case Tables is the same between MySQL and Iceberg.\nSHOW TABLES;\n\n-- You can also create tables, INSERT INTO them, and query them just like you would any other SQL database.\n</code></pre>"}, {"location": "docs/latest/docs/engines/#olake", "title": "OLake", "text": "<p>OLake is an open-source, quick and scalable tool for replicating Databases to Apache Iceberg or Data Lakehouses written in Go. Visit the Olake Iceberg Documentation for the full documentation, and additional information on Olake.</p> S3-Compatible <pre><code>{\n\"type\": \"ICEBERG\",\n    \"writer\": {\n        \"catalog_type\": \"rest\",\n        \"normalization\": false,\n        \"rest_catalog_url\": \"http://localhost:8181/catalog\",\n        \"iceberg_s3_path\": \"warehouse\",\n        \"iceberg_db\": \"ICEBERG_DATABASE_NAME\"\n    }\n}\n</code></pre>"}, {"location": "docs/latest/docs/engines/#risingwave", "title": "RisingWave", "text": "<p>RisingWave is a distributed SQL streaming database that is wire-compatible with PostgreSQL, designed for real-time data ingestion, processing, and querying. Unlike many other query engines that use a <code>CATALOG</code> abstraction, RisingWave connects to Lakekeeper through a <code>CONNECTION</code> object, which allows it to use Iceberg tables for sources, sinks, and internal tables.</p> <p>For a hands-on example, a Docker Compose setup is available in the RisingWave repository. You can find detailed deployment instructions in the official RisingWave documentation.</p> <p>Once you have both services running, you can create a <code>CONNECTION</code> in RisingWave to connect to Lakekeeper. The following is an example configuration. As parameters may change over time, please refer to the official RisingWave documentation for the most up-to-date and complete configuration options.</p> <pre><code>CREATE CONNECTION lakekeeper_catalog_conn\nWITH (\n    type = 'iceberg',\n    catalog.type = 'rest',\n    catalog.uri = 'http://lakekeeper:8181/catalog/',\n    warehouse.path = 'risingwave-warehouse',\n    s3.access.key = 'hummockadmin',\n    s3.secret.key = 'hummockadmin',\n    s3.path.style.access = 'true',\n    s3.endpoint = 'http://minio-0:9301',\n    s3.region = 'us-east-1'\n);\n</code></pre> <p>After creating the connection, you must set it as the default for your session to create and query internal Iceberg tables. The <code>SET</code> command applies the change to the current session only, while <code>ALTER SYSTEM</code> makes it persistent across restarts.</p> <pre><code>-- Set for the current session\nSET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n\n-- Set persistent for the system\nALTER SYSTEM SET iceberg_engine_connection = 'public.lakekeeper_catalog_conn';\n</code></pre>"}, {"location": "docs/latest/docs/gotchas/", "title": "Gotchas", "text": ""}, {"location": "docs/latest/docs/gotchas/#i-got-permissions-but-am-still-getting-403s", "title": "I got permissions but am still getting 403s", "text": "<p>Lakekeeper does not always return 404s for missing objects. If you are getting 403s while having correct grants, it is likely that the object you are trying to access does not exist. This is a security feature to prevent information leakage.</p>"}, {"location": "docs/latest/docs/gotchas/#im-using-helm-and-the-ui-seems-to-hang-forever", "title": "I'm using Helm and the UI seems to hang forever", "text": "<p>Check out our routing guide, both the catalog and UI create links pointing at the Lakekeeper instance. We use some heuristics by default and also offer a configuration escape hatch (<code>catalog.config.ICEBERG_REST__BASE_URI</code>).</p>"}, {"location": "docs/latest/docs/gotchas/#examples", "title": "Examples", "text": ""}, {"location": "docs/latest/docs/gotchas/#local", "title": "Local", "text": "<pre><code>k port-forward services/my-lakekeeper 7777:8181\n</code></pre> <pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is forwarded to localhost:7777\n    ICEBERG_REST__BASE_URI: \"http://localhost:7777\"\n</code></pre>"}, {"location": "docs/latest/docs/gotchas/#public", "title": "Public", "text": "<pre><code>catalog:\n   # omitting the rest of the values\n  config:\n    # assuming that the catalog is reachable at https://lakekeeper.example.com\n    ICEBERG_REST__BASE_URI: \"https://lakekeeper.example.com\"\n</code></pre>"}, {"location": "docs/latest/docs/gotchas/#im-using-postgres-15-and-the-lakekeeper-database-migrations-fail-with-syntax-error", "title": "I'm using Postgres &lt;15 and the Lakekeeper database migrations fail with syntax error", "text": "<pre><code>Caused by:\n0: error returned from database: syntax error at or near \"NULLS\"\n1: syntax error at or near \"NULLS\"\n</code></pre> <p>Lakekeeper is currently only compatible with Postgres &gt;= 15 since we rely on <code>NULLS not distinct</code> which was added with PG 15.</p>"}, {"location": "docs/latest/docs/management/", "title": "Lakekeeper Management API", "text": "<p>Lakekeeper is a rust-native Apache Iceberg REST Catalog implementation. The Management API provides endpoints to manage the server, projects, warehouses, users, and roles. If Authorization is enabled, permissions can also be managed. An interactive Swagger-UI for the specific Lakekeeper Version and configuration running is available at <code>/swagger-ui/#/</code> of Lakekeeper (by default http://localhost:8181/swagger-ui/#/).</p> <pre><code>git clone https://github.com/lakekeeper/lakekeeper.git\ncd lakekeeper/examples/minimal\ndocker compose up\n</code></pre> <p>Then open your browser at http://localhost:8181/swagger-ui/#/.</p>"}, {"location": "docs/latest/docs/opa/", "title": "Open Policy Agent (OPA)", "text": "<p>Lakekeeper's Open Policy Agent bridge enables compute engines that support fine-grained access control via Open Policy Agent (OPA) as authorization engine to respect privileges in Lakekeeper. We have also prepared a self-contained Docker Compose Example to get started quickly.</p> <p>Let's imagine we have a trusted multi-user query engine such as trino, in addition to single-user query engines like pyiceberg or daft in Jupyter Notebooks. Managing permissions in trino independently of the other tools is not an option, as we do not want to duplicate permissions across query engines. Our multi-user query engine has two options:</p> <ol> <li>Catalog enforces permissions: The engine contacts the Catalog on behalf of the user. To achieve this, the engine must be able to impersonate the user for the catalog application. In OAuth2 settings, this can be accomplished through downscoping tokens or other forms of Token Exchange.</li> <li>Compute enforces permissions: After contacting the catalog with a god-like \"I can do everything!\" user (e.g. <code>project_admin</code>), the query engine then contacts the permission system, retrieves, and enforces those permissions. Note that this requires the engine to run in a trusted environment, as whoever has root access to the engine also has access to the god-like credential.</li> </ol> <p>The Lakekeeper OPA Bridge enables solution 2, by exposing all permissions in Lakekeeper via OPA. The Bridge itself is a collection of OPA files in the <code>authz/opa-bridge</code> folder of the Lakekeeper GitHub repository.</p> <p>The bridge also comes with a translation layer for trino to translate trino to Lakekeeper permissions and thus serve trinos OPA queries. Currently trino is the only iceberg query engine we are aware of that is flexible enough to honor external permissions via OPA. Please let us know if you are aware of other engines, so that we can add support.</p>"}, {"location": "docs/latest/docs/opa/#configuration", "title": "Configuration", "text": "<p>Lakekeeper's OPA bridge needs to access the permissions API of Lakekeeper. As such, we need a technical user for OPA (Client ID, Client Secret) that OPA can use to authenticate to Lakekeeper. Please check the Authentication guide for more information on how to create technical users. We recommend to use the same user for creating the catalog in trino to ensure same access. In most scenarios, this user should have the <code>project_admin</code> role.</p> <p>The plugin can be customized by either editing the <code>configuration.rego</code> file or by setting environment variables. By editing the <code>configuration.rego</code> files you can also easily connect multiple lakekeeper instance to the same trino instance. Please find all available configuration options explained in the file.</p> <p>If configuration is done via environment variables, the following settings are available:</p> Variable Example Description <code>LAKEKEEPER_URL</code> <code>https://lakekeeper.example.com</code> URL where lakekeeper is externally reachable. Default: <code>https://localhost:8181</code> <code>LAKEKEEPER_TOKEN_ENDPOINT</code> <code>http://keycloak:8080/realms/iceberg/protocol/openid-connect/token</code> Token endpoint of the IdP used to secure Lakekeeper. This endpoint is used to exchange OPAs client credentials for an access token. <code>LAKEKEEPER_CLIENT_ID</code> <code>trino</code> Client ID used by OPA to access Lakekeeper's permissions API. <code>LAKEKEEPER_CLIENT_SECRET</code> <code>abcd</code> Client Secret for the Client ID. <code>LAKEKEEPER_SCOPE</code> <code>lakekeeper</code> Scopes to request from the IdP. Defaults to <code>lakekeeper</code>. Please check the Authentication Guide for setup. <p>All above mentioned configuration options refer to a specific Lakekeeper instance. What is missing is a mapping of trino catalogs to Lakekeeper warehouses. By default we support 4 catalogs in trino, but more can easily be added in the <code>configuration.rego</code>.</p> Variable Example Description <code>TRINO_DEV_CATALOG_NAME</code> <code>dev</code> Name of the development catalog in trino. Default: <code>dev</code> <code>LAKEKEEPER_DEV_WAREHOUSE</code> <code>development</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEV_CATALOG_NAME</code> catalog in trino. Default: <code>development</code> <code>TRINO_PROD_CATALOG_NAME</code> <code>prod</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_PROD_WAREHOUSE</code> <code>production</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_PROD_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <code>TRINO_DEMO_CATALOG_NAME</code> <code>demo</code> Name of the development catalog in trino. Default: <code>prod</code> <code>LAKEKEEPER_DEMO_WAREHOUSE</code> <code>demo</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_DEMO_CATALOG_NAME</code> catalog in trino. Default: <code>demo</code> <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> <code>lakekeeper</code> Name of the development catalog in trino. Default: <code>lakekeeper</code> <code>LAKEKEEPER_LAKEKEEPER_WAREHOUSE</code> <code>lakekeeper</code> Name of the development warehouse in lakekeeper that corresponds to the <code>TRINO_LAKEKEEPER_CATALOG_NAME</code> catalog in trino. Default: <code>production</code> <p>When OPA is running and configured, set the following configurations for trino in <code>access-control.properties</code>: <pre><code>access-control.name=opa\nopa.policy.uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/allow\nopa.log-requests=true\nopa.log-responses=true\nopa.policy.batched-uri=http://&lt;URL where OPA is reachable&gt;/v1/data/trino/batch\n</code></pre></p> <p>A full self-contained example is available on GitHub.</p>"}, {"location": "docs/latest/docs/production/", "title": "Production Checklist", "text": "<p>Lakekeeper is the heart of your data platform and needs to integrate deeply with your existing infrastructure such as IdPs. The easiest way to get Lakekeeper to production is our enterprise support. Please find more information on our commercial offerings at lakekeeper.io</p> <p>Please find following some general recommendations for productive setups:</p> <ul> <li>Use an external high-available database as a catalog backend. We recommend using a managed service in your preferred Cloud or host a high available cluster on Kubernetes yourself using your preferred operator. We are using the amazing CloudNativePG internally. Make sure the Database is backed-up regularly.</li> <li>Ensure sure both <code>LAKEKEEPER__PG_DATABASE_URL_READ</code> and <code>LAKEKEEPER__PG_DATABASE_URL_WRITE</code> are set for ideal load distribution. Most postgres deployments specify separate URLs for reading and writing to channel writes to the master while distributing reads across replicas.</li> <li>For medium or large deployments, ensure the <code>LAKEKEEPER__PG_READ_POOL_CONNECTIONS</code> and <code>LAKEKEEPER__PG_WRITE_POOL_CONNECTIONS</code> are set to a higher value to allow Lakekeeper to use more connections to the database.</li> <li>For high-available setups, ensure that multiple Lakekeeper instances are running on different nodes. We recommend our helm chart for production deployments.</li> <li>Ensure that Authentication is enabled, typically by setting <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> and / or <code>LAKEKEEPER__ENABLE_KUBERNETES_AUTHENTICATION</code>. Check our Authentication Guide for more information.</li> <li>If <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set, we recommend to set <code>LAKEKEEPER__OPENID_AUDIENCE</code> as well.</li> <li>If Authorization is desired, follow our Authorization Guide. Ensure that OpenFGA is hosted in close proximity to Lakekeeper - ideally on the same VM or Kubernetes node. In our Helm-Chart we use <code>PodAffinity</code> to achieve this.</li> <li>When using OpenFGA, make sure that Caching is enabled. Check the OpenFGA in Production section for more information.</li> <li>If the default Postgres secret backend is used, ensure that <code>LAKEKEEPER__PG_ENCRYPTION_KEY</code> is set to a long random string.</li> <li>Ensure that all Warehouses use distinct storage locations / prefixes and distinct credentials that only grant access to the prefix used for a Warehouse.</li> <li>Ensure that SSL / TLS is enabled. Lakekeeper does not terminate connections natively. Please use a reverse proxy like Nginx or Envoy to secure the connection to Lakekeeper. On Kubernetes, any Ingress controller can be used. For high-availability, failover should be handled by the reverse proxy. Lakekeeper exposes a <code>/health</code> endpoint that can be used to determine its current status. If you are using our helm-chart, probes are already built-in.</li> <li>When using our helm-chart with the default postgres secret store, we recommend to set <code>secretBackend.postgres.encryptionKeySecret</code> to use a pre-created secret to reduce the risk of overwriting the secret created by the helm-chart.</li> <li>If a trusted query engine, such as a centrally managed trino, uses Lakekeeper's OPA bridge, ensure that no users have root access to trino or OPA as those contain credentials to Lakekeeper with very high permissions.</li> <li>Specify the <code>LAKEKEEPER__OPENID_SUBJECT_CLAIM</code> configuration value if <code>LAKEKEEPER__OPENID_PROVIDER_URI</code> is set. To identify a user in OAuth tokens, by default, Lakekeeper uses the <code>oid</code> field if present, otherwise the <code>sub</code> field is used. We strongly recommend setting this configuration explicitly in production deployments. Entra-ID users want to use the <code>oid</code> claim, users from all other IdPs most likely want to use the <code>sub</code> claim.</li> <li>Create regular Backups of your Lakekeeper database (Postgres) and OpenFGA (if used). Test your backup and restore process regularly. Always backup the Lakekeeper database before upgrading Lakekeeper or OpenFGA.</li> </ul>"}, {"location": "docs/latest/docs/storage/", "title": "Storage", "text": "<p>Storage in Lakekeeper is bound to a Warehouse. Each Warehouse stores data in a location defined by a <code>StorageProfile</code> attached to it.</p> <p>Currently, we support the following storages:</p> <ul> <li>S3 (tested with AWS &amp; Minio)</li> <li>Azure Data Lake Storage Gen 2</li> <li>Google Cloud Storage (with and without Hierarchical Namespaces) When creating a Warehouse or updating storage information, Lakekeeper validates the configuration.</li> </ul> <p>By default, Lakekeeper Warehouses enforce specific URI schemas for tables and views to ensure compatibility with most query engines:</p> <ul> <li>S3 / AWS Warehouses: Must start with <code>s3://</code></li> <li>Azure / ADLS Warehouses: Must start with <code>abfss://</code></li> <li>GCP Warehouses: Must start with <code>gs://</code></li> </ul> <p>When a new table is created without an explicitly specified location, Lakekeeper automatically assigns the appropriate protocol based on the storage type. If a location is explicitly provided by the client, it must adhere to the required schema.</p> <p>// ...existing code...</p>"}, {"location": "docs/latest/docs/storage/#disabling-credential-vending-remote-signing", "title": "Disabling Credential Vending &amp; Remote Signing", "text": "<p>Lakekeeper provides multiple ways to control how credentials and remote signing information are provided to clients.</p> <p>You can disable credential vending and remote signing on a per-warehouse basis using storage profile settings. For S3 warehouses, set <code>remote-signing-enabled</code> to <code>false</code> to disable remote signing and <code>sts-enabled</code> to <code>false</code> to disable STS vended credentials. For Azure ADLS warehouses, set <code>sas-enabled</code> to <code>false</code> to disable SAS token generation. For GCS warehouses, set <code>sts-enabled</code> to <code>false</code> to disable STS token generation. When these options are disabled at the storage profile level, clients will not receive the corresponding credentials or signing information for that warehouse, regardless of the request headers. Lakekeeper downscopes vended credentials for all supported storages to the location of the table being accessed and ensures that there are no overlapping table locations within a warehouse.</p> <p>Clients can also control credential delegation per request using the <code>X-Iceberg-Access-Delegation</code> header. Lakekeeper supports the standard Iceberg REST spec values (<code>vended-credentials</code> and <code>remote-signing</code>), plus a special <code>client-managed</code> value. When set to <code>client-managed</code>, no credentials or signing information are returned, regardless of storage profile configuration. This allows clients to use their own credentials for direct storage access.</p>"}, {"location": "docs/latest/docs/storage/#allowing-alternative-protocols-s3a-s3n-wasbs", "title": "Allowing Alternative Protocols (s3a, s3n, wasbs)", "text": "<p>For S3 / AWS and Azure / ADLS Warehouses, Lakekeeper optionally supports additional protocols. To enable these, activate the \"Allow Alternative Protocols\" flag in the storage profile of the Warehouse. When enabled, the following additional protocols are accepted for table creation or registration:</p> <ul> <li>S3 / AWS Warehouses: Supports <code>s3a://</code> and <code>s3n://</code> in addition to <code>s3://</code></li> <li>Azure Warehouses: Supports <code>wasbs://</code> in addition to <code>abfss://</code></li> </ul>"}, {"location": "docs/latest/docs/storage/#s3", "title": "S3", "text": "<p>We support remote signing and vended-credentials with S3-compatible storages &amp; AWS. Both provide a secure way to access data on S3:</p> <ul> <li>Remote Signing: The client prepares an S3 request and sends its headers to the sign endpoint of Lakekeeper. Lakekeeper checks if the request is allowed, if so, it signs the request with its own credentials, creating additional headers during the process. These additional signing headers are returned to the client, which then contacts S3 directly to perform the operation on files.</li> <li>Vended Credentials: Lakekeeper uses the \"STS\" Endpoint of S3 to generate temporary credentials which are then returned to clients.</li> </ul> <p>Remote signing works natively with all S3 storages that support the default <code>AWS Signature Version 4</code>. This includes almost all S3 solutions on the market today, including Rook Ceph Rados, NetApp StorageGRID 12.0 or newer, Minio and others. Vended credentials in turn depend on an additional \"STS\" Endpoint, that is not supported by all S3 implementations. We run our integration tests for vended credentials against Minio and AWS. We recommend to setup vended credentials for all supported stores, remote signing is not supported by all clients.</p> <p>When a client requests table configuration, Lakekeeper selects between remote signing and vended credentials based on the <code>X-Iceberg-Access-Delegation</code> header and storage profile settings:</p> <ul> <li>If the header is set to <code>client-managed</code>, neither credentials nor signing information are returned</li> <li>If the header specifies <code>vended-credentials</code> or <code>remote-signing</code>, that method is used if enabled in the storage profile</li> <li>If both methods are requested or neither is specified, Lakekeeper attempts to provide vended credentials first (if STS is enabled), then falls back to remote signing (if enabled)</li> <li>If both methods are disabled at the storage profile level, no credentials are returned regardless of the header value</li> </ul> <p>For maximum client compatibility, we recommend enabling both STS and remote signing when your S3 storage supports it.</p> <p>For some older remote signing clients that cannot handle table-specific remote signing endpoint locations, Lakekeeper needs to identifying a table by its location in the storage. Since there are multiple canonical ways to specify S3 resources (virtual-host &amp; path), Lakekeeper warehouses by default use a heuristic to determine which style is used. For some setups these heuristics may not work, or you may want to enforce a specific style. In this case, you can set the <code>remote-signing-url-style</code> field to either <code>path</code> or <code>virtual-host</code> in your storage profile. <code>path</code> will always use the first path segment as the bucket name. <code>virtual-host</code> will use the first subdomain if it is followed by <code>.s3</code> or <code>.s3-</code>. The default mode is <code>auto</code> which first tries <code>virtual-host</code> and falls back to <code>path</code> if it fails.</p>"}, {"location": "docs/latest/docs/storage/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an S3 storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the S3 bucket. Must be between 3-63 characters, containing only lowercase letters, numbers, dots, and hyphens. Must begin and end with a letter or number. <code>region</code> String Yes - AWS region where the bucket is located. For S3-compatible storage, any string can be used (e.g., \"local-01\"). <code>sts-enabled</code> Boolean Yes - Whether to enable STS for vended credentials. Not all S3 compatible object stores support \"AssumeRole\" via STS. We strongly recommend to enable sts if the storage system supports it. <code>remote-signing-enabled</code> Boolean No <code>true</code> Whether to enable remote signing for S3 requests. When disabled, clients cannot use remote signing for this storage profile even if STS is disabled. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>endpoint</code> URL No None Optional endpoint URL for S3 requests. If not provided, the region will be used to determine the endpoint. If both are provided, the endpoint takes precedence. Example: <code>http://s3-de.my-domain.com:9000</code> <code>flavor</code> String No <code>aws</code> S3 flavor to use. Options: <code>aws</code> (Amazon S3) or <code>s3-compat</code> (for S3-compatible solutions like MinIO). <code>path-style-access</code> Boolean No <code>false</code> Whether to use path style access for S3 requests. If the underlying S3 supports both virtual host and path styles, we recommend not setting this option. <code>assume-role-arn</code> String No None Optional ARN to assume when accessing the bucket from Lakekeeper. This is also used as the default for <code>sts-role-arn</code> if that is not specified. <code>sts-role-arn</code> String No Value of <code>assume-role-arn</code> Optional role ARN to assume for STS vended-credentials. Either <code>assume-role-arn</code> or <code>sts-role-arn</code> must be provided if <code>sts-enabled</code> is true and <code>flavor</code> is <code>aws</code>. <code>sts-token-validity-seconds</code> Integer No <code>3600</code> The validity period of STS tokens in seconds. Controls how long the vended credentials remain valid before they need to be refreshed. <code>sts-session-tags</code> Object No <code>{}</code> An optional JSON object containing key-value pairs of session tags to apply when assuming roles via STS. These tags are attached to the temporary credentials and can be used for access control, auditing, or cost allocation. Each key and value must be a string. Example: <code>{\"Environment\": \"production\", \"Team\": \"data-engineering\"}</code> <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>s3a://</code> and <code>s3n://</code> in locations. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. Tables with <code>s3a</code> paths are not accessible outside the Java ecosystem. <code>remote-signing-url-style</code> String No <code>auto</code> S3 URL style detection mode for remote signing. Options: <code>auto</code>, <code>path-style</code>, or <code>virtual-host</code>. When set to <code>auto</code>, Lakekeeper tries virtual-host style first, then path style. <code>push-s3-delete-disabled</code> Boolean No <code>true</code> Controls whether the <code>s3.delete-enabled=false</code> flag is sent to clients. Only has an effect if \"soft-deletion\" is enabled for this Warehouse. This prevents clients like Spark from directly deleting files during operations like <code>DROP TABLE xxx PURGE</code>, ensuring soft-deletion works properly. However, it also affects operations like <code>expire_snapshots</code> that require file deletion. For more information, please check the Soft Deletion Documentation. <code>aws-kms-key-arn</code> String No None ARN of the AWS KMS Key that is used to encrypt the bucket. Vended Credentials is granted <code>kms:Decrypt</code> and <code>kms:GenerateDataKey</code> on the key. <code>legacy-md5-behavior</code> Boolean No <code>false</code> A flag to enable the legacy behavior of using MD5 checksums for operations that require checksums."}, {"location": "docs/latest/docs/storage/#aws", "title": "AWS", "text": ""}, {"location": "docs/latest/docs/storage/#direct-file-access-with-access-key", "title": "Direct File-Access with Access Key", "text": "<p>First create a new S3 bucket for the warehouse. Buckets can be re-used for multiple Warehouses as long as the <code>key-prefix</code> is different. We recommend to block all public access.</p> <p>Secondly we need to create an AWS role that can access and delegate access to the bucket. We start by creating a new Policy that allows access to data in the bucket. We call this policy <code>LakekeeperWarehouseDev</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBuckets\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"ListBucketContent\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::lakekeeper-aws-demo\"\n        },\n        {\n            \"Sid\": \"DataAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::lakekeeper-aws-demo/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Now create a new user, we call the user <code>LakekeeperWarehouseDev</code>, and attach the previously created policy. When the user is created, click on \"Security credentials\" and \"Create access key\". Note down the access key and secret key for later use.</p> <p>We are done if we only rely on remote signing. For vended credentials, we need to perform one more step. Create a new role that we call <code>LakekeeperWarehouseDevRole</code>. This role needs to be trusted by the user, which is achieved via with the following trust policy: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"TrustLakekeeperWarehouseDev\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::&lt;aws-account-id&gt;:user/LakekeeperWarehouseDev\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></p> <p>Also attach the <code>LakekeeperWarehouseDev</code> policy created earlier.</p> <p>We are now ready to create the Warehouse via the UI or REST-API using the following values (make sure to replace everything in <code>&lt;&gt;</code>):</p> <p><pre><code>{\n    \"warehouse-name\": \"aws_docs\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n        \"sts-role-arn\": \"arn:aws:iam::&lt;aws account id&gt;:role/LakekeeperWarehouseDevRole\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre> As part of the <code>storage-profile</code>, the field <code>assume-role-arn</code> can optionally be specified. If it is specified, this role is assumed for every IO Operation of Lakekeeper. It is also used as <code>sts-role-arn</code>, unless <code>sts-role-arn</code> is specified explicitly. If no <code>assume-role-arn</code> is specified, whatever authentication method / user os configured via the <code>storage-credential</code> is used directly for IO Operations, so needs to have S3 access policies attached directly (as shown in the example above).</p>"}, {"location": "docs/latest/docs/storage/#system-identities-managed-identities", "title": "System Identities / Managed Identities", "text": "<p>Since Lakekeeper version 0.8, credentials for S3 access can also be loaded directly from the environment. Lakekeeper integrates with the AWS SDK to support standard environment-based authentication, including all common configuration options through AWS_* environment variables.</p> <p>Note</p> <p>When using system identities, we strongly recommend configuring external-id values. This prevents unauthorized cross-account role access and ensures roles can only be assumed by authorized Lakekeeper warehouses.</p> <p>Without external IDs, any user with warehouse creation permissions in Lakekeeper could potentially access any role the system identity is allowed to assume. For more information, see AWS's documentation on external IDs.</p> <p>Below is a step-by-step guide for setting up a secure system identity configuration:</p> <p>Firstly, create a dedicated AWS user to serve as your system identity. Do not attach any direct permissions or trust policies to this user. This user will only have the ability to assume specific roles with the proper external ID</p> <p>Secondly, configure Lakekeeper with this identity by setting the following environment variables.</p> <pre><code>AWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\nAWS_DEFAULT_REGION=...\n# Required for System Credentials to work:\nLAKEKEEPER__S3_REQUIRE_EXTERNAL_ID_FOR_SYSTEM_CREDENTIALS=true\n</code></pre> <p>In addition to the standard <code>AWS_*</code> environment variables, Lakekeeper supports all authentication methods available in the AWS SDK, including instance profiles, container credentials, and SSO configurations.</p> <p>For enhanced security, Lakekeeper enforces that warehouses using system identities must specify both an <code>external-id</code> and an <code>assume-role-arn</code> when configured. This implementation follows AWS security best practices by preventing unauthorized role assumption. These default requirements can be adjusted through settings described in the Configuration Guide.</p> <p>For this example, assume the system identity has the ARN <code>arn:aws:iam::123:user/lakekeeper-system-identity</code>.</p> <p>When creating a warehouse, users must configure an IAM role with an appropriate trust policy. The following trust policy template enables the Lakekeeper system identity to assume the role, while enforcing external ID validation:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>The role also needs S3 access, so attach a policy like this: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInWarehouseFolder\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/&lt;key-prefix if used&gt;/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowRootAndHomeListing\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;\",\n                \"arn:aws:s3:::&lt;bucket-name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>We are now ready to create the Warehouse using the system identity: <pre><code>{\n    \"warehouse-name\": \"aws_docs_managed_identity\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"credential-type\": \"aws-system-identity\",\n        \"external-id\": \"&lt;external id configured in the trust policy of the role&gt;\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"assume-role-arn\": \"&lt;arn of the role that was created&gt;\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"&lt;region of the bucket&gt;\",\n        \"sts-enabled\": true,\n        \"flavor\": \"aws\",\n        \"key-prefix\": \"&lt;path to warehouse in bucket&gt;\"\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre></p> <p>The specified <code>assume-role-arn</code> is used for Lakekeeper's reads and writes of the object store. It is also used as a default for <code>sts-role-arn</code>, which is the role that is assumed when generating vended credentials for clients (with an attached policy for the accessed table).</p>"}, {"location": "docs/latest/docs/storage/#cors-configuration", "title": "CORS Configuration", "text": "<p>For browser-based access to S3 buckets (required for DuckDB WASM), you need to configure CORS (Cross-Origin Resource Sharing) on your S3 bucket.</p> <p>To configure CORS for your S3 bucket:</p> <ol> <li>In the AWS S3 Configuration Menu, klick on the name of your bucket</li> <li>Choose Permissions Tab</li> <li>In the Cross-origin resource sharing (CORS) section, choose Edit</li> <li>In the CORS configuration editor text box, type or copy and paste a new CORS configuration, or edit an existing configuration. The CORS configuration is a JSON file. The text that you type in the editor must be valid JSON. See below for an example.</li> <li>Choose Save changes</li> </ol> <p>Example CORS policy:</p> <pre><code>[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"POST\",\n            \"PUT\",\n            \"DELETE\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"https://lakekeeper.example.com\"\n        ],\n        \"ExposeHeaders\": []\n    }\n]\n</code></pre> <p>Replace <code>https://lakekeeper.example.com</code> with the origin where your Lakekeeper instance is hosted.</p>"}, {"location": "docs/latest/docs/storage/#sts-session-tags", "title": "STS Session Tags", "text": "<p>The optional <code>sts-session-tags</code> setting can be used to provide Session Tags when assuming roles via STS. Doing so requires that the IAM Role's Trust Relationship also allow <code>sts:TagSession</code>. Here's the above example with this addition:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"&lt;Use a secure random string that cannot be guessed. Treat it like a password.&gt;\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowSessionTagging\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123:user/lakekeeper-system-identity\"\n            },\n            \"Action\": \"sts:TagSession\"\n        }\n    ]\n}\n</code></pre> <p>If wanting to use a session tag in an ABAC policy, one can reference that tag via <code>${aws:PrincipalTag/&lt;tag name&gt;}</code>. For example, here's a policy that dynamically sets the S3 path based on a <code>tenant</code> tag: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowAllAccessInTenantWarehouse\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket-name&gt;/${aws:PrincipalTag/tenant}/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Sid\": \"AllowListingInTenantWarehouse\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucket-name&gt;\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"${aws:PrincipalTag/tenant}/*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre></p>"}, {"location": "docs/latest/docs/storage/#s3-compatible", "title": "S3 Compatible", "text": "<p>Unlike for AWS, we do not need any special trust-setup for vended credentials / STS with most S3 compatible solutions like Minio. Instead, we just need a bucket and an access key / secret key combination that is able to read and write from it. If <code>sts-role-arn</code> is provided, it will be sent as part of the request to the STS service. Keep in mind that the specific S3 compatible solution may ignore the parameter. Conversely, if <code>sts-role-arn</code> is not specified, the request to the STS service will not contain it. Make sure to select <code>flavor</code> to have the value <code>s3-compat</code>! This setting should work for most self-hosted S3 solutions.</p> <p>An warehouse create call could look like this:</p> <pre><code>{\n    \"warehouse-name\": \"minio_dev\",\n    \"storage-credential\": {\n        \"type\": \"s3\",\n        \"aws-access-key-id\": \"&lt;Access Key of the created user&gt;\",\n        \"aws-secret-access-key\": \"&lt;Secret Key of the created user&gt;\",\n        \"credential-type\": \"access-key\"\n    },\n    \"storage-profile\": {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of the bucket&gt;\",\n        \"region\": \"local-01\",\n        \"sts-enabled\": true,\n        \"flavor\": \"s3-compat\",\n        \"key-prefix\": \"lakekeeper-dev-warehouse\",\n    },\n    \"delete-profile\": {\n        \"type\": \"hard\"\n    }\n}\n</code></pre>"}, {"location": "docs/latest/docs/storage/#cloudflare-r2", "title": "Cloudflare R2", "text": "<p>Lakekeeper supports Cloudflare R2 storage with all S3 compatible clients, including vended credentials via the <code>/accounts/{account_id}/r2/temp-access-credentials</code> Endpoint.</p> <p>First we create a new Bucket. In the cloudflare UI, Select \"R2 Object Storage\" -&gt; \"Overview\" and select \"+ Create Bucket\". We call our bucket <code>lakekeeper-dev</code>. Click on the bucket, select the \"Settings\" tab, and note down the \"S3 API\" displayed.</p> <p>Secondly, we create an API Token for Lakekeeper as follows:</p> <ol> <li>Go back to the Overview Page (\"R2 Object Storage\" -&gt; \"Overview\") and select \"Manage API tokens\" in the \"{} API\" dropdown.</li> <li>In the R2 token page select \"Create Account API token\". Give the token any name. Select the \"Admin Read &amp; Write\" permission, this is unfortunately required at the time of writing, as the <code>/accounts/{account_id}/r2/temp-access-credentials</code> does not accept other tokens. Click \"Create Account API Token\".</li> <li>Note down the \"Token value\", \"Access Key ID\" and \"Secret Access Key\"</li> </ol> <p>Finally, we can create the Warehouse in Lakekeeper via the UI or API. A POST request to <code>/management/v1/warehouse</code> expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"r2_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n        \"credential-type\": \"cloudflare-r2\",\n        \"account-id\": \"&lt;Cloudflare Account ID, typically the long alphanumeric string before the first dot in the S3 API URL&gt; \",\n        \"access-key-id\": \"access-key-id-from-above\",\n        \"secret-access-key\": \"secret-access-key-from-above\",\n        \"token\": \"token-from-above\",\n    },\n  \"storage-profile\":\n    {\n        \"type\": \"s3\",\n        \"bucket\": \"&lt;name of your cloudflare r2 bucket, lakekeeper-dev in our example&gt;\",\n        \"region\": \"&lt;your cloudflare region, i.e. weur&gt;\",\n        \"key-prefix\": \"path/to/my/warehouse\",\n        \"endpoint\": \"&lt;S3 API Endpoint, i.e. https://&lt;account-id&gt;.eu.r2.cloudflarestorage.com&gt;\"\n    },\n}\n</code></pre> <p>For cloudflare R2 credentials, the following parameters are automatically set:</p> <ul> <li><code>assume-role-arn</code> is set to None, as this is not supported</li> <li><code>sts-enabled</code> is set to <code>true</code></li> <li><code>flavor</code> is set to <code>s3-compat</code></li> </ul> <p>It is required to specify the <code>endpoint</code>. Use a Data Location Hint as region.</p>"}, {"location": "docs/latest/docs/storage/#azure-data-lake-storage-gen-2", "title": "Azure Data Lake Storage Gen 2", "text": "<p>To add a Warehouse backed by ADLS, we need two Azure objects: The Storage Account itself and an App Registration which Lakekeeper can use to access it and delegate access to compute engines.</p>"}, {"location": "docs/latest/docs/storage/#configuration-parameters_1", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for an ADLS storage profile:</p> Parameter Type Required Default Description <code>account-name</code> String Yes - Name of the Azure storage account. <code>filesystem</code> String Yes - Name of the ADLS filesystem, in blob storage also known as container. <code>sas-enabled</code> Boolean No <code>true</code> Whether to enable SAS (Shared Access Signature) token generation for Azure Data Lake Storage. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <code>key-prefix</code> String No None Subpath in the filesystem to use. <code>allow-alternative-protocols</code> Boolean No <code>false</code> Whether to allow <code>wasbs://</code> in locations in addition to <code>abfss://</code>. This is disabled by default and should only be enabled for migrating legacy Hadoop-based tables via the register endpoint. <code>host</code> String No <code>dfs.core.windows.net</code> The host to use for the storage account. <code>authority-host</code> URL No <code>https://login.microsoftonline.com</code> The authority host to use for authentication. <code>sas-token-validity-seconds</code> Integer No <code>3600</code> The validity period of the SAS token in seconds. <p>Lets start by creating a new \"App Registration\":</p> <ol> <li>Create a new \"App Registration\"<ul> <li>Name: choose any, for this example we choose <code>Lakekeeper Warehouse (Development)</code></li> <li>Redirect URI: Leave empty</li> </ul> </li> <li>When the App Registration is created, select \"Manage\" -&gt; \"Certificates &amp; secrets\" and create a \"New client secret\". Note down the secrets \"Value\".</li> <li>In the \"Overview\" page of the \"App Registration\" note down the <code>Application (client) ID</code> and the <code>Directory (tenant) ID</code>.</li> </ol> <p>Next, we create a new Storage Account. Make sure to select \"Enable hierarchical namespace\" in the \"Advanced\" section. For existing Storage Accounts make sure \"Hierarchical namespace: Enabled\" is shown in the \"Overview\" page. There are no specific requirements otherwise. Note down the name of the storage account. When the storage account is created, we need to grant the correct permissions to the \"App Registration\" and create the filesystem / container where the data is stored:</p> <ol> <li>Open the Storage Account and select \"Data storage\" -&gt; Containers. Add a new Container, we call it <code>warehouse-dev</code>.</li> <li>Next, select \"Access Control (IAM)\" in the left menu and \"Add role assignment\". Grant the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the <code>Lakekeeper Warehouse (Development)</code> App Registration that we previously created.</li> </ol> <p>We are now ready to create the Warehouse via the UI or the REST API. Use the following information:</p> <ul> <li>client-id: The <code>Application (client) ID</code> of the <code>Lakekeeper Warehouse (Development)</code> App Registration.</li> <li>client-secret: The \"Value\" of the client secret that we noted down previously.</li> <li>tenant-id: The <code>Directory (tenant) ID</code> from the Applications Overview page.</li> <li>account-name: Name of the Storage Account</li> <li>filesystem: Name of the container (that Azure also calls filesystem) previously created. In our example its <code>warehouse-dev</code>.</li> </ul> <p>A POST request to <code>/management/v1/warehouse</code> would expects the following body:</p> <pre><code>{\n  \"warehouse-name\": \"azure_dev\",\n  \"delete-profile\": { \"type\": \"hard\" },\n  \"storage-credential\":\n    {\n      \"client-id\": \"...\",\n      \"client-secret\": \"...\",\n      \"credential-type\": \"client-credentials\",\n      \"tenant-id\": \"...\",\n      \"type\": \"az\",\n    },\n  \"storage-profile\":\n    {\n      \"account-name\": \"...\",\n      \"filesystem\": \"warehouse-dev\",\n      \"type\": \"adls\",\n    },\n}\n</code></pre>"}, {"location": "docs/latest/docs/storage/#azure-system-identity", "title": "Azure System Identity", "text": "<p>Warning</p> <p>Enabling Azure system identities allows Lakekeeper to access any storage location that the managed identity has permissions for. To minimize security risks, ensure the managed identity is restricted to only the necessary resources. Additionally, limit Warehouse creation permission in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>Azure system identities can be used to authenticate Lakekeeper to ADLS Gen 2, without specifying credentials explicitly on Warehouse creation. This feature is disabled by default and must be explicitly enabled system-wide by setting the following environment variable:</p> <pre><code>LAKEKEEPER__ENABLE_AZURE_SYSTEM_CREDENTIALS=true\n</code></pre> <p>When enabled, Lakekeeper will use the managed identity of the virtual machine or application it is running on to access ADLS. Ensure that the managed identity has the necessary permissions to access the storage account and container. For example, assign the <code>Storage Blob Data Contributor</code> and <code>Storage Blob Delegator</code> roles to the managed identity for the relevant storage account as described above.</p>"}, {"location": "docs/latest/docs/storage/#google-cloud-storage", "title": "Google Cloud Storage", "text": "<p>Google Cloud Storage can be used to store Iceberg tables through the <code>gs://</code> protocol.</p>"}, {"location": "docs/latest/docs/storage/#configuration-parameters_2", "title": "Configuration Parameters", "text": "<p>The following table describes all configuration parameters for a GCS storage profile:</p> Parameter Type Required Default Description <code>bucket</code> String Yes - Name of the GCS bucket. <code>key-prefix</code> String No None Subpath in the bucket to use for this warehouse. <code>sts-enabled</code> Boolean No <code>true</code> Whether to enable STS (Security Token Service) downscoped token generation for GCS. When disabled, clients cannot use vended credentials for this storage profile. Defaults to <code>true</code>. <p>The service account should have appropriate permissions (such as Storage Admin role) on the bucket. Since Lakekeeper Version 0.8.2, hierarchical Namespaces are supported.</p>"}, {"location": "docs/latest/docs/storage/#authentication-options", "title": "Authentication Options", "text": "<p>Lakekeeper supports two primary authentication methods for GCS:</p>"}, {"location": "docs/latest/docs/storage/#service-account-key", "title": "Service Account Key", "text": "<p>You can provide a service account key directly when creating a warehouse. This is the most straightforward way to give Lakekeeper access to your GCS bucket:</p> <pre><code>{\n  \"warehouse-name\": \"gcs_dev\",\n  \"storage-profile\": {\n    \"type\": \"gcs\",\n    \"bucket\": \"...\",\n    \"key-prefix\": \"...\"\n  },\n  \"storage-credential\": {\n    \"type\": \"gcs\",\n    \"credential-type\": \"service-account-key\",\n    \"key\": {\n      \"type\": \"service_account\",\n      \"project_id\": \"example-project-1234\",\n      \"private_key_id\": \"....\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n.....\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"abc@example-project-1234.iam.gserviceaccount.com\",\n      \"client_id\": \"123456789012345678901\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abc%example-project-1234.iam.gserviceaccount.com\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n  }\n}\n</code></pre> <p>The service account key should be created in the Google Cloud Console and should have the necessary permissions to access the bucket (typically Storage Admin role on the bucket).</p>"}, {"location": "docs/latest/docs/storage/#gcp-system-identity", "title": "GCP System Identity", "text": "<p>Warning</p> <p>Enabling GCP system identities grants Lakekeeper access to any storage location the service account has permissions for. Carefully review and limit the permissions of the service account to avoid unintended access to sensitive resources. Additionally, limit Warehouse creation permissions in Lakekeeper to users who are authorized to access all locations that the system identity can access.</p> <p>GCP system identities allow Lakekeeper to authenticate using the service account that the application is running as. This can be either a Compute Engine default service account or a user-assigned service account. To enable this feature system-wide, set the following environment variable:</p> <p><pre><code>LAKEKEEPER__ENABLE_GCP_SYSTEM_CREDENTIALS=true\n</code></pre> When using system identity, Lakekeeper will use the service account associated with the application or virtual machine to access Google Cloud Storage (GCS). Ensure that the service account has the necessary permissions, such as the Storage Admin role on the target bucket.</p>"}, {"location": "docs/latest/docs/table-maintenance/", "title": "Table Maintenance", "text": ""}, {"location": "docs/latest/docs/table-maintenance/#metadata-file-cleanup", "title": "Metadata File Cleanup", "text": "<p>Lakekeeper honors the Iceberg table properties <code>write.metadata.delete-after-commit.enabled</code> and <code>write.metadata.previous-versions-max</code>. Starting with Lakekeeper v0.10.0, <code>delete-after-commit</code> is enabled by default (it was disabled in earlier versions). On each table commit, when <code>delete-after-commit</code> is enabled, Lakekeeper keeps the current table metadata file plus up to <code>write.metadata.previous-versions-max</code> previous metadata files (default: 100) and deletes the oldest tracked metadata file from the metadata log once that limit is exceeded. This cleanup applies only to metadata files tracked in the metadata log; it does not remove orphaned metadata files.</p> <p>For example: if <code>write.metadata.previous-versions-max=20</code>, Lakekeeper retains 21 files in total (the current plus 20 previous); committing a 22nd version deletes the oldest tracked metadata file.</p> <p>Link to Expire Snapshots</p>"}, {"location": "docs/latest/docs/table-maintenance/#expire-snapshots", "title": "Expire Snapshots", "text": "<p>Lakekeeper automatically expires old table snapshots based on configurable age and retention policies. This helps manage storage costs and performance by removing outdated snapshot metadata and associated data files.</p> <p>Expire snapshots can be configured per warehouse and optionally overridden at the table level using Iceberg table properties.</p>"}, {"location": "docs/latest/docs/table-maintenance/#configuration", "title": "Configuration", "text": "<p>Configuration can be set via the Management UI or REST API endpoints:</p> <ul> <li>GET <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> <li>POST <code>/management/v1/warehouse/{warehouse_id}/task-queue/expire_snapshots/config</code></li> </ul> Parameter Type Default Description <code>enable-expire-snapshots</code> boolean <code>false</code> Enable automatic snapshot expiration for all tables in the warehouse. Can be overridden per table with <code>lakekeeper.history.expire.enabled</code> <code>max-snapshot-age-ms</code> integer <code>432000000</code> (5 days) Maximum age of snapshots in milliseconds before expiration. Override per table with <code>history.expire.max-snapshot-age-ms</code> <code>min-snapshots-to-keep</code> integer <code>1</code> Minimum snapshots to retain on each table branch. Override per table with <code>history.expire.min-snapshots-to-keep</code> <code>min-snapshots-to-expire</code> integer <code>20</code> Minimum snapshots required before expiration job is scheduled (prevents expensive jobs for few snapshots). Override per table with <code>lakekeeper.history.expire.min-snapshots-to-expire</code> <code>max-ref-age-ms</code> integer <code>9223372036854775807</code> (no limit) Maximum age for snapshot references (except main branch). Main branch references never expire"}, {"location": "docs/latest/docs/table-maintenance/#table-level-overrides", "title": "Table-Level Overrides", "text": "<p>Individual tables can override warehouse settings using these Iceberg table properties:</p> <ul> <li><code>lakekeeper.history.expire.enabled</code> - Enable/disable for specific table</li> <li><code>history.expire.max-snapshot-age-ms</code> - Custom max age for table snapshots  </li> <li><code>history.expire.min-snapshots-to-keep</code> - Custom minimum retention for table</li> <li><code>lakekeeper.history.expire.min-snapshots-to-expire</code> - Custom threshold for table</li> </ul> <p>Note</p> <p>Tables with <code>gc.enabled=false</code> are excluded from automatic expiration regardless of other settings.</p>"}, {"location": "docs/latest/docs/table-maintenance/#production-deployment", "title": "Production Deployment", "text": "<p>For production workloads, we recommend running expire snapshots workers in dedicated pods to avoid impacting REST API performance. This can be achieved by:</p> <ol> <li>API pods: Set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS=0</code> to disable workers</li> <li>Worker pods: Use default worker configuration (2 workers) to handle expire snapshots tasks or set <code>LAKEKEEPER__TASK_EXPIRE_SNAPSHOTS_WORKERS</code> to desired number of workers</li> </ol>"}, {"location": "docs/latest/docs/table-maintenance/#task-scheduling", "title": "Task Scheduling", "text": "<p>Expire snapshots tasks are intelligently scheduled immediately after table commits when needed, eliminating the overhead of cron-based polling. This ensures timely cleanup while maintaining optimal performance.</p>"}, {"location": "docs/latest/docs/api/", "title": "Index", "text": "OpenAPI moved to docs/docs/api Folder"}, {"location": "docs/latest/docs/api/catalog/", "title": "Catalog", "text": ""}, {"location": "docs/latest/docs/api/management-plus/", "title": "Management plus", "text": ""}, {"location": "docs/latest/docs/api/management/", "title": "Management", "text": ""}]}